<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Introduction to diffusion models course Exported on 6/19/2025 at 08:08:05 GMT+8 from Cursor (1.0.0)\nUser\nNotes Lectures Labs Instructors Introduction to Flow Matching and Diffusion Models MIT Computer Science Class 6.S184: Generative AI with Stochastic Differential Equations Diffusion and flow-based models have become the state of the art for generative AI across a wide range of data modalities, including images, videos, shapes, molecules, music, and more! This course aims to build up the mathematical framework underlying these models from first principles. At the end of the class, students will have built a toy image diffusion model from scratch, and along the way, will have gained hands-on experience with the mathematical toolbox of stochastic differential equations that is useful in many other fields. This course is ideal for students who want to develop a principled understanding of the theory and practice of generative AI.\n"><title>2025å¹´6æœˆ18æ—¥</title><link rel=canonical href=https://tannal.github.io/zh-cn/p/2025%E5%B9%B46%E6%9C%8818%E6%97%A5/><link rel=stylesheet href=/scss/style.min.b0e76e98cdd4f7082d1fc36f7af7e0843a81201db2197c03ca426751dc923735.css><meta property='og:title' content="2025å¹´6æœˆ18æ—¥"><meta property='og:description' content="Introduction to diffusion models course Exported on 6/19/2025 at 08:08:05 GMT+8 from Cursor (1.0.0)\nUser\nNotes Lectures Labs Instructors Introduction to Flow Matching and Diffusion Models MIT Computer Science Class 6.S184: Generative AI with Stochastic Differential Equations Diffusion and flow-based models have become the state of the art for generative AI across a wide range of data modalities, including images, videos, shapes, molecules, music, and more! This course aims to build up the mathematical framework underlying these models from first principles. At the end of the class, students will have built a toy image diffusion model from scratch, and along the way, will have gained hands-on experience with the mathematical toolbox of stochastic differential equations that is useful in many other fields. This course is ideal for students who want to develop a principled understanding of the theory and practice of generative AI.\n"><meta property='og:url' content='https://tannal.github.io/zh-cn/p/2025%E5%B9%B46%E6%9C%8818%E6%97%A5/'><meta property='og:site_name' content='è°­ç›Ÿ'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='è®¡ç®—æœº'><meta property='article:tag' content='é€»è¾‘'><meta property='article:published_time' content='2025-06-18T00:00:00+00:00'><meta property='article:modified_time' content='2025-06-18T00:00:00+00:00'><meta name=twitter:title content="2025å¹´6æœˆ18æ—¥"><meta name=twitter:description content="Introduction to diffusion models course Exported on 6/19/2025 at 08:08:05 GMT+8 from Cursor (1.0.0)\nUser\nNotes Lectures Labs Instructors Introduction to Flow Matching and Diffusion Models MIT Computer Science Class 6.S184: Generative AI with Stochastic Differential Equations Diffusion and flow-based models have become the state of the art for generative AI across a wide range of data modalities, including images, videos, shapes, molecules, music, and more! This course aims to build up the mathematical framework underlying these models from first principles. At the end of the class, students will have built a toy image diffusion model from scratch, and along the way, will have gained hands-on experience with the mathematical toolbox of stochastic differential equations that is useful in many other fields. This course is ideal for students who want to develop a principled understanding of the theory and practice of generative AI.\n"><link rel="shortcut icon" href=/favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=åˆ‡æ¢èœå•>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/zh-cn/><img src=/img/avatar_hu_23460dfa2f6975f3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ğŸ¤Ÿ</span></figure><div class=site-meta><h1 class=site-name><a href=/zh-cn>è°­ç›Ÿ</a></h1><h2 class=site-description>stay hungry, stay foolish</h2></div></header><ol class=menu-social><li><a href=https://space.bilibili.com/3546751254399476 target=_blank title="B ç«™ã€Œèµ·ä¸ªåå­—å«çŠŸé©´ã€" rel=me><!doctype html><svg t="1712105268862" class="icon" viewBox="0 0 1024 1024" p-id="5725" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M1019.54782609 345.3106087c-3.20556522-142.1133913-127.15408696-169.36069565-127.15408696-169.36069566s-96.70121739-.53426087-222.25252174-1.60278261l91.3586087-88.15304347s14.42504348-18.16486957-10.15095652-38.46678261c-24.576-20.30191304-26.17878261-11.21947826-34.72695653-5.87686957-7.47965217 5.3426087-117.00313043 112.72904348-136.23652174 131.96243479-49.68626087.0-101.50956522-.53426087-151.73008695-.53426087h17.63060869S315.392 43.98747826 306.84382609 38.1106087s-9.61669565-14.42504348-34.72695652 5.87686956c-24.576 20.30191304-10.15095652 38.46678261-10.15095653 38.46678261l93.49565218 90.82434783c-101.50956522.0-189.12834783.53426087-229.73217392 2.13704347C-5.69878261 213.34817391 4.45217391 345.3106087 4.45217391 345.3106087s1.60278261 283.15826087.0 426.34017391c14.42504348 143.18191304 124.48278261 166.15513043 124.48278261 166.15513043s43.8093913 1.06852174 76.39930435 1.06852174c3.20556522 9.08243478 5.87686957 53.96034783 56.0973913 53.96034783 49.68626087.0 56.0973913-53.96034783 56.09739131-53.96034783s365.96869565-1.60278261 396.42156522-1.60278261c1.60278261 15.49356522 9.08243478 56.63165217 59.30295652 56.09739131 49.68626087-1.06852174 53.42608696-59.30295652 53.42608695-59.30295652s17.09634783-1.60278261 67.85113044.0c118.60591304-21.90469565 125.55130435-160.81252174 125.55130435-160.81252174s-2.13704348-285.82956522-.53426087-427.94295652zM917.504 798.36382609c0 22.43895652-17.6306087 40.60382609-39.53530435 40.60382608H156.71652174c-21.90469565.0-39.53530435-18.16486957-39.53530435-40.60382608V320.20034783c0-22.43895652 17.6306087-40.60382609 39.53530435-40.60382609h721.25217391c21.90469565.0 39.53530435 18.16486957 39.53530435 40.60382609v478.16347826z" fill="#8a8a8a" p-id="5726"/><path d="M409.088 418.816l-203.264 38.912 17.408 76.288 201.216-38.912zm109.568 202.24c-49.664 106.496-94.208 26.112-94.208 26.112l-33.28 21.504s65.536 89.6 128 21.504c73.728 68.096 130.048-22.016 130.048-22.016l-30.208-19.456c0-.512-52.736 75.776-100.352-27.648zM619.008 495.104l201.728 38.912 16.896-76.288-202.752-38.912z" fill="#8a8a8a" p-id="5727"/></svg></a></li><li><a href=https://github.com/tannal target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com/megotannal target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li><li><a href=https://www.zhihu.com/people/roseduan target=_blank title=Zhihu rel=me><!doctype html><svg t="1704259577746" class="icon" viewBox="0 0 1024 1024" p-id="5040" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M570.581333 806.272h61.952L652.928 876.117333 764.074667 806.272h130.986666V230.186667h-324.48V806.272zM636.501333 292.693333h192.64V743.68h-73.898666l-73.813334 46.378667L668.032 743.808l-31.530667-.128V292.736zM515.754667 493.738667H377.429333a2999.466667 2999.466667.0 005.802667-194.56h135.338667S523.776 239.445334 495.872 240.128H261.76c9.216-34.730667 20.821333-70.613333 34.688-107.690667.0.0-63.701333.0-85.333333 57.130667C202.112 213.12 176.128 303.786667 129.877333 396.416c15.573333-1.706667 67.114667-3.114667 97.450667-58.794667 5.589333-15.616 6.656-17.621333 13.568-38.485333h76.373333c0 27.776-3.157333 177.109333-4.437333 194.474667h-138.24c-31.104.0-41.173333 62.549333-41.173333 62.549333h173.482666C295.253333 688.256 232.789333 799.573333 119.466667 887.466667c54.186667 15.488 108.202667-2.432 134.912-26.197334.0.0 60.8-55.338667 94.122666-183.381333L491.264 849.834667s20.906667-71.168-3.285333-105.856c-20.053333-23.637333-74.24-87.552-97.322667-110.72l-38.698667 30.72c11.52-36.992 18.474667-72.96 20.821334-107.690667h163.072s-.213333-62.549333-20.053334-62.549333z" p-id="5041" fill="#8a8a8a"/></svg></a></li><li><a href=https://mp.weixin.qq.com/s/hs36eiU7Zr-UdJ0YBtZgKQ target=_blank title="å¾®ä¿¡å…¬ä¼—å· roseduanå†™å­—çš„åœ°æ–¹" rel=me><!doctype html><svg t="1704259948565" class="icon" viewBox="0 0 1194 1024" p-id="5272" xmlns:xlink="http://www.w3.org/1999/xlink" width="233.203125" height="200"><path d="M728.064 535.296a35.498667 35.498667.0 11-70.912.0 35.498667 35.498667.0 0170.912.0m246.016.0a35.498667 35.498667.0 11-70.997333.0 35.498667 35.498667.0 0170.997333.0" fill="#8a8a8a" p-id="5273"/><path d="M902.144 930.133333l-6.656 1.450667a594.176 594.176.0 01-18.602667 3.669333c-26.453333 4.778667-44.629333 6.826667-64 6.144C645.034666 935.850666 514.218666 853.504 461.824 722.688a446.464 446.464.0 01-6.826667-19.114667l-1.962666-6.058666a199.68 199.68.0 01-3.84-13.653334 338.858667 338.858667.0 01-9.216-75.690666c0-171.008 157.013333-305.92 354.304-314.709334l8.874666-.682666c7.850667-.597333 12.970667-.853333 18.773334-.853334h14.762666l9.301334.170667c188.16 14.677333 340.394667 156.672 340.394666 323.925333.0 78.336-38.826667 155.733333-109.653333 222.293334a303.530667 303.530667.0 01-7.082667 6.4l9.984 71.082666a49.152 49.152.0 01-69.12 58.453334l-98.816-46.421334a582.485333 582.485333.0 01-9.472 2.304zm220.16-314.026666c0-131.754667-124.586667-247.978667-279.125333-260.096H821.930667c-3.754667.0-7.68.085333-13.994667.597333l-10.666667.768C631.466667 364.8 503.978667 474.453333 503.978667 608.256c0 18.773333 2.730667 40.874667 7.509333 60.842667.597333 2.474667 1.450667 5.632 2.56 9.216l1.706667 4.949333c2.133333 6.570667 4.266667 12.629333 5.632 15.957333 42.325333 105.728 149.930667 173.397333 293.717333 178.176 13.738667.512 28.16-1.109333 50.346667-5.12a531.626667 531.626667.0 0016.64-3.242666l5.632-1.28c4.693333-1.109333 8.448-1.962667 19.370666-4.778667a32 32 0 0121.418667 2.133333L1013.76 905.216l-9.984-69.290667a32 32 0 0113.909333-31.232c1.194667-.853333 8.618667-6.912 15.189334-13.056 58.709333-55.210667 89.429333-116.394667 89.429333-175.616zM12.970667 378.197333C12.970667 171.52 206.677333 5.376 442.709333 5.376c208.384.0 394.24 130.304 431.872 306.090667 1.28 5.973333 1.28 11.264.512 16.896a32 32 0 01-63.658666-5.973334C779.178667 179.2 621.482667 69.376 442.709333 69.376c-202.666666.0-365.738666 139.776-365.738666 308.906667.0 89.941333 46.165333 171.52 136.021333 237.568a32 32 0 0111.434667 35.84l-23.296 69.973333 104.96-48.896a32 32 0 0122.442666-1.706667l5.461334 1.706667c54.357333 11.093333 76.288 14.506667 108.714666 14.506667a200.789333 200.789333.0 0032.256-4.010667c-.512.170667-1.536.512-2.730666 1.536a33.109333 33.109333.0 0139.253333 1.109333A32 32 0 01516.864 730.88c-11.776 14.848-47.36 20.309333-74.24 20.309333-37.12.0-62.037333-3.584-119.893333-15.530666L198.570667 793.6a49.493333 49.493333.0 01-68.949334-59.733333l26.88-80.64c-93.525333-75.52-143.530666-170.24-143.530666-275.029334z" fill="#8a8a8a" p-id="5274"/><path d="M344.576 254.378667a44.373333 44.373333.0 11-88.746667.085333 44.373333 44.373333.0 0188.746667.0M636.586667 254.378667A44.373333 44.373333.0 11547.84 254.464a44.373333 44.373333.0 0188.746667.0" fill="#8a8a8a" p-id="5275"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/zh-cn/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>ä¸»é¡µ</span></a></li><li><a href=/zh-cn/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>å½’æ¡£</span></a></li><li><a href=/zh-cn/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>æœç´¢</span></a></li><li><a href=/zh-cn/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 7a4 4 0 108 0A4 4 0 008 7"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>å…³äº</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://tannal.github.io/>English</option><option value=https://tannal.github.io/zh-cn/ selected>Chinese</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>æš—è‰²æ¨¡å¼</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ç›®å½•</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#è¯¾ç¨‹ç›®å½•syllabus>è¯¾ç¨‹ç›®å½•ï¼ˆSyllabusï¼‰</a><ol><li><a href=#1-è¯¾ç¨‹ç®€ä»‹>1. è¯¾ç¨‹ç®€ä»‹</a></li><li><a href=#2-è¯¾ç¨‹ç¬”è®°course-notes>2. è¯¾ç¨‹ç¬”è®°ï¼ˆCourse Notesï¼‰</a></li><li><a href=#3-è®²åº§lectures>3. è®²åº§ï¼ˆLecturesï¼‰</a></li><li><a href=#4-å®éªŒlabs>4. å®éªŒï¼ˆLabsï¼‰</a></li><li><a href=#5-æ•™å¸ˆå›¢é˜Ÿinstructors>5. æ•™å¸ˆå›¢é˜Ÿï¼ˆInstructorsï¼‰</a></li><li><a href=#6-å…ˆä¿®è¦æ±‚prerequisites>6. å…ˆä¿®è¦æ±‚ï¼ˆPrerequisitesï¼‰</a></li><li><a href=#7-å…¶ä»–è¯´æ˜>7. å…¶ä»–è¯´æ˜</a></li></ol></li></ol><ol><li><a href=#1-openblas>1. OpenBLAS</a></li><li><a href=#2-hipblas>2. HipBLAS</a></li><li><a href=#3-musa>3. MUSA</a></li><li><a href=#4-metal>4. Metal</a></li><li><a href=#5-vulkan>5. Vulkan</a></li><li><a href=#6-sycl>6. SYCL</a></li><li><a href=#7-å…¶å®ƒç›¸å…³å‚æ•°>7. å…¶å®ƒç›¸å…³å‚æ•°</a></li><li><a href=#æ€»ç»“>æ€»ç»“</a></li><li><a href=#gradio-çš„ä¸»è¦ç‰¹ç‚¹>Gradio çš„ä¸»è¦ç‰¹ç‚¹</a></li><li><a href=#å…¸å‹ç”¨æ³•ç¤ºä¾‹>å…¸å‹ç”¨æ³•ç¤ºä¾‹</a></li><li><a href=#é€‚ç”¨åœºæ™¯>é€‚ç”¨åœºæ™¯</a></li><li><a href=#æ€»ç»“-1>æ€»ç»“</a></li><li><a href=#1-unconditional-generationæ— æ¡ä»¶ç”Ÿæˆ>1. Unconditional Generationï¼ˆæ— æ¡ä»¶ç”Ÿæˆï¼‰</a></li><li><a href=#2-conditional-generationæœ‰æ¡ä»¶ç”Ÿæˆ>2. Conditional Generationï¼ˆæœ‰æ¡ä»¶ç”Ÿæˆï¼‰</a></li><li><a href=#å›¾ä¸­è§£é‡Š>å›¾ä¸­è§£é‡Š</a></li><li><a href=#æ€»ç»“ä¸€å¥è¯>æ€»ç»“ä¸€å¥è¯</a></li></ol><ol><li><ol><li><a href=#1-æå‡ºäº†noise2musicç³»ç»ŸåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬æ¡ä»¶éŸ³ä¹ç”Ÿæˆ>1. <strong>æå‡ºäº†Noise2Musicç³»ç»Ÿï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬æ¡ä»¶éŸ³ä¹ç”Ÿæˆ</strong></a></li><li><a href=#2-åˆ›æ–°çš„ä¸¤é˜¶æ®µç”Ÿæˆæµç¨‹>2. <strong>åˆ›æ–°çš„ä¸¤é˜¶æ®µç”Ÿæˆæµç¨‹</strong></a></li><li><a href=#3-å¤šç§ä¸­é—´è¡¨ç¤ºçš„æ¢ç´¢>3. <strong>å¤šç§ä¸­é—´è¡¨ç¤ºçš„æ¢ç´¢</strong></a></li><li><a href=#4-å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„è‡ªåŠ¨æ„å»º>4. <strong>å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„è‡ªåŠ¨æ„å»º</strong></a></li><li><a href=#5-æ¨¡å‹æ¶æ„ä¸æŠ€æœ¯åˆ›æ–°>5. <strong>æ¨¡å‹æ¶æ„ä¸æŠ€æœ¯åˆ›æ–°</strong></a></li><li><a href=#6-ç³»ç»Ÿæ€§è¯„æµ‹ä¸å¯¹æ¯”>6. <strong>ç³»ç»Ÿæ€§è¯„æµ‹ä¸å¯¹æ¯”</strong></a></li><li><a href=#7-å¯¹éŸ³ä¹ç”Ÿæˆé¢†åŸŸçš„æ¨åŠ¨>7. <strong>å¯¹éŸ³ä¹ç”Ÿæˆé¢†åŸŸçš„æ¨åŠ¨</strong></a></li></ol></li><li><a href=#æ€»ç»“ä¸€å¥è¯-1>æ€»ç»“ä¸€å¥è¯</a></li><li><a href=#è®ºæ–‡ä¸­ç›¸å…³ä¿¡æ¯>è®ºæ–‡ä¸­ç›¸å…³ä¿¡æ¯</a></li><li><a href=#ç»éªŒæ¨ç®—>ç»éªŒæ¨ç®—</a><ol><li><a href=#å‚è€ƒç±»ä¼¼é¡¹ç›®>å‚è€ƒç±»ä¼¼é¡¹ç›®</a></li></ol></li><li><a href=#ç²—ç•¥ä¼°ç®—>ç²—ç•¥ä¼°ç®—</a></li><li><a href=#è®ºæ–‡åŸæ–‡ç›¸å…³æè¿°>è®ºæ–‡åŸæ–‡ç›¸å…³æè¿°</a></li><li><a href=#æ€»ç»“-2>æ€»ç»“</a></li><li><a href=#1-è®­ç»ƒæ•°æ®é›†è‡ªå»º>1. <strong>è®­ç»ƒæ•°æ®é›†ï¼ˆè‡ªå»ºï¼‰</strong></a><ol><li><a href=#æ•°æ®æ¥æº><strong>æ•°æ®æ¥æº</strong></a></li><li><a href=#æ–‡æœ¬æ ‡ç­¾æè¿°å¦‚ä½•è·å¾—><strong>æ–‡æœ¬æ ‡ç­¾ï¼ˆæè¿°ï¼‰å¦‚ä½•è·å¾—ï¼Ÿ</strong></a></li><li><a href=#æœ€ç»ˆæ•°æ®é›†è§„æ¨¡><strong>æœ€ç»ˆæ•°æ®é›†è§„æ¨¡</strong></a></li></ol></li><li><a href=#2-è¯„æµ‹æ•°æ®é›†å…¬å¼€>2. <strong>è¯„æµ‹æ•°æ®é›†ï¼ˆå…¬å¼€ï¼‰</strong></a></li><li><a href=#3-mulamcap-æ•°æ®é›†è®ºæ–‡è´¡çŒ®>3. <strong>MuLaMCap æ•°æ®é›†ï¼ˆè®ºæ–‡è´¡çŒ®ï¼‰</strong></a></li><li><a href=#æ€»ç»“-3>æ€»ç»“</a></li><li><a href=#1-pure-data-pd>1. <strong>Pure Data (Pd)</strong></a></li><li><a href=#2-openmusic-om>2. <strong>OpenMusic++ (OM#)</strong></a></li><li><a href=#3-ossia-score>3. <strong>ossia score</strong></a></li><li><a href=#4-supercollider>4. <strong>SuperCollider</strong></a></li><li><a href=#5-nodebox>5. <strong>NodeBox</strong></a></li><li><a href=#6-lilypond--pythonmusic21>6. <strong>LilyPond + Python/Music21</strong></a></li><li><a href=#7-orca>7. <strong>Orca</strong></a></li><li><a href=#8-inscore>8. <strong>INScore</strong></a></li><li><a href=#æ€»ç»“-4>æ€»ç»“</a></li><li><a href=#1-ä¸»æµå»ºè®®transformer>1. <strong>ä¸»æµå»ºè®®ï¼šTransformer</strong></a></li><li><a href=#2-æ‰©æ•£æ¨¡å‹diffusion-models>2. <strong>æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰</strong></a></li><li><a href=#3-å…¶ä»–æ¶æ„>3. <strong>å…¶ä»–æ¶æ„</strong></a></li><li><a href=#4-æ•°æ®å¤„ç†å»ºè®®>4. <strong>æ•°æ®å¤„ç†å»ºè®®</strong></a></li><li><a href=#5-ç®—åŠ›ä¸æ•°æ®>5. <strong>ç®—åŠ›ä¸æ•°æ®</strong></a></li><li><a href=#6-å…¶ä»–èµ„æº>6. <strong>å…¶ä»–èµ„æº</strong></a></li><li><a href=#æ€»ç»“ä¸€å¥è¯-2><strong>æ€»ç»“ä¸€å¥è¯</strong></a></li></ol><ol><li><a href=#1-mamba-çš„æ ¸å¿ƒæ€æƒ³>1. <strong>Mamba çš„æ ¸å¿ƒæ€æƒ³</strong></a></li><li><a href=#2-mamba-ä¸-transformer-çš„åŒºåˆ«>2. <strong>Mamba ä¸ Transformer çš„åŒºåˆ«</strong></a></li><li><a href=#3-mamba-çš„ä¼˜åŠ¿>3. <strong>Mamba çš„ä¼˜åŠ¿</strong></a></li><li><a href=#4-åº”ç”¨åœºæ™¯>4. <strong>åº”ç”¨åœºæ™¯</strong></a></li><li><a href=#5-ä¸€å¥è¯æ€»ç»“>5. <strong>ä¸€å¥è¯æ€»ç»“</strong></a></li><li><a href=#1-å¤„ç†æ™®é€šè¡¨å•æ•°æ®>1. å¤„ç†æ™®é€šè¡¨å•æ•°æ®</a></li><li><a href=#2-å¤„ç†æ–‡ä»¶ä¸Šä¼ >2. å¤„ç†æ–‡ä»¶ä¸Šä¼ </a></li><li><a href=#3-ç»“åˆè¡¨å•å­—æ®µå’Œæ–‡ä»¶>3. ç»“åˆè¡¨å•å­—æ®µå’Œæ–‡ä»¶</a></li><li><a href=#4-è·¯ç”±æ³¨å†Œç¤ºä¾‹>4. è·¯ç”±æ³¨å†Œç¤ºä¾‹</a></li><li><a href=#5-æ³¨æ„äº‹é¡¹>5. æ³¨æ„äº‹é¡¹</a></li><li><a href=#è¯¦ç»†è§£é‡Š>è¯¦ç»†è§£é‡Š</a></li><li><a href=#è§£å†³æ–¹æ³•>è§£å†³æ–¹æ³•</a></li><li><a href=#æ€»ç»“-5>æ€»ç»“</a><ol><li><a href=#1-largest-contentful-paint-lcp>1. <strong>Largest Contentful Paint (LCP)</strong></a></li><li><a href=#2-cumulative-layout-shift-cls>2. <strong>Cumulative Layout Shift (CLS)</strong></a></li><li><a href=#3-interaction-to-next-paint-inp>3. <strong>Interaction to Next Paint (INP)</strong></a></li></ol></li><li><a href=#æ€»ç»“-6>æ€»ç»“</a></li><li><a href=#plumber-çš„æ„æ€>Plumber çš„æ„æ€</a><ol><li><a href=#1-è‹±æ–‡é‡Šä¹‰>1. <strong>è‹±æ–‡é‡Šä¹‰</strong></a></li><li><a href=#2-å¸¸è§ç”¨æ³•>2. <strong>å¸¸è§ç”¨æ³•</strong></a></li><li><a href=#3-itç¼–ç¨‹é¢†åŸŸ>3. <strong>IT/ç¼–ç¨‹é¢†åŸŸ</strong></a></li></ol></li><li><a href=#æ€»ç»“-7>æ€»ç»“</a></li><li><a href=#è§£å†³æ–¹æ³•-1>è§£å†³æ–¹æ³•</a><ol><li><a href=#1-åŠ ä¸Š>1. <strong>åŠ ä¸Š <code>-Recurse -Force</code> å¹¶æŒ‡å®šæ–°ç›®å½•å</strong></a></li><li><a href=#2-è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•æ…ç”¨>2. <strong>è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•ï¼ˆæ…ç”¨ï¼ï¼‰</strong></a></li><li><a href=#3-åªå¤åˆ¶ç›®å½•å†…å®¹åˆ°å·²å­˜åœ¨ç›®å½•>3. <strong>åªå¤åˆ¶ç›®å½•å†…å®¹åˆ°å·²å­˜åœ¨ç›®å½•</strong></a></li></ol></li><li><a href=#æ€»ç»“-8>æ€»ç»“</a></li><li><a href=#å¸¸è§åŸå› ä¸è§£å†³åŠæ³•>å¸¸è§åŸå› ä¸è§£å†³åŠæ³•</a><ol><li><a href=#1-ä¸»ä»“åº“çš„>1. <strong>ä¸»ä»“åº“çš„ <code>.gitmodules</code> æˆ– <code>lilaq</code> ç›®å½•æŒ‡å‘äº†ä¸€ä¸ªä¸å­˜åœ¨çš„ commit</strong></a></li><li><a href=#2-ä½ æœ¬åœ°çš„-submodule-ç›®å½•ä¸å¹²å‡€æˆ–æŸå>2. <strong>ä½ æœ¬åœ°çš„ submodule ç›®å½•ä¸å¹²å‡€æˆ–æŸå</strong></a></li><li><a href=#3-è¿œç¨‹-submodule-ä»“åº“æœ‰é—®é¢˜>3. <strong>è¿œç¨‹ submodule ä»“åº“æœ‰é—®é¢˜</strong></a></li><li><a href=#4-ä¸»ä»“åº“çš„-submodule-æŒ‡å‘äº†ä¸€ä¸ªæœª-push-çš„-commit>4. <strong>ä¸»ä»“åº“çš„ submodule æŒ‡å‘äº†ä¸€ä¸ªæœª push çš„ commit</strong></a></li></ol></li><li><a href=#æ£€æŸ¥å‘½ä»¤>æ£€æŸ¥å‘½ä»¤</a></li><li><a href=#æ€»ç»“-9>æ€»ç»“</a></li><li><a href=#1-ä¸åŠ --u-çš„æƒ…å†µ>1. ä¸åŠ  <code>-u</code> çš„æƒ…å†µ</a></li><li><a href=#2-åŠ --u-çš„æƒ…å†µ>2. åŠ  <code>-u</code> çš„æƒ…å†µ</a></li><li><a href=#3-åŠ --upatch-çš„æƒ…å†µgo-117>3. åŠ  <code>-u=patch</code> çš„æƒ…å†µï¼ˆGo 1.17+ï¼‰</a></li><li><a href=#4-æ€»ç»“å¯¹æ¯”>4. æ€»ç»“å¯¹æ¯”</a></li><li><a href=#5-æ³¨æ„äº‹é¡¹-1>5. æ³¨æ„äº‹é¡¹</a></li><li><a href=#6-ä¾‹å­>6. ä¾‹å­</a></li><li><a href=#1-bearer-çš„å«ä¹‰>1. <code>Bearer</code> çš„å«ä¹‰</a></li><li><a href=#2-å…¸å‹ç”¨æ³•>2. å…¸å‹ç”¨æ³•</a></li><li><a href=#3-è¿™ä¸ª-token-æ˜¯ä»€ä¹ˆ>3. è¿™ä¸ª token æ˜¯ä»€ä¹ˆï¼Ÿ</a></li><li><a href=#4-bearer-token-çš„ç‰¹ç‚¹>4. Bearer Token çš„ç‰¹ç‚¹</a></li><li><a href=#5-æ€»ç»“>5. æ€»ç»“</a></li><li><a href=#é€‰é¢˜èƒŒæ™¯--ç ”ç©¶èƒŒæ™¯>é€‰é¢˜èƒŒæ™¯ / ç ”ç©¶èƒŒæ™¯</a><ol><li><a href=#1-æ–‡æœ¬è¡¥å…¨text-infillingçš„é‡è¦æ€§ä¸åº”ç”¨å‰æ™¯>1. <strong>æ–‡æœ¬è¡¥å…¨ï¼ˆText Infillingï¼‰çš„é‡è¦æ€§ä¸åº”ç”¨å‰æ™¯</strong></a></li><li><a href=#2-ç°æœ‰æ–¹æ³•çš„å±€é™æ€§>2. <strong>ç°æœ‰æ–¹æ³•çš„å±€é™æ€§</strong></a></li><li><a href=#3-å­¦æœ¯ä¸å®è·µéš¾é¢˜>3. <strong>å­¦æœ¯ä¸å®è·µéš¾é¢˜</strong></a></li><li><a href=#4-é€‰é¢˜ç¼˜èµ·ä¸ä¾æ®>4. <strong>é€‰é¢˜ç¼˜èµ·ä¸ä¾æ®</strong></a></li></ol></li><li><a href=#æ–‡çŒ®ç»¼è¿°>æ–‡çŒ®ç»¼è¿°</a><ol><li><a href=#1-ä»»åŠ¡èµ·æºä¸æ—©æœŸç ”ç©¶>1. ä»»åŠ¡èµ·æºä¸æ—©æœŸç ”ç©¶</a></li><li><a href=#2-åŸºäºè¯­è¨€æ¨¡å‹çš„è¡¥å…¨æ–¹æ³•>2. åŸºäºè¯­è¨€æ¨¡å‹çš„è¡¥å…¨æ–¹æ³•</a><ol><li><a href=#21-å•å‘è‡ªå›å½’è¯­è¨€æ¨¡å‹>2.1 å•å‘è‡ªå›å½’è¯­è¨€æ¨¡å‹</a></li><li><a href=#22-åŒå‘æ©ç è¯­è¨€æ¨¡å‹>2.2 åŒå‘æ©ç è¯­è¨€æ¨¡å‹</a></li><li><a href=#23-å˜é•¿è¡¥å…¨ä¸æ’å…¥å¼ç”Ÿæˆ>2.3 å˜é•¿è¡¥å…¨ä¸æ’å…¥å¼ç”Ÿæˆ</a></li></ol></li><li><a href=#3-ç”Ÿæˆå¼è¡¥å…¨ä¸è¿ç§»å­¦ä¹ >3. ç”Ÿæˆå¼è¡¥å…¨ä¸è¿ç§»å­¦ä¹ </a></li><li><a href=#4-è¿‘æœŸä»£è¡¨æ€§å·¥ä½œ>4. è¿‘æœŸä»£è¡¨æ€§å·¥ä½œ</a></li><li><a href=#5-ç°æœ‰ç ”ç©¶çš„ä¸è¶³ä¸æŒ‘æˆ˜>5. ç°æœ‰ç ”ç©¶çš„ä¸è¶³ä¸æŒ‘æˆ˜</a></li></ol></li><li><a href=#ç»¼è¿°æ€»ç»“>ç»¼è¿°æ€»ç»“</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/zh-cn/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>ç¥ç»ç½‘ç»œ</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/zh-cn/p/2025%E5%B9%B46%E6%9C%8818%E6%97%A5/>2025å¹´6æœˆ18æ—¥</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2025-06-18T00:00:00Z>Jun 18, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>é˜…è¯»æ—¶é•¿: 145 åˆ†é’Ÿ</time></div></footer></div></header><section class=article-content><h1 id=introduction-to-diffusion-models-course>Introduction to diffusion models course</h1><p><em>Exported on 6/19/2025 at 08:08:05 GMT+8 from Cursor (1.0.0)</em></p><hr><p><strong>User</strong></p><p>Notes
Lectures
Labs
Instructors
Introduction to Flow Matching and Diffusion Models
MIT Computer Science Class 6.S184: Generative AI with Stochastic Differential Equations
Diffusion and flow-based models have become the state of the art for generative AI across a wide range of data modalities, including images, videos, shapes, molecules, music, and more! This course aims to build up the mathematical framework underlying these models from first principles. At the end of the class, students will have built a toy image diffusion model from scratch, and along the way, will have gained hands-on experience with the mathematical toolbox of stochastic differential equations that is useful in many other fields. This course is ideal for students who want to develop a principled understanding of the theory and practice of generative AI.</p><p>Course Notes
The course notes serve as the backbone of the course and provide a self-contained explanation of all material in the class. In contrast, lectures slides will generally not be self-contained and are intended to provide accompanying visualizations during the lecture. You may view the notes by clicking on the colored link below.</p><p>View the course notes here!
Lectures
Lecture Topic Slides Recording
1 Flow and Diffusion Models
Introduction to generative models
Ordinary and stochastic differential equations
Sampling from flow and diffusion models
[slides 1]
2 Constructing a Training Target
Conditional and marginal probability paths
Continuity and Fokker-Planck equations
Marginal vector field and marginal score function
[slides 2]
3 Training Flow and Diffusion Models
Flow matching
Score matching
Various approaches to diffusion models
[slides 3]
4 Building an Image Generator
Guidance and conditional generation
Neural network architectures
Survey of state-of-the-art models
[slides 4]
5 Generative Robotics
Guest lecture by Benjamin Burchfiel (Toyota Research)
Large behavior models
Diffusion models for robotics
N/A
6 Generative Protein Design
Guest lecture by Jason Yim (MIT)
Designing new proteins with AI
Flow matching for protein structure generation
[slides 6]
Labs
There are 3 labs given as exercises accompanying the class to give you hands-on practical experience. The labs will guide you through building a flow matching and diffusion model from scratch step-by-step. To do the exercises, perform the following steps:
Click on the &ldquo;Open in Colab&rdquo; link to open the lab in Google Drive.
Click on the &ldquo;Open in Google Colaboratory&rdquo; link at the center top of the page. A jupyter notebook should appear.
Click on &ldquo;File&rdquo; â†’ &ldquo;Save a copy in Drive&rdquo; to save a copy of the lab to your own Google Drive.
Follow the instructions in the lab to complete the exercises.
Lab 1: Working with SDEs
Open in Colab
Lab 2: Flow Matching and Score Matching
Open in Colab
Lab 3: Conditional Image Generation
Open in Colab
Stuck? Solutions can be found here.</p><p>Instructors
This class was co-taught by Peter and Ezra. We are fortunate to have Tommi Jaakkola as our sponsor and advisor.</p><p>Instructor Photo
Peter Holderrieth
PhD Student</p><p>Instructor Photo
Ezra Erives
MEng Student</p><p>Prerequisites: Linear algebra, real analysis, and basic probability theory. Students should be familiar with Python and have some experience with PyTorch.</p><p>Questions? Email either Peter or Ezra!</p><p>Remark about LLMs: This course does not cover large language models (LLMs). LLMs involve discrete data such as text, while this course focuses on data lying in continuous spaces such as images, videos, and protein structures.</p><p>Acknowledgements
We would like to thank the following individuals and organizations without whose support this course would not be possible:
Professor Tommi Jaakkola without whose support this class would not be possible
Lisa Bella, Ellen Reid, and everyone else at MIT EECS for their generous support
Christian Fiedler, Tim Griesbach, Benedikt Geiger, and Albrecht Holderrieth for invaluable feedback on the lecture notes
Elaine Mello from MIT Open Learning for support with lecture recordings
Ashay Athalye from Students for Open and Universal Learning for helping to edit and publish lecture recordings
Cameron Diao, Tally Portnoi, Andi Qu, Roger Trullo, ÃdÃ¡m BuriÃ¡n, Zewen Yang, and many others for their invaluable contributions to the labs
The Missing Semester of Your CS Education upon whose website this one was inspired
Participants in the original course offering (MIT 6.S184/6.S975, taught over IAP 2025), as well as readers like you for your interest in this course
Thank you!</p><p>To cite this course, please use:
@misc{flowsanddiffusions2025,
author = {Peter Holderrieth and Ezra Erives},
title = {Introduction to Flow Matching and Diffusion Models},
year = {2025},
url = {https://diffusion.csail.mit.edu/}
}
Source code.</p><p>Licensed under CC BY-NC-SA.</p><p>è¿™é—¨è¯¾è®²diffusionå‘¢ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>æ˜¯çš„ï¼Œè¿™é—¨è¯¾ç¡®å®è®²è§£äº† diffusionï¼ˆæ‰©æ•£ï¼‰æ¨¡å‹ã€‚</p><p>MIT 6.S184ã€ŠIntroduction to Flow Matching and Diffusion Modelsã€‹è¯¾ç¨‹ä¸“é—¨ä»‹ç»äº†æµåŒ¹é…ï¼ˆflow matchingï¼‰å’Œæ‰©æ•£ï¼ˆdiffusionï¼‰æ¨¡å‹åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸­çš„åº”ç”¨ã€‚è¯¾ç¨‹å†…å®¹åŒ…æ‹¬ï¼š</p><ul><li>ç”Ÿæˆæ¨¡å‹çš„åŸºæœ¬åŸç†</li><li>å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰å’Œéšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰</li><li>å¦‚ä½•ç”¨æµå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œé‡‡æ ·</li><li>æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚score matchingã€flow matchingç­‰ï¼‰</li><li>å®é™…æ„å»ºå›¾åƒç”Ÿæˆå™¨</li><li>æ‰©æ•£æ¨¡å‹åœ¨æœºå™¨äººã€è›‹ç™½è´¨è®¾è®¡ç­‰é¢†åŸŸçš„åº”ç”¨</li></ul><p>è¯¾ç¨‹è¿˜åŒ…å«äº†åŠ¨æ‰‹å®éªŒï¼ˆLabsï¼‰ï¼Œå¸¦ä½ ä»é›¶å®ç°æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…æ¨¡å‹ã€‚</p><p><strong>æ€»ç»“</strong>ï¼šè¿™é—¨è¯¾ç³»ç»Ÿåœ°è®²è§£äº†æ‰©æ•£æ¨¡å‹çš„ç†è®ºã€æ•°å­¦åŸºç¡€å’Œå®é™…å®ç°ï¼Œéå¸¸é€‚åˆæƒ³æ·±å…¥ç†è§£æ‰©æ•£æ¨¡å‹çš„åŒå­¦ã€‚</p><p>å¦‚æœä½ å¯¹æ‰©æ•£æ¨¡å‹æ„Ÿå…´è¶£ï¼Œè¿™é—¨è¯¾æ˜¯å¾ˆå¥½çš„å­¦ä¹ èµ„æºï¼</p><hr><p><strong>User</strong></p><p>è¯¦ç»†è®²ä¸€ä¸‹ç›®å½•</p><hr><p><strong>Cursor</strong></p><p>å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯MIT 6.S184ã€ŠIntroduction to Flow Matching and Diffusion Modelsã€‹è¯¾ç¨‹çš„è¯¦ç»†ç›®å½•è¯´æ˜ï¼š</p><hr><h2 id=è¯¾ç¨‹ç›®å½•syllabus>è¯¾ç¨‹ç›®å½•ï¼ˆSyllabusï¼‰</h2><h3 id=1-è¯¾ç¨‹ç®€ä»‹>1. è¯¾ç¨‹ç®€ä»‹</h3><ul><li>ä»‹ç»ç”Ÿæˆæ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µ</li><li>è¯¾ç¨‹ç›®æ ‡ä¸å­¦ä¹ æˆæœ</li><li>é€‚ç”¨å¯¹è±¡ä¸å…ˆä¿®çŸ¥è¯†</li></ul><hr><h3 id=2-è¯¾ç¨‹ç¬”è®°course-notes>2. è¯¾ç¨‹ç¬”è®°ï¼ˆCourse Notesï¼‰</h3><ul><li>è¯¾ç¨‹çš„ä¸»å¹²å†…å®¹ï¼Œç³»ç»Ÿè®²è§£æ‰€æœ‰ç†è®ºä¸æ–¹æ³•</li><li>è‡ªæˆä½“ç³»ï¼Œé€‚åˆè‡ªå­¦</li><li>åŒ…å«æ•°å­¦æ¨å¯¼ã€å…¬å¼ã€ä¾‹å­å’Œä»£ç ç‰‡æ®µ</li></ul><hr><h3 id=3-è®²åº§lectures>3. è®²åº§ï¼ˆLecturesï¼‰</h3><div class=table-wrapper><table><thead><tr><th>è®²æ¬¡</th><th>ä¸»é¢˜</th><th>ä¸»è¦å†…å®¹</th></tr></thead><tbody><tr><td>1</td><td>Flow and Diffusion Models<br>æµä¸æ‰©æ•£æ¨¡å‹</td><td>- ç”Ÿæˆæ¨¡å‹ç®€ä»‹<br>- å¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ä¸éšæœºå¾®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰<br>- å¦‚ä½•ç”¨æµå’Œæ‰©æ•£æ¨¡å‹é‡‡æ ·</td></tr><tr><td>2</td><td>Constructing a Training Target<br>æ„å»ºè®­ç»ƒç›®æ ‡</td><td>- æ¡ä»¶ä¸è¾¹ç¼˜æ¦‚ç‡è·¯å¾„<br>- è¿ç»­æ€§æ–¹ç¨‹ä¸Fokker-Planckæ–¹ç¨‹<br>- è¾¹ç¼˜å‘é‡åœºä¸è¾¹ç¼˜scoreå‡½æ•°</td></tr><tr><td>3</td><td>Training Flow and Diffusion Models<br>è®­ç»ƒæµä¸æ‰©æ•£æ¨¡å‹</td><td>- Flow matchingæ–¹æ³•<br>- Score matchingæ–¹æ³•<br>- å„ç±»æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒæ–¹å¼</td></tr><tr><td>4</td><td>Building an Image Generator<br>æ„å»ºå›¾åƒç”Ÿæˆå™¨</td><td>- æ¡ä»¶ç”Ÿæˆä¸å¼•å¯¼ï¼ˆguidanceï¼‰<br>- ç¥ç»ç½‘ç»œæ¶æ„<br>- ç°æœ‰ä¸»æµæ¨¡å‹ç»¼è¿°</td></tr><tr><td>5</td><td>Generative Robotics<br>ç”Ÿæˆå¼æœºå™¨äºº</td><td>- å˜‰å®¾è®²åº§ï¼ˆToyota Researchï¼‰<br>- è¡Œä¸ºå¤§æ¨¡å‹<br>- æ‰©æ•£æ¨¡å‹åœ¨æœºå™¨äººé¢†åŸŸçš„åº”ç”¨</td></tr><tr><td>6</td><td>Generative Protein Design<br>ç”Ÿæˆå¼è›‹ç™½è´¨è®¾è®¡</td><td>- å˜‰å®¾è®²åº§ï¼ˆMITï¼‰<br>- AIè¾…åŠ©è›‹ç™½è´¨è®¾è®¡<br>- è›‹ç™½è´¨ç»“æ„ç”Ÿæˆä¸­çš„æµåŒ¹é…</td></tr></tbody></table></div><hr><h3 id=4-å®éªŒlabs>4. å®éªŒï¼ˆLabsï¼‰</h3><div class=table-wrapper><table><thead><tr><th>å®éªŒ</th><th>ä¸»é¢˜</th><th>ä¸»è¦å†…å®¹</th></tr></thead><tbody><tr><td>1</td><td>Working with SDEs<br>æ“ä½œéšæœºå¾®åˆ†æ–¹ç¨‹</td><td>- SDEåŸºç¡€<br>- SDEæ•°å€¼è§£æ³•<br>- SDEåœ¨ç”Ÿæˆæ¨¡å‹ä¸­çš„ä½œç”¨</td></tr><tr><td>2</td><td>Flow Matching and Score Matching<br>æµåŒ¹é…ä¸scoreåŒ¹é…</td><td>- å®ç°flow matchingç®—æ³•<br>- å®ç°score matchingç®—æ³•<br>- ç†è®ºä¸å®è·µç»“åˆ</td></tr><tr><td>3</td><td>Conditional Image Generation<br>æ¡ä»¶å›¾åƒç”Ÿæˆ</td><td>- æ¡ä»¶ç”Ÿæˆæ–¹æ³•<br>- ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒ<br>- è¿›é˜¶å®éªŒä¸åº”ç”¨</td></tr></tbody></table></div><p>æ¯ä¸ªå®éªŒéƒ½é…æœ‰Google Colabç¬”è®°æœ¬ï¼Œä¾¿äºåŠ¨æ‰‹å®è·µã€‚</p><hr><h3 id=5-æ•™å¸ˆå›¢é˜Ÿinstructors>5. æ•™å¸ˆå›¢é˜Ÿï¼ˆInstructorsï¼‰</h3><ul><li>Peter Holderriethï¼ˆåšå£«ç”Ÿï¼‰</li><li>Ezra Erivesï¼ˆç¡•å£«ç”Ÿï¼‰</li><li>é¡¾é—®ï¼šTommi Jaakkola æ•™æˆ</li></ul><hr><h3 id=6-å…ˆä¿®è¦æ±‚prerequisites>6. å…ˆä¿®è¦æ±‚ï¼ˆPrerequisitesï¼‰</h3><ul><li>çº¿æ€§ä»£æ•°</li><li>å®åˆ†æ</li><li>åŸºç¡€æ¦‚ç‡è®º</li><li>Pythonå’ŒPyTorchç¼–ç¨‹ç»éªŒ</li></ul><hr><h3 id=7-å…¶ä»–è¯´æ˜>7. å…¶ä»–è¯´æ˜</h3><ul><li>è¯¾ç¨‹ä¸æ¶‰åŠå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸“æ³¨äºè¿ç»­ç©ºé—´ï¼ˆå¦‚å›¾åƒã€è§†é¢‘ã€è›‹ç™½è´¨ç»“æ„ï¼‰çš„ç”Ÿæˆæ¨¡å‹</li><li>è¯¾ç¨‹èµ„æ–™å’Œä»£ç å¼€æºï¼Œéµå¾ªCC BY-NC-SAåè®®</li></ul><hr><p><strong>æ€»ç»“</strong>ï¼š<br>æœ¬è¯¾ç¨‹ä»ç†è®ºåˆ°å®è·µï¼Œç³»ç»Ÿè®²è§£äº†æµåŒ¹é…å’Œæ‰©æ•£æ¨¡å‹çš„åŸç†ã€æ•°å­¦åŸºç¡€ã€è®­ç»ƒæ–¹æ³•åŠå®é™…åº”ç”¨ï¼Œå¹¶é…æœ‰ä¸°å¯Œçš„å®éªŒå’Œä»£ç èµ„æºï¼Œéå¸¸é€‚åˆå¸Œæœ›æ·±å…¥ç†è§£ç”Ÿæˆå¼AIçš„åŒå­¦ã€‚</p><p>å¦‚éœ€æ›´è¯¦ç»†çš„æ¯ä¸€è®²å†…å®¹æˆ–å®éªŒå†…å®¹ï¼Œå¯ä»¥è¿›ä¸€æ­¥å±•å¼€è¯´æ˜ï¼</p><hr><p><strong>User</strong></p><p>Skip to content
Navigation Menu
leejet
stable-diffusion.cpp</p><p>Code
Issues 260
Pull requests 64
Discussions
Actions
Projects
Security</p><pre><code>Insights
</code></pre><p>Owner avatar
stable-diffusion.cpp
Public</p><p>leejet/stable-diffusion.cpp
t
Name Last commit message
Last commit date
vmobilis
vmobilis
fix missing argument in prototype of stbi_write_jpg (#613)
10c6501
Â·
Mar 9, 2025
.github/workflows</p><p>chore: fix cuda on github action (#580)</p><p>Feb 5, 2025
assets</p><p>feat: add sd3.5 support (#445)</p><p>Oct 24, 2024
docs</p><p>feat: add PhotoMaker Version 2 support (#358)</p><p>Nov 23, 2024
examples</p><p>chore: silence some warnings about precision loss (#620)</p><p>Mar 9, 2025
ggml @ ff90529</p><p>sync: update ggml</p><p>Mar 1, 2025
thirdparty</p><p>fix missing argument in prototype of stbi_write_jpg (#613)</p><p>Mar 9, 2025
.clang-format</p><p>style: format code</p><p>Jan 29, 2024
.dockerignore</p><p>chore: add sd Dockerfile</p><p>Aug 22, 2023
.gitignore</p><p>feat: Control Net support + Textual Inversion (embeddings) (#131)</p><p>Jan 29, 2024
.gitmodules</p><p>sync: update ggml submodule url</p><p>Aug 3, 2024
CMakeLists.txt</p><p>chore: add global SYCL compile flags (#597)</p><p>Feb 22, 2025
Dockerfile</p><p>chore: add sd Dockerfile</p><p>Aug 22, 2023
Dockerfile.musa</p><p>chore: bump MUSA SDK version to rc3.1.1 (#599)</p><p>Feb 22, 2025
LICENSE</p><p>Initial commit</p><p>Aug 13, 2023
README.md</p><p>docs: fix typo in readme (#574)</p><p>Feb 22, 2025
clip.hpp</p><p>fix: fix embeddings with quantized models (#601)</p><p>Mar 1, 2025
common.hpp</p><p>feat: flexible model architecture for dit models (Flux & SD3) (#490)</p><p>Nov 30, 2024
conditioner.hpp</p><p>fix: suport sdxl embedddings (#621)</p><p>Mar 9, 2025
control.hpp</p><p>feat: support Inpaint models (#511)</p><p>Dec 28, 2024
denoiser.hpp</p><p>feat: implement DDIM with the &ldquo;trailing&rdquo; timestep spacing and TCD (#568)</p><p>Feb 22, 2025
diffusion_model.hpp</p><p>feat: support Inpaint models (#511)</p><p>Dec 28, 2024
esrgan.hpp</p><p>feat: flexible model architecture for dit models (Flux & SD3) (#490)</p><p>Nov 30, 2024
face_detect.py</p><p>feat: add PhotoMaker Version 2 support (#358)</p><p>Nov 23, 2024
flux.hpp</p><p>feat: support Inpaint models (#511)</p><p>Dec 28, 2024
format-code.sh</p><p>refactor: reorganize code and use c api (#133)</p><p>Jan 1, 2024
ggml_extend.hpp</p><p>fix: force binary mask for inpaint models (#589)</p><p>Feb 22, 2025
gits_noise.inl</p><p>fix: clang complains about needless braces (#618)</p><p>Mar 9, 2025
lora.hpp</p><p>feat: partial LyCORIS support (tucker decomposition for LoCon + LoHa â€¦</p><p>Feb 22, 2025
mmdit.hpp</p><p>feat: flexible model architecture for dit models (Flux & SD3) (#490)</p><p>Nov 30, 2024
model.cpp</p><p>fix: ModelLoader::load_tensors duplicated check (#623)</p><p>Mar 9, 2025
model.h</p><p>sync: update ggml</p><p>Mar 1, 2025
pmid.hpp</p><p>feat: flexible model architecture for dit models (Flux & SD3) (#490)</p><p>Nov 30, 2024
preprocessing.hpp</p><p>fix: fix the issue with dynamic linking</p><p>Feb 25, 2024
rng.hpp</p><p>refactor: reorganize code and use c api (#133)</p><p>Jan 1, 2024
rng_philox.hpp</p><p>refactor: reorganize code and use c api (#133)</p><p>Jan 1, 2024
stable-diffusion.cpp</p><p>fix: correct img2img time (#616)</p><p>Mar 9, 2025
stable-diffusion.h</p><p>sync: update ggml</p><p>Mar 1, 2025
t5.hpp</p><p>feat: flexible model architecture for dit models (Flux & SD3) (#490)</p><p>Nov 30, 2024
tae.hpp</p><p>fix: fix race condition causing inconsistent value for decoder_only (â€¦</p><p>Mar 1, 2025
unet.hpp</p><p>feat: support Inpaint models (#511)</p><p>Dec 28, 2024
upscaler.cpp</p><p>chore: change SD_CUBLAS/SD_USE_CUBLAS to SD_CUDA/SD_USE_CUDA</p><p>Dec 28, 2024
util.cpp</p><p>fix: make get_files_from_dir works with absolute path (#598)</p><p>Feb 22, 2025
util.h</p><p>feat: add PhotoMaker Version 2 support (#358)</p><p>Nov 23, 2024
vae.hpp</p><p>feat: flexible model architecture for dit models (Flux & SD3) (#490)</p><p>Nov 30, 2024
vocab.hpp</p><p>feat: add sd3 support (#298)</p><p>Jul 28, 2024
Repository files navigation</p><p>README</p><pre><code>MIT license
</code></pre><p>stable-diffusion.cpp</p><p>Inference of Stable Diffusion and Flux in pure C/C++
Features</p><pre><code>Plain C/C++ implementation based on ggml, working in the same way as llama.cpp

Super lightweight and without external dependencies

SD1.x, SD2.x, SDXL and SD3/SD3.5 support
    !!!The VAE in SDXL encounters NaN issues under FP16, but unfortunately, the ggml_conv_2d only operates under FP16. Hence, a parameter is needed to specify the VAE that has fixed the FP16 NaN issue. You can find it here: SDXL VAE FP16 Fix.

Flux-dev/Flux-schnell Support

SD-Turbo and SDXL-Turbo support

PhotoMaker support.

16-bit, 32-bit float support

2-bit, 3-bit, 4-bit, 5-bit and 8-bit integer quantization support

Accelerated memory-efficient CPU inference
    Only requires ~2.3GB when using txt2img with fp16 precision to generate a 512x512 image, enabling Flash Attention just requires ~1.8GB.

AVX, AVX2 and AVX512 support for x86 architectures

Full CUDA, Metal, Vulkan and SYCL backend for GPU acceleration.

Can load ckpt, safetensors and diffusers models/checkpoints. Standalone VAEs models
    No need to convert to .ggml or .gguf anymore!

Flash Attention for memory usage optimization

Original txt2img and img2img mode

Negative prompt

stable-diffusion-webui style tokenizer (not all the features, only token weighting for now)

LoRA support, same as stable-diffusion-webui

Latent Consistency Models support (LCM/LCM-LoRA)

Faster and memory efficient latent decoding with TAESD

Upscale images generated with ESRGAN

VAE tiling processing for reduce memory usage

Control Net support with SD 1.5

Sampling method
    Euler A
    Euler
    Heun
    DPM2
    DPM++ 2M
    DPM++ 2M v2
    DPM++ 2S a
    LCM

Cross-platform reproducibility (--rng cuda, consistent with the stable-diffusion-webui GPU RNG)

Embedds generation parameters into png output as webui-compatible text string

Supported platforms
    Linux
    Mac OS
    Windows
    Android (via Termux)
</code></pre><p>TODO</p><pre><code>More sampling methods
Make inference faster
    The current implementation of ggml_conv_2d is slow and has high memory usage
Continuing to reduce memory usage (quantizing the weights of ggml_conv_2d)
Implement Inpainting support
</code></pre><p>Usage</p><p>For most users, you can download the built executable program from the latest release. If the built product does not meet your requirements, you can choose to build it manually.
Get the Code</p><p>git clone &ndash;recursive <a class=link href=https://github.com/leejet/stable-diffusion.cpp target=_blank rel=noopener>https://github.com/leejet/stable-diffusion.cpp</a>
cd stable-diffusion.cpp</p><pre><code>If you have already cloned the repository, you can use the following command to update the repository to the latest code.
</code></pre><p>cd stable-diffusion.cpp
git pull origin master
git submodule init
git submodule update</p><p>Download weights</p><p>download original weights(.ckpt or .safetensors). For example</p><pre><code>Stable Diffusion v1.4 from https://huggingface.co/CompVis/stable-diffusion-v-1-4-original
Stable Diffusion v1.5 from https://huggingface.co/runwayml/stable-diffusion-v1-5
Stable Diffuison v2.1 from https://huggingface.co/stabilityai/stable-diffusion-2-1
Stable Diffusion 3 2B from https://huggingface.co/stabilityai/stable-diffusion-3-medium
</code></pre><p>curl -L -O <a class=link href=https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt target=_blank rel=noopener>https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt</a></p><h1 id=curl--l--o>curl -L -O <a class=link href=https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors target=_blank rel=noopener>https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors</a></h1><h1 id=curl--l--o-1>curl -L -O <a class=link href=https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-nonema-pruned.safetensors target=_blank rel=noopener>https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-nonema-pruned.safetensors</a></h1><h1 id=curl--l--o-2>curl -L -O <a class=link href=https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/sd3_medium_incl_clips_t5xxlfp16.safetensors target=_blank rel=noopener>https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/sd3_medium_incl_clips_t5xxlfp16.safetensors</a></h1><p>Build
Build from scratch</p><p>mkdir build
cd build
cmake ..
cmake &ndash;build . &ndash;config Release</p><p>Using OpenBLAS</p><p>cmake .. -DGGML_OPENBLAS=ON
cmake &ndash;build . &ndash;config Release</p><p>Using CUDA</p><p>This provides BLAS acceleration using the CUDA cores of your Nvidia GPU. Make sure to have the CUDA toolkit installed. You can download it from your Linux distro&rsquo;s package manager (e.g. apt install nvidia-cuda-toolkit) or from here: CUDA Toolkit. Recommended to have at least 4 GB of VRAM.</p><p>cmake .. -DSD_CUDA=ON
cmake &ndash;build . &ndash;config Release</p><p>Using HipBLAS</p><p>This provides BLAS acceleration using the ROCm cores of your AMD GPU. Make sure to have the ROCm toolkit installed.</p><p>Windows User Refer to docs/hipBLAS_on_Windows.md for a comprehensive guide.</p><p>cmake .. -G &ldquo;Ninja&rdquo; -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ -DSD_HIPBLAS=ON -DCMAKE_BUILD_TYPE=Release -DAMDGPU_TARGETS=gfx1100
cmake &ndash;build . &ndash;config Release</p><p>Using MUSA</p><p>This provides BLAS acceleration using the MUSA cores of your Moore Threads GPU. Make sure to have the MUSA toolkit installed.</p><p>cmake .. -DCMAKE_C_COMPILER=/usr/local/musa/bin/clang -DCMAKE_CXX_COMPILER=/usr/local/musa/bin/clang++ -DSD_MUSA=ON -DCMAKE_BUILD_TYPE=Release
cmake &ndash;build . &ndash;config Release</p><p>Using Metal</p><p>Using Metal makes the computation run on the GPU. Currently, there are some issues with Metal when performing operations on very large matrices, making it highly inefficient at the moment. Performance improvements are expected in the near future.</p><p>cmake .. -DSD_METAL=ON
cmake &ndash;build . &ndash;config Release</p><p>Using Vulkan</p><p>Install Vulkan SDK from <a class=link href=https://www.lunarg.com/vulkan-sdk/ target=_blank rel=noopener>https://www.lunarg.com/vulkan-sdk/</a>.</p><p>cmake .. -DSD_VULKAN=ON
cmake &ndash;build . &ndash;config Release</p><p>Using SYCL</p><p>Using SYCL makes the computation run on the Intel GPU. Please make sure you have installed the related driver and IntelÂ® oneAPI Base toolkit before start. More details and steps can refer to llama.cpp SYCL backend.</p><h1 id=export-relevant-env-variables>Export relevant ENV variables</h1><p>source /opt/intel/oneapi/setvars.sh</p><h1 id=option-1-use-fp32-recommended-for-better-performance-in-most-cases>Option 1: Use FP32 (recommended for better performance in most cases)</h1><p>cmake .. -DSD_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx</p><h1 id=option-2-use-fp16>Option 2: Use FP16</h1><p>cmake .. -DSD_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_SYCL_F16=ON</p><p>cmake &ndash;build . &ndash;config Release</p><p>Example of text2img by using SYCL backend:</p><pre><code>download stable-diffusion model weight, refer to download-weight.

run ./bin/sd -m ../models/sd3_medium_incl_clips_t5xxlfp16.safetensors --cfg-scale 5 --steps 30 --sampling-method euler  -H 1024 -W 1024 --seed 42 -p &quot;fantasy medieval village world inside a glass sphere , high detail, fantasy, realistic, light effect, hyper detail, volumetric lighting, cinematic, macro, depth of field, blur, red light and clouds from the back, highly detailed epic cinematic concept art cg render made in maya, blender and photoshop, octane render, excellent composition, dynamic dramatic cinematic lighting, aesthetic, very inspirational, world inside a glass sphere by james gurney by artgerm with james jean, joe fenton and tristan eaton by ross tran, fine details, 4k resolution&quot;
</code></pre><p>Using Flash Attention</p><p>Enabling flash attention for the diffusion model reduces memory usage by varying amounts of MB. eg.:</p><pre><code>flux 768x768 ~600mb
SD2 768x768 ~1400mb
</code></pre><p>For most backends, it slows things down, but for cuda it generally speeds it up too. At the moment, it is only supported for some models and some backends (like cpu, cuda/rocm, metal).</p><p>Run by adding &ndash;diffusion-fa to the arguments and watch for:</p><p>[INFO ] stable-diffusion.cpp:312 - Using flash attention in the diffusion model</p><p>and the compute buffer shrink in the debug log:</p><p>[DEBUG] ggml_extend.hpp:1004 - flux compute buffer size: 650.00 MB(VRAM)</p><p>Run</p><p>usage: ./bin/sd [arguments]</p><p>arguments:
-h, &ndash;help show this help message and exit
-M, &ndash;mode [MODEL] run mode (txt2img or img2img or convert, default: txt2img)
-t, &ndash;threads N number of threads to use during computation (default: -1)
If threads &lt;= 0, then threads will be set to the number of CPU physical cores
-m, &ndash;model [MODEL] path to full model
&ndash;diffusion-model path to the standalone diffusion model
&ndash;clip_l path to the clip-l text encoder
&ndash;clip_g path to the clip-l text encoder
&ndash;t5xxl path to the the t5xxl text encoder
&ndash;vae [VAE] path to vae
&ndash;taesd [TAESD_PATH] path to taesd. Using Tiny AutoEncoder for fast decoding (low quality)
&ndash;control-net [CONTROL_PATH] path to control net model
&ndash;embd-dir [EMBEDDING_PATH] path to embeddings
&ndash;stacked-id-embd-dir [DIR] path to PHOTOMAKER stacked id embeddings
&ndash;input-id-images-dir [DIR] path to PHOTOMAKER input id images dir
&ndash;normalize-input normalize PHOTOMAKER input id images
&ndash;upscale-model [ESRGAN_PATH] path to esrgan model. Upscale images after generate, just RealESRGAN_x4plus_anime_6B supported by now
&ndash;upscale-repeats Run the ESRGAN upscaler this many times (default 1)
&ndash;type [TYPE] weight type (f32, f16, q4_0, q4_1, q5_0, q5_1, q8_0, q2_k, q3_k, q4_k)
If not specified, the default is the type of the weight file
&ndash;lora-model-dir [DIR] lora model directory
-i, &ndash;init-img [IMAGE] path to the input image, required by img2img
&ndash;control-image [IMAGE] path to image condition, control net
-o, &ndash;output OUTPUT path to write result image to (default: ./output.png)
-p, &ndash;prompt [PROMPT] the prompt to render
-n, &ndash;negative-prompt PROMPT the negative prompt (default: &ldquo;&rdquo;)
&ndash;cfg-scale SCALE unconditional guidance scale: (default: 7.0)
&ndash;skip-layers LAYERS Layers to skip for SLG steps: (default: [7,8,9])
&ndash;skip-layer-start START SLG enabling point: (default: 0.01)
&ndash;skip-layer-end END SLG disabling point: (default: 0.2)
SLG will be enabled at step int([STEPS]<em>[START]) and disabled at int([STEPS]</em>[END])
&ndash;strength STRENGTH strength for noising/unnoising (default: 0.75)
&ndash;style-ratio STYLE-RATIO strength for keeping input identity (default: 20%)
&ndash;control-strength STRENGTH strength to apply Control Net (default: 0.9)
1.0 corresponds to full destruction of information in init image
-H, &ndash;height H image height, in pixel space (default: 512)
-W, &ndash;width W image width, in pixel space (default: 512)
&ndash;sampling-method {euler, euler_a, heun, dpm2, dpm++2s_a, dpm++2m, dpm++2mv2, ipndm, ipndm_v, lcm}
sampling method (default: &ldquo;euler_a&rdquo;)
&ndash;steps STEPS number of sample steps (default: 20)
&ndash;rng {std_default, cuda} RNG (default: cuda)
-s SEED, &ndash;seed SEED RNG seed (default: 42, use random seed for &lt; 0)
-b, &ndash;batch-count COUNT number of images to generate
&ndash;schedule {discrete, karras, exponential, ays, gits} Denoiser sigma schedule (default: discrete)
&ndash;clip-skip N ignore last layers of CLIP network; 1 ignores none, 2 ignores one layer (default: -1)
&lt;= 0 represents unspecified, will be 1 for SD1.x, 2 for SD2.x
&ndash;vae-tiling process vae in tiles to reduce memory usage
&ndash;vae-on-cpu keep vae in cpu (for low vram)
&ndash;clip-on-cpu keep clip in cpu (for low vram)
&ndash;diffusion-fa use flash attention in the diffusion model (for low vram)
Might lower quality, since it implies converting k and v to f16.
This might crash if it is not supported by the backend.
&ndash;control-net-cpu keep controlnet in cpu (for low vram)
&ndash;canny apply canny preprocessor (edge detection)
&ndash;color Colors the logging tags according to level
-v, &ndash;verbose print extra info</p><p>txt2img example</p><p>./bin/sd -m ../models/sd-v1-4.ckpt -p &ldquo;a lovely cat&rdquo;</p><h1 id=binsd--m-modelsv1-5-pruned-emaonlysafetensors--p-a-lovely-cat>./bin/sd -m ../models/v1-5-pruned-emaonly.safetensors -p &ldquo;a lovely cat&rdquo;</h1><h1 id=binsd--m-modelssd_xl_base_10safetensors-vae-modelssdxl_vae-fp16-fixsafetensors--h-1024--w-1024--p-a-lovely-cat--v>./bin/sd -m ../models/sd_xl_base_1.0.safetensors &ndash;vae ../models/sdxl_vae-fp16-fix.safetensors -H 1024 -W 1024 -p &ldquo;a lovely cat&rdquo; -v</h1><h1 id=binsd--m-modelssd3_medium_incl_clips_t5xxlfp16safetensors--h-1024--w-1024--p-a-lovely-cat-holding-a-sign-says-stable-diffusion-cpp-cfg-scale-45-sampling-method-euler--v>./bin/sd -m ../models/sd3_medium_incl_clips_t5xxlfp16.safetensors -H 1024 -W 1024 -p &lsquo;a lovely cat holding a sign says "Stable Diffusion CPP"&rsquo; &ndash;cfg-scale 4.5 &ndash;sampling-method euler -v</h1><h1 id=binsd-diffusion-model--modelsflux1-dev-q3_kgguf-vae-modelsaesft-clip_l-modelsclip_lsafetensors-t5xxl-modelst5xxl_fp16safetensors---p-a-lovely-cat-holding-a-sign-says-fluxcpp-cfg-scale-10-sampling-method-euler--v>./bin/sd &ndash;diffusion-model ../models/flux1-dev-q3_k.gguf &ndash;vae ../models/ae.sft &ndash;clip_l ../models/clip_l.safetensors &ndash;t5xxl ../models/t5xxl_fp16.safetensors -p &ldquo;a lovely cat holding a sign says &lsquo;flux.cpp&rsquo;&rdquo; &ndash;cfg-scale 1.0 &ndash;sampling-method euler -v</h1><h1 id=binsd--m--modelssd35_largesafetensors-clip_l-modelsclip_lsafetensors-clip_g-modelsclip_gsafetensors-t5xxl-modelst5xxl_fp16safetensors---h-1024--w-1024--p-a-lovely-cat-holding-a-sign-says-stable-diffusion-35-large-cfg-scale-45-sampling-method-euler--v>./bin/sd -m ..\models\sd3.5_large.safetensors &ndash;clip_l ..\models\clip_l.safetensors &ndash;clip_g ..\models\clip_g.safetensors &ndash;t5xxl ..\models\t5xxl_fp16.safetensors -H 1024 -W 1024 -p &lsquo;a lovely cat holding a sign says "Stable diffusion 3.5 Large"&rsquo; &ndash;cfg-scale 4.5 &ndash;sampling-method euler -v</h1><p>Using formats of different precisions will yield results of varying quality.
f32 f16 q8_0 q5_0 q5_1 q4_0 q4_1</p><p>img2img example</p><pre><code>./output.png is the image generated from the above txt2img pipeline
</code></pre><p>./bin/sd &ndash;mode img2img -m ../models/sd-v1-4.ckpt -p &ldquo;cat with blue eyes&rdquo; -i ./output.png -o ./img2img_output.png &ndash;strength 0.4</p><p>More Guides</p><pre><code>LoRA
LCM/LCM-LoRA
Using PhotoMaker to personalize image generation
Using ESRGAN to upscale results
Using TAESD to faster decoding
Docker
Quantization and GGUF
</code></pre><p>Bindings</p><p>These projects wrap stable-diffusion.cpp for easier use in other languages/frameworks.</p><pre><code>Golang: seasonjs/stable-diffusion
C#: DarthAffe/StableDiffusion.NET
Python: william-murray1204/stable-diffusion-cpp-python
Rust: newfla/diffusion-rs
</code></pre><p>UIs</p><p>These projects use stable-diffusion.cpp as a backend for their image generation.</p><pre><code>Jellybox
Stable Diffusion GUI
Stable Diffusion CLI-GUI
</code></pre><p>Contributors</p><p>Thank you to all the people who have already contributed to stable-diffusion.cpp!</p><p>Contributors
Star History</p><p>Star History Chart
References</p><pre><code>ggml
stable-diffusion
sd3-ref
stable-diffusion-stability-ai
stable-diffusion-webui
ComfyUI
k-diffusion
latent-consistency-model
generative-models
PhotoMaker
</code></pre><p>About</p><p>Stable Diffusion and Flux in pure C/C++
Topics
flux ai cplusplus image-generation diffusion text2image image2image img2img txt2img latent-diffusion stable-diffusion ggml flux-dev flux-schnell
Resources
Readme
License
MIT license
Activity
Stars
4.2k stars
Watchers
59 watching
Forks
388 forks
Report repository
Releases 138
master-10feacf Latest
Mar 9, 2025</p><ul><li><p>137 releases
Packages
No packages published
Contributors 42</p><p>@leejet
@stduhpf
@Green-Sky
@ursg
@phudtran
@Cyberhan123
@FSSRepo
@vmobilis
@bssrdf
@daniandtheweb
@yeahdongcn
@piallai
@ring-c
@idostyle</p></li><li><p>28 contributors
Languages</p><p>C++ 100.0%</p></li></ul><p>Footer
Â© 2025 GitHub, Inc.
Footer navigation</p><pre><code>Terms
Privacy
Security
Status
Docs
Contact
</code></pre><p>é™¤äº†cudaä»¥å¤–è¿™äº›éƒ½æ˜¯ä»€ä¹ˆå‘€ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>ä½ é—®çš„â€œé™¤äº†cudaä»¥å¤–è¿™äº›éƒ½æ˜¯ä»€ä¹ˆå‘€ï¼Ÿâ€æ˜¯æŒ‡åœ¨ stable-diffusion.cpp è¿™ä¸ªé¡¹ç›®çš„æ„å»ºè¯´æ˜é‡Œï¼Œé™¤äº† CUDA ä»¥å¤–çš„å„ç§åŠ é€Ÿ/åç«¯é€‰é¡¹ã€‚ä¸‹é¢æˆ‘æ¥è¯¦ç»†è§£é‡Šä¸€ä¸‹æ¯ä¸ªé€‰é¡¹çš„å«ä¹‰å’Œé€‚ç”¨åœºæ™¯ï¼š</p><hr><h2 id=1-openblas>1. OpenBLAS</h2><ul><li><strong>ä½œç”¨</strong>ï¼šä½¿ç”¨ OpenBLAS åº“è¿›è¡Œ BLASï¼ˆåŸºç¡€çº¿æ€§ä»£æ•°å­ç¨‹åºï¼‰åŠ é€Ÿï¼Œæå‡ CPU ä¸Šçš„çŸ©é˜µè¿ç®—é€Ÿåº¦ã€‚</li><li><strong>é€‚ç”¨</strong>ï¼šæ²¡æœ‰ GPU æˆ–è€…åªæƒ³ç”¨ CPU çš„ç”¨æˆ·ã€‚</li><li><strong>å¹³å°</strong>ï¼šLinuxã€Windowsã€Mac éƒ½æ”¯æŒã€‚</li></ul><hr><h2 id=2-hipblas>2. HipBLAS</h2><ul><li><strong>ä½œç”¨</strong>ï¼šä½¿ç”¨ AMD çš„ ROCm å¹³å°å’Œ HipBLAS åº“ï¼Œåœ¨ AMD æ˜¾å¡ä¸Šè¿›è¡ŒåŠ é€Ÿã€‚</li><li><strong>é€‚ç”¨</strong>ï¼šæœ‰ AMD æ˜¾å¡çš„ç”¨æˆ·ã€‚</li><li><strong>å¹³å°</strong>ï¼šä¸»è¦æ˜¯ Linuxï¼Œéƒ¨åˆ† Windowsï¼ˆéœ€ç‰¹æ®Šé…ç½®ï¼‰ã€‚</li></ul><hr><h2 id=3-musa>3. MUSA</h2><ul><li><strong>ä½œç”¨</strong>ï¼šä¸ºä¸­å›½æ‘©å°”çº¿ç¨‹ï¼ˆMoore Threadsï¼‰æ˜¾å¡æä¾›åŠ é€Ÿï¼Œä½¿ç”¨ MUSA å·¥å…·é“¾ã€‚</li><li><strong>é€‚ç”¨</strong>ï¼šæœ‰æ‘©å°”çº¿ç¨‹æ˜¾å¡çš„ç”¨æˆ·ã€‚</li><li><strong>å¹³å°</strong>ï¼šä¸»è¦æ˜¯ Linuxã€‚</li></ul><hr><h2 id=4-metal>4. Metal</h2><ul><li><strong>ä½œç”¨</strong>ï¼šè‹¹æœè‡ªå®¶çš„ GPU åŠ é€Ÿæ¡†æ¶ï¼Œé€‚ç”¨äº Mac ç”µè„‘ï¼ˆå°¤å…¶æ˜¯ M1/M2 èŠ¯ç‰‡ï¼‰ã€‚</li><li><strong>é€‚ç”¨</strong>ï¼šMac ç”¨æˆ·ï¼Œå°¤å…¶æ˜¯ Apple Siliconã€‚</li><li><strong>å¹³å°</strong>ï¼šmacOSã€‚</li></ul><hr><h2 id=5-vulkan>5. Vulkan</h2><ul><li><strong>ä½œç”¨</strong>ï¼šè·¨å¹³å°çš„é«˜æ€§èƒ½å›¾å½¢å’Œè®¡ç®— APIï¼Œå¯ä»¥ç”¨æ¥åœ¨å¤šç§ GPUï¼ˆNvidiaã€AMDã€Intelï¼‰ä¸ŠåŠ é€Ÿã€‚</li><li><strong>é€‚ç”¨</strong>ï¼šæœ‰æ”¯æŒ Vulkan çš„æ˜¾å¡å’Œé©±åŠ¨çš„ç”¨æˆ·ã€‚</li><li><strong>å¹³å°</strong>ï¼šWindowsã€Linuxã€éƒ¨åˆ† Macã€‚</li></ul><hr><h2 id=6-sycl>6. SYCL</h2><ul><li><strong>ä½œç”¨</strong>ï¼šç”± Intel ä¸»å¯¼çš„è·¨å¹³å°å¹¶è¡Œç¼–ç¨‹æ¡†æ¶ï¼Œä¸»è¦ç”¨äº Intel GPUï¼ˆå¦‚ ARCã€é›†æˆæ˜¾å¡ï¼‰ï¼Œä¹Ÿæ”¯æŒéƒ¨åˆ†å…¶ä»–å¹³å°ã€‚</li><li><strong>é€‚ç”¨</strong>ï¼šæœ‰ Intel GPU æˆ–å¸Œæœ›ç”¨ oneAPI ç”Ÿæ€çš„ç”¨æˆ·ã€‚</li><li><strong>å¹³å°</strong>ï¼šä¸»è¦æ˜¯ Linuxï¼Œéƒ¨åˆ† Windowsã€‚</li></ul><hr><h2 id=7-å…¶å®ƒç›¸å…³å‚æ•°>7. å…¶å®ƒç›¸å…³å‚æ•°</h2><ul><li><strong>FP16/FP32</strong>ï¼šæŒ‡çš„æ˜¯æµ®ç‚¹ç²¾åº¦ï¼ŒFP16ï¼ˆåŠç²¾åº¦ï¼‰æ›´çœå†…å­˜ï¼ŒFP32ï¼ˆå•ç²¾åº¦ï¼‰æ›´ç²¾ç¡®ã€‚</li><li><strong>Quantization</strong>ï¼šé‡åŒ–ï¼ŒæŒ‡ç”¨æ›´ä½ä½å®½ï¼ˆå¦‚4ä½ã€8ä½ï¼‰å­˜å‚¨æƒé‡ï¼Œè¿›ä¸€æ­¥å‡å°‘å†…å­˜å ç”¨ã€‚</li><li><strong>Flash Attention</strong>ï¼šä¸€ç§ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶å†…å­˜å’Œé€Ÿåº¦çš„æ–¹æ³•ï¼Œéƒ¨åˆ†åç«¯æ”¯æŒã€‚</li></ul><hr><h2 id=æ€»ç»“>æ€»ç»“</h2><ul><li><strong>CUDA</strong>ï¼šNvidia æ˜¾å¡ä¸“ç”¨åŠ é€Ÿã€‚</li><li><strong>OpenBLAS</strong>ï¼šCPU åŠ é€Ÿã€‚</li><li><strong>HipBLAS</strong>ï¼šAMD æ˜¾å¡åŠ é€Ÿã€‚</li><li><strong>MUSA</strong>ï¼šæ‘©å°”çº¿ç¨‹æ˜¾å¡åŠ é€Ÿã€‚</li><li><strong>Metal</strong>ï¼šMacï¼ˆè‹¹æœï¼‰GPUåŠ é€Ÿã€‚</li><li><strong>Vulkan</strong>ï¼šè·¨å¹³å° GPU åŠ é€Ÿã€‚</li><li><strong>SYCL</strong>ï¼šIntel GPU/oneAPI åŠ é€Ÿã€‚</li></ul><p>ä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„ç¡¬ä»¶ç¯å¢ƒé€‰æ‹©åˆé€‚çš„åç«¯ã€‚å¦‚æœä½ ç”¨çš„æ˜¯ Nvidia æ˜¾å¡ï¼Œä¼˜å…ˆé€‰ CUDAï¼›å¦‚æœæ˜¯ AMD æ˜¾å¡ï¼Œé€‰ HipBLASï¼›å¦‚æœæ˜¯ Macï¼Œé€‰ Metalï¼›å¦‚æœæ˜¯ Intel GPUï¼Œé€‰ SYCLï¼›æ²¡æœ‰ç‹¬æ˜¾å°±ç”¨ OpenBLASï¼ˆCPUï¼‰ã€‚</p><p>å¦‚æœè¿˜æœ‰å…·ä½“æŸä¸ªå‚æ•°ä¸æ˜ç™½ï¼Œå¯ä»¥ç»§ç»­é—®æˆ‘ï¼</p><hr><p><strong>User</strong></p><p>è·³åˆ°ä¸»è¦å†…å®¹æ— éšœç¢åŠŸèƒ½å¸®åŠ©
æ— éšœç¢åŠŸèƒ½åé¦ˆ
gradio</p><p>ç™»å½•
å…¨éƒ¨
å›¾ç‰‡
è§†é¢‘
æ–°é—»
çŸ­è§†é¢‘
ç½‘é¡µ
å›¾ä¹¦
æ›´å¤š
å·¥å…·
Gradio</p><p>Gradio
<a class=link href=https://gradio.app target=_blank rel=noopener>https://gradio.app</a>
Â·
ç¿»è¯‘æ­¤é¡µ
Present and share. Gradio can be embedded in Python notebooks or presented as a webpage. A Gradio interface can automatically generate a public link you can &mldr;
Quickstart
Creating A Chatbot Fast - Introduction - Sharing Your App
Documentation
Interface - Introduction - Blocks - ChatInterface - Image - Textbox
Playground
return &ldquo;Hello " + name + &ldquo;!&rdquo; demo = gr.Interface(fn=greet, inputs &mldr;
Interface
Interface is Gradio&rsquo;s main high-level class, and allows you to &mldr;
Gradio Lite
@gradio/lite is a JavaScript library that enables you to run Gradio &mldr;
gradio.appç«™å†…çš„å…¶å®ƒç›¸å…³ä¿¡æ¯ Â»
gradio-app/gradio: Build and share delightful machine &mldr;</p><p>GitHub
<a class=link href=https://github.com target=_blank rel=noopener>https://github.com</a> â€º gradio-app â€º gradio
Â·
ç¿»è¯‘æ­¤é¡µ
Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary &mldr;
ç›¸å…³é—®é¢˜
What is Gradio used for?
Which is better Gradio or Streamlit?
Is Gradio owned by Huggingface?
Is Gradio free to use?
åé¦ˆ
Gradio - X</p><p>X Â· Gradio
è¶…è¿‡ 5.3ä¸‡ä½å…³æ³¨è€…
Build and share machine learning apps in 3 lines of Python. Part of the @Huggingface family. DMs are open for sharing your gradio app with us!
è§†é¢‘</p><p>11:46
YouTube Â· Raj Kapadia
Introduction to Gradio Web UI development | Gradio | Python
YouTube Â· Raj Kapadia
2023å¹´7æœˆ26æ—¥</p><p>6 é‡è¦æ—¶åˆ» åœ¨æ­¤è§†é¢‘ä¸­</p><p>55:55
YouTube Â· HuggingFace
[Launch] How to Build Machine Learning APIs Using Gradio
YouTube Â· HuggingFace
2024å¹´6æœˆ7æ—¥
10 é‡è¦æ—¶åˆ» åœ¨æ­¤è§†é¢‘ä¸­</p><p>25:14
YouTube Â· HuggingFace
Building an LLM Application with Gradio
YouTube Â· HuggingFace
2024å¹´7æœˆ6æ—¥
7 é‡è¦æ—¶åˆ» åœ¨æ­¤è§†é¢‘ä¸­
æŸ¥çœ‹å…¨éƒ¨
å›¾ç‰‡
Gradio Library-Interfaces for your Machine Learning Models
Gradio Library-Interfaces for your Machine Learning Models</p><p>YouTube
Open Source LLM Observability for Gradio - Langfuse
Open Source LLM Observability for Gradio - Langfuse</p><p>Langfuse
Creating Interactive Machine Learning Demos with Gradio &mldr;
Creating Interactive Machine Learning Demos with Gradio &mldr;</p><p>GeeksforGeeks
æ˜¾ç¤ºæ›´å¤šå›¾ç‰‡
Gradio</p><p>Hugging Face
<a class=link href=https://huggingface.co target=_blank rel=noopener>https://huggingface.co</a> â€º gradio
Â·
ç¿»è¯‘æ­¤é¡µ
Making machine learning more accessible through our open-source Python library. Rapidly create UIs to demo & deploy your model. Share with anyone, anywhere.
Gradio</p><p>GitHub
<a class=link href=https://github.com target=_blank rel=noopener>https://github.com</a> â€º gradio-app
Â·
ç¿»è¯‘æ­¤é¡µ
Delightfully easy-to-use open-source tools that make machine learning easier and more accessible - Gradio.
ç”¨æˆ·è¿˜æœç´¢äº†
gradioæ•™ç¨‹
gradioä»‹ç»
gradioæ˜¯ä»€ä¹ˆ
gradioå®‰è£…
gradioä¸­æ–‡
gradioçŸ¥ä¹
gradioæ¡†æ¶
gradioä¸­æ–‡æ–‡æ¡£
1
2
3
4
5
6
7
8
9
10
ä¸‹ä¸€é¡µ
é¦™æ¸¯ - æ ¹æ®æ‚¨çš„æ´»åŠ¨è®°å½•</p><ul><li>æ›´æ–°ä½ç½®ä¿¡æ¯
å¸®åŠ©å‘é€åé¦ˆéšç§æƒæ¡æ¬¾</li></ul><p>gradioæ˜¯ä»€ä¹ˆï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>Gradio æ˜¯ä¸€ä¸ªå¼€æºçš„ Python åº“ï¼Œå¯ä»¥è®©ä½ <strong>éå¸¸å¿«é€Ÿåœ°ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹ã€API æˆ–ä»»æ„ Python å‡½æ•°åˆ›å»ºäº¤äº’å¼çš„ Web ç•Œé¢</strong>ã€‚ä½ åªéœ€è¦å‡ è¡Œä»£ç ï¼Œå°±èƒ½æŠŠä½ çš„æ¨¡å‹å˜æˆä¸€ä¸ªå¯ä»¥åœ¨æµè§ˆå™¨ä¸­äº¤äº’çš„åº”ç”¨ï¼Œå¹¶ä¸”å¯ä»¥ä¸€é”®åˆ†äº«ç»™ä»–äººä½¿ç”¨ã€‚</p><hr><h2 id=gradio-çš„ä¸»è¦ç‰¹ç‚¹>Gradio çš„ä¸»è¦ç‰¹ç‚¹</h2><ol><li><p><strong>å¿«é€Ÿæ­å»ºç•Œé¢</strong><br>åªéœ€å‡ è¡Œä»£ç ï¼Œå°±èƒ½ä¸ºä½ çš„æ¨¡å‹æˆ–å‡½æ•°ç”Ÿæˆä¸€ä¸ªå¯è§†åŒ–çš„ Web ç•Œé¢ã€‚</p></li><li><p><strong>æ”¯æŒå¤šç§è¾“å…¥è¾“å‡ºç±»å‹</strong><br>æ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘ã€æ»‘å—ã€ä¸‹æ‹‰èœå•ç­‰å¤šç§è¾“å…¥è¾“å‡ºç»„ä»¶ã€‚</p></li><li><p><strong>æ˜“äºåˆ†äº«</strong><br>Gradio å¯ä»¥è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªå…¬å¼€çš„åˆ†äº«é“¾æ¥ï¼Œåˆ«äººæ— éœ€å®‰è£…ä»»ä½•ä¸œè¥¿ï¼Œç›´æ¥åœ¨æµè§ˆå™¨è®¿é—®å³å¯ä½“éªŒä½ çš„æ¨¡å‹ã€‚</p></li><li><p><strong>é›†æˆåˆ° Jupyter Notebook</strong><br>å¯ä»¥ç›´æ¥åœ¨ Jupyter Notebook é‡Œè¿è¡Œå’Œå±•ç¤º Gradio ç•Œé¢ã€‚</p></li><li><p><strong>ä¸ Hugging Face é›†æˆ</strong><br>Gradio æ˜¯ Hugging Face ç”Ÿæ€çš„ä¸€éƒ¨åˆ†ï¼Œå¾ˆå¤š Hugging Face ä¸Šçš„æ¨¡å‹ demo éƒ½æ˜¯ç”¨ Gradio æ­å»ºçš„ã€‚</p></li></ol><hr><h2 id=å…¸å‹ç”¨æ³•ç¤ºä¾‹>å…¸å‹ç”¨æ³•ç¤ºä¾‹</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gradio</span> <span class=k>as</span> <span class=nn>gr</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>greet</span><span class=p>(</span><span class=n>name</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=s2>&#34;Hello &#34;</span> <span class=o>+</span> <span class=n>name</span> <span class=o>+</span> <span class=s2>&#34;!&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>demo</span> <span class=o>=</span> <span class=n>gr</span><span class=o>.</span><span class=n>Interface</span><span class=p>(</span><span class=n>fn</span><span class=o>=</span><span class=n>greet</span><span class=p>,</span> <span class=n>inputs</span><span class=o>=</span><span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=n>outputs</span><span class=o>=</span><span class=s2>&#34;text&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>demo</span><span class=o>.</span><span class=n>launch</span><span class=p>()</span>
</span></span></code></pre></div><p>è¿è¡Œåä¼šå¼¹å‡ºä¸€ä¸ªç½‘é¡µï¼Œä½ å¯ä»¥è¾“å…¥åå­—ï¼Œç‚¹å‡»æŒ‰é’®ï¼Œçœ‹åˆ°è¾“å‡ºã€‚</p><hr><h2 id=é€‚ç”¨åœºæ™¯>é€‚ç”¨åœºæ™¯</h2><ul><li>æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¼”ç¤ºå’Œæµ‹è¯•</li><li>å¿«é€Ÿæ­å»ºåŸå‹å’Œå°å·¥å…·</li><li>è®©éæŠ€æœ¯äººå‘˜ä¹Ÿèƒ½ä½“éªŒå’Œä½¿ç”¨ä½ çš„æ¨¡å‹</li><li>ä½œä¸º API çš„å¯è§†åŒ–å‰ç«¯</li></ul><hr><h2 id=æ€»ç»“-1>æ€»ç»“</h2><p><strong>Gradio å°±æ˜¯è®©ä½ ç”¨æœ€ç®€å•çš„æ–¹å¼ï¼ŒæŠŠ Python ä»£ç å˜æˆå¯äº¤äº’çš„ç½‘é¡µåº”ç”¨ï¼Œå°¤å…¶é€‚åˆæœºå™¨å­¦ä¹ å’Œ AI ç›¸å…³çš„æ¼”ç¤ºå’Œåˆ†äº«ã€‚</strong></p><p>å¦‚æœä½ æƒ³è®©åˆ«äººæ–¹ä¾¿åœ°ä½“éªŒä½ çš„æ¨¡å‹ï¼ŒGradio æ˜¯éå¸¸æ¨èçš„å·¥å…·ï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>gradio</span> <span class=k>as</span> <span class=nn>gr</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>greet</span><span class=p>(</span><span class=n>name</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=s2>&#34;Hello &#34;</span> <span class=o>+</span> <span class=n>name</span> <span class=o>+</span> <span class=s2>&#34;!&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>demo</span> <span class=o>=</span> <span class=n>gr</span><span class=o>.</span><span class=n>Interface</span><span class=p>(</span><span class=n>fn</span><span class=o>=</span><span class=n>greet</span><span class=p>,</span> <span class=n>inputs</span><span class=o>=</span><span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=n>outputs</span><span class=o>=</span><span class=s2>&#34;text&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>demo</span><span class=o>.</span><span class=n>launch</span><span class=p>()</span>
</span></span></code></pre></div><hr><p><strong>User</strong></p><p>conditionalå’Œä¸æ˜¯condictionalçš„åŒºåˆ«ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>ä½ çš„é—®é¢˜éå¸¸å¥½ï¼ä¸‹é¢ç”¨é€šä¿—æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šä¸€ä¸‹**conditionalï¼ˆæœ‰æ¡ä»¶çš„ï¼‰<strong>å’Œ</strong>unconditionalï¼ˆæ— æ¡ä»¶çš„ï¼‰**ç”Ÿæˆçš„åŒºåˆ«ï¼š</p><hr><h2 id=1-unconditional-generationæ— æ¡ä»¶ç”Ÿæˆ>1. Unconditional Generationï¼ˆæ— æ¡ä»¶ç”Ÿæˆï¼‰</h2><ul><li><strong>å®šä¹‰</strong>ï¼šæ¨¡å‹ç›´æ¥ä»æ•´ä½“æ•°æ®åˆ†å¸ƒ ( p_{data} ) é‡Œé‡‡æ ·ï¼Œä¸ä¾èµ–ä»»ä½•é¢å¤–ä¿¡æ¯æˆ–æ¡ä»¶ã€‚</li><li><strong>ä¾‹å­</strong>ï¼šä½ è®©ä¸€ä¸ªç”Ÿæˆæ¨¡å‹â€œéšä¾¿ç”»ä¸€å¼ å›¾ç‰‡â€ï¼Œå®ƒå¯èƒ½ç”»å‡ºç‹—ã€çŒ«ã€é£æ™¯ã€æ±½è½¦â€¦â€¦ä»€ä¹ˆéƒ½æœ‰å¯èƒ½ã€‚</li><li><strong>å…¬å¼</strong>ï¼š( z \sim p_{data} )</li><li><strong>ç‰¹ç‚¹</strong>ï¼šæ²¡æœ‰è¾“å…¥æç¤ºï¼Œç”Ÿæˆç»“æœå®Œå…¨ç”±æ•°æ®åˆ†å¸ƒå†³å®šã€‚</li></ul><hr><h2 id=2-conditional-generationæœ‰æ¡ä»¶ç”Ÿæˆ>2. Conditional Generationï¼ˆæœ‰æ¡ä»¶ç”Ÿæˆï¼‰</h2><ul><li><strong>å®šä¹‰</strong>ï¼šæ¨¡å‹åœ¨ç»™å®šæŸä¸ªæ¡ä»¶ï¼ˆå¦‚æ–‡æœ¬æç¤ºã€ç±»åˆ«æ ‡ç­¾ç­‰ï¼‰ä¸‹ï¼Œä»<strong>æ¡ä»¶æ•°æ®åˆ†å¸ƒ</strong> ( p_{data}(\cdot|y) ) é‡Œé‡‡æ ·ã€‚</li><li><strong>ä¾‹å­</strong>ï¼šä½ å‘Šè¯‰æ¨¡å‹â€œç”»ä¸€åªç‹—â€ï¼Œå®ƒå°±åªä¼šç”»å‡ºå„ç§å„æ ·çš„ç‹—ï¼›ä½ è¯´â€œç”»ä¸€åªçŒ«â€ï¼Œå®ƒå°±åªä¼šç”»çŒ«ã€‚</li><li><strong>å…¬å¼</strong>ï¼š( z \sim p_{data}(\cdot|y) )ï¼Œå…¶ä¸­ ( y ) æ˜¯æ¡ä»¶ï¼ˆæ¯”å¦‚â€œDogâ€ã€â€œCatâ€ã€â€œLandscapeâ€ï¼‰ã€‚</li><li><strong>ç‰¹ç‚¹</strong>ï¼šæœ‰è¾“å…¥æç¤ºï¼Œç”Ÿæˆç»“æœå—æ¡ä»¶æ§åˆ¶ï¼Œæ›´å¯æ§ã€æ›´ç¬¦åˆéœ€æ±‚ã€‚</li></ul><hr><h2 id=å›¾ä¸­è§£é‡Š>å›¾ä¸­è§£é‡Š</h2><ul><li>å·¦è¾¹ï¼ˆæ— æ¡ä»¶ï¼‰ï¼šåªé‡‡æ ·â€œDogâ€å›¾ç‰‡ï¼Œæ²¡æ³•æŒ‡å®šåˆ«çš„å†…å®¹ã€‚</li><li>å³è¾¹ï¼ˆæœ‰æ¡ä»¶ï¼‰ï¼šå¯ä»¥æ ¹æ®ä¸åŒçš„æ¡ä»¶ ( y )ï¼ˆå¦‚â€œDogâ€ã€â€œCatâ€ã€â€œLandscapeâ€ï¼‰ç”Ÿæˆå¯¹åº”çš„å›¾ç‰‡ã€‚</li></ul><hr><h2 id=æ€»ç»“ä¸€å¥è¯>æ€»ç»“ä¸€å¥è¯</h2><ul><li><strong>Unconditional</strong>ï¼šå•¥éƒ½ä¸è¯´ï¼Œæ¨¡å‹è‡ªå·±å†³å®šç”Ÿæˆä»€ä¹ˆã€‚</li><li><strong>Conditional</strong>ï¼šä½ ç»™ä¸ªæ¡ä»¶ï¼ˆæ¯”å¦‚ä¸€å¥è¯æˆ–æ ‡ç­¾ï¼‰ï¼Œæ¨¡å‹æŒ‰ä½ çš„è¦æ±‚ç”Ÿæˆã€‚</li></ul><hr><p>å¦‚æœä½ æƒ³è®©AIâ€œå¬ä½ çš„è¯â€ç”Ÿæˆå†…å®¹ï¼Œå°±éœ€è¦ç”¨<strong>conditional generation</strong>ã€‚å¦‚æœåªæ˜¯è®©AIâ€œè‡ªç”±å‘æŒ¥â€ï¼Œé‚£å°±æ˜¯<strong>unconditional generation</strong>ã€‚</p><p>å¦‚éœ€æ›´æ·±å…¥çš„æ•°å­¦æˆ–ä»£ç è§£é‡Šï¼Œä¹Ÿå¯ä»¥ç»§ç»­é—®æˆ‘ï¼</p><hr><p><strong>User</strong></p><p>Noise2Music: Text-conditioned Music Generation with Diffusion Models
Qingqing Huang * 1 Daniel S. Park * 1 Tao Wang â€  1 Timo I. Denk â€  1 Andy Ly â€  1 Nanxin Chen â€  1
Zhengdong Zhang 1 Zhishuai Zhang 1 Jiahui Yu 1 Christian Frank 1 Jesse Engel 1 Quoc V. Le 1
William Chan 2 Zhifeng Chen 2 Wei Han â€  1
Abstract
We introduce Noise2Music, where a series of diffusion models is trained to generate high-quality
30-second music clips from text prompts. Two
types of diffusion models, a generator model,
which generates an intermediate representation
conditioned on text, and a cascader model, which
generates high-fidelity audio conditioned on the
intermediate representation and possibly the text,
are trained and utilized in succession to generate high-fidelity music. We explore two options
for the intermediate representation, one using a
spectrogram and the other using audio with lower
fidelity. We find that the generated audio is not
only able to faithfully reflect key elements of the
text prompt such as genre, tempo, instruments,
mood, and era, but goes beyond to ground finegrained semantics of the prompt. Pretrained large
language models play a key role in this storyâ€”
they are used to generate paired text for the audio
of the training set and to extract embeddings of
the text prompts ingested by the diffusion models.
Generated examples:
<a class=link href=https://google-research.github.io/noise2music target=_blank rel=noopener>https://google-research.github.io/noise2music</a></p><ol><li>Introduction
Deep neural networks have been shown to have remarkable
generative ability. In this work, we explore the generative
capability of deep models for audio.
We introduce Noise2Music, a diffusion-based (SohlDickstein et al., 2015; Song & Ermon, 2019; Ho et al.,</li></ol><ol start=2020><li>method of generating music from text prompts and
demonstrate its capability by generating 30-second long
24kHz music clips.
Modeling: We train a series of cascading diffusion models
*Equal contribution â€ Core contributors 1Google Research 2Work
done while at Google. Correspondence to: Qingqing Huang
<a class=link href=mailto:qqhuang@google.com>qqhuang@google.com</a>.
Preprint.
(Ho et al., 2022b), where the first model learns the generative task of producing a compressed representation of a 30-
second waveform from a text prompt, and the second model
learns to generate a 16kHz waveform conditioned on the
compressed representation and optionally the text prompt.
We have investigated two options for the intermediate representation: a log-mel spectrogram, or a 3.2kHz waveform.
1D U-Nets are used for learning the noise vectors for the
diffusion model. The diffusion models are conditioned on
user prompts in the format of free-form text, which are encoded by a pre-trained language model (LM) and ingested
by the 1D U-Net layers via cross attention. A final superresolution cascader is used to generate the 24kHz audio
from the 16kHz waveform.
Data mining: A large amount of training data is crucial
for producing high-quality samples from a deep generative
model. We employ a data mining pipeline to construct a
large-scale training dataset of diverse music audio clips,
each paired with multiple descriptive text labels. The text
labels for the audio are generated by employing a pair of pretrained deep models: first, we use a large language model to
generate a large set of generic music descriptive sentences
as caption candidates; we then use a pre-trained music-text
joint embedding model to score each unlabeled music clip
against all the caption candidates and select the captions
with the highest similarity score as pseudo labels for the
audio clip. We are able to annotate O(150K) hours of audio
sources this way to constitute our training data.
MuLaMCap: As a by-product of this work, we introduce
MuLan-LaMDA Music Caption dataset (MuLaMCap), consisting of O(400K) music-text pairs obtained by annotating
the music content from AudioSet by the process described
above. Compared to the original AudioSet ontology, where
141 out of 632 label classes are music related, the captions
in MuLaMCap come from a large vocabulary consisting of 4
million music descriptive sentences and phrases, which have
a much higher degree of diversity and granularity. We expect this dataset to be utilized for applications beyond sound
classification, e.g., music captioning, retrieval or generation.
Evaluation: We measure the quality of our text conditioned
music generation model with two metrics: the Frechet Audio Â´
Distance (FAD) (Kilgour et al., 2018) which measures how
arXiv:2302.03917v2 [cs.SD] 6 Mar 2023
Noise2Music
the quality of generated audio clips compare to that of two
benchmark datasets, e.g. the music split of AudioSet (Gemmeke et al., 2017) and MagnaTagATune (Law et al., 2009);
and the MuLan similarity score (Huang et al., 2022) which
measures the semantic alignment between text prompts and
the corresponding generated audio clips.
Generative ability: Our models demonstrate that they
can go beyond simple music attribute conditioning, e.g.,
genre, instrument, era, and are able to handle complex
and fine-grained semantics which can reflect soft attributes
such as atmosphere, feeling or activity. This is achieved
by constructing a training dataset that not only relies
on the metadata tags, but that also leverages the pretrained music-text joint embedding model to ground the
semantics to audio features. Cherry-picked examples
of music generated from text prompts can be found in
google-research.github.io/noise2music#table-1 and googleresearch.github.io/noise2music#table-4.</li></ol><ol start=2><li>Related Work
Generative models: Deep generative models have a long
and successful history in a wide range of domains. More
recently, a significant amount of effort has been focused
toward scaling up the dataset size for training models
that can produce extremely high quality samples. Here
we compile an incomplete list of such recent developments in text (Brown et al., 2020; Thoppilan et al., 2022),
speech (Wang et al., 2018; Chen et al., 2021; Borsos et al.,
2022), images (Ramesh et al., 2022; Saharia et al., 2022b;
Yu et al., 2022), and audio (Briot, 2021; Dhariwal et al.,
2020; MubertAI, 2022; Kreuk et al., 2022).
Diffusion models: Diffusion models, introduced in (SohlDickstein et al., 2015; Song & Ermon, 2019; Ho et al.,</li></ol><ol start=2020><li>have shown the capability to generate high quality
images (Ho et al., 2020; 2022b), audio (Yang et al., 2022;
Popov et al., 2021) and video (Ho et al., 2022c;a). Cascaded
diffusion models (Ho et al., 2022b; Saharia et al., 2022b),
which uses a series of diffusion models to generate a lowfidelity image and refine the image in succession to produce
a high-fidelity image, has been adapted to audio in this work.
Audio generation: Various methods have been employed
to generate audio conditioned on external input. Some relevant examples are provided in the context of the text-toaudio task, in which text-conditioned spectrogram generation and spectrogram-conditioned audio has been intensively studied (Popov et al., 2021; Chen et al., 2021; Kong
et al., 2021; Wu & Shi, 2021; Chen et al., 2022). Restricting our attention to audio generation based on descriptive
text, text conditioned general sound event generation has
been approached with auto-regressive methods by AudioGen (Kreuk et al., 2022) as well as diffusion-based methods
that operate on discrete audio codes by DiffSound (Yang
et al., 2022). If we narrow our scope to music generation,
Jukebox (Dhariwal et al., 2020), Mubert (MubertAI, 2022),
and MusicLM (Agostinelli et al., 2023) have taken an autoregressive approach, while Riffusion (Forsgren & Martiros,</li><li>employed diffusion for spectrogram generation.
Conditional signals in audio generation: Broadly speaking, two approaches have been taken on how the conditional signal, which steers the model to generate a specific
style of music, is parameterized and communicated to an
audio generation model. One approach is to project the
signal to a pre-defined, interpretable embedding spaceâ€”
Jukebox (Dhariwal et al., 2020) relies on a fixed vocabulary
of artists and genres mined from the training data to condition the decoder, while Mubert (MubertAI, 2022) matches
the user prompt to a set of tags in a predefined vocabulary.
The other, taken by works such as AudioGen (Kreuk et al.,</li><li>and MusicLM (Agostinelli et al., 2023) is to use a
pre-trained text encoder to encode arbitrary user prompts.
Authorsâ€™ Note: During the completion of this work, concurrent research which has overlap with this work has appeared
(Schneider et al., 2023).</li></ol><ol start=3><li>Methods
3.1. Diffusion models
Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020) are powerful generative models
that generate a sample by iteratively denoising random noise.
Here we review the minimal amount of information on diffusion models required for understanding our work. More
details can be found in the supplementary material.
The input to a diffusion model, which we consider to be a
generative model of some sample space, is the conditioning
signal c, a randomly sampled time step t and a sample xt
obtained by corrupting the original sample x via a Gaussian
diffusion process with a noise schedule parameterized by
the standard deviation Ïƒt of the noise at time t. The range of
time t is set to be [0, 1], from which it is uniformly sampled
during training, and the diffusion is viewed to progress in
the direction of increasing time. The dynamics of Gaussian
diffusion are well understoodâ€”the distribution of xt is completely parameterized by a single noise vector  that belongs
to a standard normal distribution, as xt maybe written as
a function of the original sample, the deterministic noise
schedule, and the noise vector , i.e., xt(x, Ïƒ, ), where it
should be understood that Ïƒ is used to denote the entire
noise schedule. The model Î¸ is trained to identify the noise
vector given this input. The diffusion loss can be written as
Ex,c,,t
wtkÎ¸(xt, c, t) âˆ’ k
2</li></ol><p>, (1)
where wt is a fixed weight function of choice.
Noise2Music
Inference is carried out by taking random noise at time t = 1
and denoising it by utilizing the noise predictions given by
the model. We use ancestral (or DDPM) sampling (Ho et al.,
2020), which provides a flexible framework for inference
allowing multiple parameters that can affect the quality
of the generated sample. First, the level of stochasticity
of the denoising process can be controlled by varying the
stochasticity parameter Î³ of the sampler. Also, an arbitrary
denoising schedule can be used, where one may choose an
arbitrary partition of the interval 0 = t0 &lt; Â· Â· Â· &lt; tn = 1 to
discretize the denoising steps.
Thus a variety of choices present themselves when one
wishes to train a diffusion model. We utilize multiple options with respect to the following elements, further details
of which can be found in the supplementary material:
â€¢ Loss weight (wt): simplified weight wt = 1 (Ho et al.,
2020) and sigma weight wt = Ïƒ
2
t
â€¢ Variance schedule: linear (Ho et al., 2020) and cosine
(Nichol & Dhariwal, 2021) schedules
â€¢ Stochasticity parameter: Î³ = 0 or 1
â€¢ Denoising step schedule
Classifier-free guidance (CFG): CFG (Ho & Salimans,
2022) is a method for improving the alignment between generated samples and conditional inputs. The conditional input
of a portion of the training samples in each training batch are
hidden from the network during training, enabling the network to learn how to predict the noise vector unconditionally
and conditionally. At inference, the noise vector with and
without the conditional input are computed, and the final
noise vector applied is set to wÎ¸(xt, c) + (1 âˆ’ w)Î¸(xt, Â·)
with w > 1. Dynamic clipping (Saharia et al., 2022b) is
applied to avoid over-saturation due to CFG.
3.2. Architecture
We deploy the 1D Efficient U-Net, a one-dimension version of the Efficient U-Net introduced in (Saharia et al.,
2022b), for the diffusion model. The U-Net model, depicted
in Figure 1, consists of a series of down-sampling and upsampling blocks which are connected by residual connections. A down/up-sampling block consists of a down/upsampling layer followed by a series of blocks obtained by
composing 1D convolutional layers, self/cross-attention layers and combine layers. The combine layer enables a single
vector to interact with a sequence of vectors, where the single vector is used to produce a channel-wise scaling and
bias. These blocks closely follow the structure of the blocks
of the efficient U-Nets constructed in (Saharia et al., 2022b),
with the two-dimensional convolutions replaced by their
one-dimensional counterparts. The exact structure of the
blocks are further elaborated in the supplementary material.
There are four possible routes of entry to the model. The
stacked input and output both consist of sequences of some
length T, while the diffusion time t is encoded into a single
time embedding vector and interacts with the model through
the aforementioned combine layers within the down and
up-sampling blocks. Given that we would like to produce a
sequence of length T, the noisy sample xt is always part of
the stacked input on the left-most side of the figure, while
the output is interpreted as the noise prediction . For the
cascading models, the low-fidelity audio on which the model
is conditioned on can be up-sampled and stacked. Meanwhile, a sequence of vectors with an arbitrary length may
interact with the blocks through cross-attention. This is the
route through which the text prompts are fed into the model.
There is also room for the model to be conditioned on an
aligned, but compressed representation of the sequence by
addition at the bottom of the â€œUâ€ of the U-Net.
3.3. Cascaded diffusion
We train two kinds of diffusion models in this work to
produce high-quality 30-second audio from text prompts.
Following (Ho et al., 2022b), we train generator models
that generate some intermediate representation of the final
audio conditioned on a text prompt, and cascader models
that produce the final audio based on the intermediate representation. For the intermediate representation, we consider
both low-fidelity audio and spectrograms.
3.3.1. WAVEFORM MODEL
Generator Model: The generator model generates 3.2kHz
audio that is conditioned on the text input. A sequence of
vectors derived from the text input is produced and fed into
the network as a cross-attention sequence.
Cascader Model: The cascader model generates 16kHz
audio that is conditioned on both the text prompt and the
low-fidelity audio generated by the generator model based
on the text prompt. The text conditioning takes place via
cross attention. Meanwhile, the low-fidelity audio is upsampled and stacked with xt and fed into the model. The upsampling is done by applying fast Fourier transform (FFT) to
the low-fi audio sequence and then applying inverse FFT to
obtain the high-fi audio from the low-fi Fourier coefficients.
3.3.2. SPECTROGRAM MODEL
Generator Model: This model generates a log-mel spectrogram conditioned on the text input. The spectrgram has 80
channels and a frequency of 100 features per second. The
input and output sequences now have a channel dimension
in addition to the sequence dimension. The pixel values of
the log-mel spectrogram are normalized to lie within [âˆ’1, 1].
Text conditioning is achieved through cross attention.
Noise2Music
Figure 1. U-Net architecture used for the diffusion models. A series of down-sampling, then up-sampling blocks is applied to the main
input sequence of length T to produce an output sequence of length T. The outputs of the down-sampling blocks are added to the outputs
of up-sampling blocks via residual connections. There are four modes of inputs to the model. The first is the (possibly stacked) main input
of sequence length T, entering on the left-hand side of the diagram. T is the target sequence length. Second, there is a time embedding
vector. Third, there can be a text embedding sequence that can be attended to by the down/up-sampling blocks via cross attention. Lastly,
there can be input of length T /C that is aligned with the sequence of length T with compression rate C.
Vocoder Model: The vocoder model generates 16kHz audio that is conditioned only on the spectrogram, which is
treated as aligned input. The down and up-sampling rates of
the U-Net model are tuned to achieve the compression rate
of the spectrogram against the audio.
3.3.3. SUPER-RESOLUTION CASCADER
A final light-weight cascader is used to generate 24kHz
audio from the 16kHz waveform produced by either model.
The 16kHz audio is up-sampled and stacked with xt as input
to the model. Text conditioning is not used for this model.
3.4. Text understanding
It has been shown in the context of text-to-image diffusion
models (Saharia et al., 2022b; Rombach et al., 2021) that
powerful text encoders are able to capture the complexity
and compositionality of music descriptive text prompts. We
adopt the T5 encoder (Raffel et al., 2020) and use the nonpooled token embedding sequence to condition the diffusion
models. A thorough comparison with alternative contextual
signals such as embeddings from different large language
models, or a single vector embedding derived from CLIPlike (Radford et al., 2021) text encoders trained on musictext pairs (Huang et al., 2022; Manco et al., 2022) is beyond
the scope of this work.
3.5. Pseudo labeling for music audio
Having large scale training data is a necessary component
for ensuring the quality of generative deep neural networks.
For example, Imagen (Saharia et al., 2022b) was trained on
O(1B) image-text pairs. Despite the fact that music content
is widely available, high quality paired music-text data is
scarce, especially in the case of free-form text that describes
the music attributes beyond high-level metadata such as title,
artist name, album name, and release year.
To generate such music-text pairs, we take a pseudo-labeling
approach via leveraging MuLan (Huang et al., 2022), a
pre-trained text and music audio joint embedding model,
together with LaMDA (Thoppilan et al., 2022), a pre-trained
large language model, to assign pseudo labels with finegrained semantic to unlabeled music audio clips.
We first curate several music caption vocabulary sets, each
consisting of a large list of music descriptive texts. As
demonstrated below, these texts vastly differ from the captions from the label classes in standard music classification
benchmarks, e.g., MagnaTagATune, FMA, and AudioSet,
in their scale and the fine-grained semantic granularity. We
consider the following three caption vocabularies:
LaMDA-LF: We prime the large language model LaMDA
to describe a list of 150k popular songs provided the song
title and artist names. The precise prompt template is provided in the supplementary material. We then process the
LaMDA responses into 4 million clean long-form sentences
that are likely to be describing music. We use LaMDA as
our LM of choice because it is trained for dialogue applications, and expect the generated text to be closer to user
prompts for generating music.
Rater-LF: We obtain 10,028 rater written captions from
MusicCaps (Agostinelli et al., 2023), and split each caption
into individual sentences. This produces 35,333 musicdescribing long-form sentences.
Rater-SF: From the same evaluation set above, we collect
all the short-form music aspect tags written by the raters,
which amounts to a vocabulary of size 23,906.
Examples of the caption vocabulary are presented in Table 1.
Noise2Music
Table 1. Caption vocabulary examples.
Vocabulary Examples
LaMDA-LF
(4M)
â€œA light, atmospheric drum groove provides a tropical feel.â€,
â€œA light EDM drumbeat carries a bass guitar, strings,
a simple piano, and percussion in the background.â€
Rater-LF
(35k)
â€œA Scottish tenor drum plays a marching beat.â€
â€œA bass guitar with a punchy sound contrasts the guitar.â€
Rater-SF
(24k)
â€œ50â€™s popâ€, â€œwide passionate male vocalâ€, â€œvintage vibesâ€,
â€œpatriotic moodâ€, â€œvivacious celloâ€, â€œexercise musicâ€
We use the MuLan model as a zero-shot music classifier to
assign captions from the vocabulary to unlabeled audio clips.
MuLan consists of a text encoder and an audio encoder,
which are trained on a large amount of highly noisy textmusic pairs with a contrastive learning scheme. Similar
to how CLIP (Radford et al., 2021) co-embeds image and
text, a 10-second long music audio clip and a sentence that
describes the music are placed closely in the same semantic
embedding space learned by MuLan. For each audio clip,
we compute its audio embedding by first segmenting the clip
into non-overlapping 10-second windows, and computing
the average of the MuLan audio embeddings of each window.
The text embeddings of all the candidate captions in the
vocabulary are also computed. The top K captions that are
closest to the audio in the embedding space are selected for
each clip. We compute the frequency counts of the captions
among all clips, and for each clip further sample K0 out of
the K captions with probabilities inverse to the frequency
counts. This last sampling step serves to balance the label
distribution and increase the diversity of the captions. We
use K = 10 and K0 = 3.
As a warm up for pseudo-labeling our large training set, we
produce MuLaMCap, a music captioning dataset derived
from the AudioSet (Gemmeke et al., 2017), by applying
this pseudo-labeling method to 388,262 / 4,497 examples
from the AudioSet train / test sets which have labels in
the music subtree of AudioSet ontology. Each 10-second
audio with music content is associated with 3 captions from
the LaMDA-LF vocabulary, 3 captions from Rater-LF, and
6 short form captions from Rater-SF.
3.6. Training data mining
To assemble a large-scale collection of audio-text pairs, we
collect approximately 6.8M music audio source files. From
each soundtrack, we extract six non-overlapping 30-second
clips. This amounts to nearly 340k hours of music. Audio
is sampled at 24kHz for training the super-resoluton model
and 16kHz for training all other models.
For each soundtrack, we consider three types of noisy text
labelsâ€”the song title, named entity tags associated with
the soundtrack (e.g., genre, artist name, instrument), and
the pseudo labels. We use three pseudo labels from the
Table 2. Models trained in this work. The token length refers to
the token length of the text prompts at training time.
Model # Params # Training Token Loss Noise
steps length weight schedule
Waveform generator 724M 1.6M 64 Simplified Cosine
Waveform cascader 487M 460k 64 Simplified Linear
Spectrogram generator 745M 1.8M 96 Sigma Linear
Spectrogram vocoder 25.7M 840k - Simplified Linear
Super-resolution cascader 81M 270k - Simplified Linear
LaMDA-LF vocabulary, and six pseudo labels from the
Rater-SF vocabulary. The pseudo labels from LaMDA-LF
and Rater-SF provide complementary information to the
named entity tags. Compared to the objective and highlevel tags, the pseudo labels include subjective descriptions
related to activity (â€œmusic for highway drivingâ€) and mood
(â€œa laid back feelâ€), and also include compositional elements
with fine-grained semantics. Since we evaluate our model
on MusicCaps (Agostinelli et al., 2023) from which the
sentences of Rater-LF were derived, we exclude any pseudo
labels from the Rater-LF vocabulary from our training data.
We include a small amount of high-quality audio to the
large pseudo-labeled training set. The audio is taken from a
subset of music tracks, which does not require attribution,
from an internally maintained music library. The music
tracks are segmented to non-overlapping 30-second clips,
while the metadata of the tracks are concatenated to form
the text prompt of the audio. This contributes O(300) hours
of annotated audio to our training data.
4. Experiments and Results
4.1. Model training details
We train four 1D U-Net models, the waveform generator and
cascader, and the spectrogram generator and vocoder for this
work. We have summarized some basic information about
the models in Table 2, while we relegate further details about
the models to the supplementary material. We note that we
found the sigma-weighted loss, which weighs the loss more
heavily on the â€œback endâ€ of the denoising schedule, crucial
for convergence of the spectrogram generator.
All the models, with the exception of the vocoder, are trained
on audio-text pairs, while the vocoder is only trained on audio. For each audio sample, a text batch is formed. The
three long prompts constitute three independent elements of
the text batch, while the shorter prompts are concatenated,
then segmented into a set token length reported in Table 2
and added to the text batch. For each audio clip, a random element of the corresponding text batch is selected at training
time and fed to the model as the paired text to the audio.
The models are trained with Adam optimization with Î²1 =
0.9 and Î²2 = 0.999. A cosine learning rate schedule with
the end point set to 2.5 M steps is used with peak learning
Noise2Music
Table 3. Inference parameters for the models used in this work.
Model Denoising Stochasticity CFG scale
step schedule parameter
Waveform generator Front-heavy 0 10.0
Waveform cascader Front-heavy 1 5.0
Spectrogram generator Back-heavy 0 5.0
Spectrogram vocoder Front-heavy 0 N/A
Super-resolution cascader Front-heavy 0 N/A
rate 1e-4 and 10k warm-up steps. An exponential moving
average (EMA) of the model parameters are taken with
decay rate 0.9999 and used at inference time. The superresolution cascader is trained with batch size 4096, while
all other models use batch size 2048. To apply CFG at
inference time, we occlude the text prompts for 10% of
the samples in each training batch. For these samples, the
output of the cross attention layers are set to zero.
While the generator models use self-attention, the cascaders
and vocoder do not. Thus while we need to train the generator models on the entire 30-second representation of the
audio, the cascader and vocoder models are trained on 3 to
4-second randomly sampled snippets.
Following (Ho et al., 2022b), two augmentations are applied
at training time for the cascader/vocoder models. One is to
randomly corrupt the conditioning low-fidelity audio or the
spectrogram input by applying diffusion noise. To do so, a
random diffusion time is chosen within [0, tmax] and applied
to the intermediate representation of the audio, i.e., the upsampled low-fi audio or the spectrogram. For the cascader
tmax is set to 0.5 while for the vocoder and super-resolution
cascader it is set to 1.0. The other is blur augmentation. For
the cascader model, a 1D blur kernel of size 10 is used with a
Gaussian blur kernel whose standard deviation ranges from
0.1 to 5.0. For the vocoder model, a 2D 5x5 blur kernel is
applied with the standard deviation ranging from 0.2 to 1.0.
4.2. Model inference and serving
4.2.1. MODEL INFERENCE
We adjust three inference hyperparameters, the denoising
schedule, the stochasticity parameter, and the CFG scale.
The parameters used for each model are listed in Table 3.
We parameterize the denoising step schedule by the time
step sizes [Î´1, Â· Â· Â· , Î´N ] that translate into denoising steps
introduced in section 3.1 via accumulation: tn =
Pn
i=1 Î´n.
The inference cost is proportional to the number of timesteps. Thus optimizing the time step schedule with a fixed
inference cost amounts to distributing a fixed number of time
steps that add up to the total time, 1. The parameter space for
the denoising step schedule is extremely large. Nevertheless,
we experiment with three different kinds of schedules we
denote â€œfront-heavy,â€ â€œuniform,â€ and â€œback-heavy.â€ The
front-heavy schedule allots many steps to the â€œfrontâ€ of the
Table 4. Inference time cost on four TPU V4 for four samples.
Model time/step (ms) steps time (s)
Waveform generator 25.0 1000 25.0
Waveform cascader 75.0 800 60.0
Spectrogram generator 8.3 1000 8.3
Spectrogram vocoder 29.9 100 0.3
Super-resolution cascader 71.7 800 57.3
schedule near t = 0 whereas the â€œback-heavyâ€ schedule
expends more steps near t = 1. The uniform schedule uses
evenly-spaced time steps. The exact schedules used are
produced in the supplementary material.
4.2.2. MODEL SERVING
We serve the models on Google Cloud TPU V4, where each
service request generates four 30-second music clips. We
apply GSPMD (Xu et al., 2021) to partition the model on
four TPU V4 devices, reducing the serving time by more
than 50%. Table 4 shows the inference time cost when the
model is served on four TPU V4 to produce four samples.
4.3. Evaluation
4.3.1. PARAMETER SELECTION FOR THE MODELS
Model parameters, including the architecture, training hyperparameters, checkpoints and inference parameters are
selected in a heuristic fashion. A small set of dev prompts,
independent of the prompts in any of the evaluation sets
presented, are devised by the authors, which are used to generate audio from the trained models. Model parameters are
selected based on the quality of the generation results, evaluated according the judgement of the authors, as well as practical limitations such as the availability of computational
resources and time. Evaluations are conducted on 16kHz
waveformsâ€”the super-resolution cascader is not utilized to
generate waveforms for producing evalution metrics.
4.3.2. EVALUATION METRICS
We measure the quality of our text conditioned music generation model with two kinds of metrics: the Frechet Audio Â´
Distance (FAD) (Kilgour et al., 2018) and the MuLan similarity score (Huang et al., 2022).
FAD measures how the audio quality of the generated audio
examples compare to that of a set of reference audio clips.
In particular, an audio encoder is used to compute the audio
embeddings of both the set of generated audio examples
and the background audio clips in the evaluation dataset.
Assuming that the distribution of the embeddings from each
set are Gaussian, and the Freechet distance between the Â´
two distributions are computed from the mean embedding
vectors and the correlation matrix of the two sets.
Three audio encoders are utilized for computing the FAD
Noise2Music
metric: a VGG1
audio event embedding model (Hershey
et al., 2017) trained on YouTube-8M (Abu-El-Haija et al.,
2016); the Trill (Shor et al., 2020) model2
, a convolutional
speech representation learning model trained on speech containing clips from AudioSet; and the MuLan audio encoder.
VGG and Trill produce frame-wise embeddings while MuLanâ€™s embeddings are clip-wise. Since the audio encoders
are trained on different datasets and tasks, FAD computed
with those audio representations focus on different aspects
of the audio. We hypothesize that FADVGG evaluates the
general audio quality, FADTrill is more indicative of the vocal
quality, and FADMulan captures global musical semantics.
The contrastive model MuLan provides us a way to quantify
the similarity between audio-text pairs as well as audioaudio pairs. For a given text-audio or audio-audio pair,
we define the MuLan similarity as the cosine similarity
between the MuLan embeddings of the two entities. For
a given evaluation set of music-text pairs, we compute the
average similarity between the audio generated from the
text prompts of the dataset and either the text or the ground
truth audio associated to the text. As a reference, we also
compute the average MuLan similarity of the evaluation set
against the ground truth audio, as well as a â€œrandomâ€ audio
pairing obtained by shuffling the ground truth audio.
4.3.3. EVALUATION DATASETS
We report the FAD and MuLan similarity with respect to
the following three datasets consisting of text-music pairs.
First, we re-purpose the audio tagging benchmark MagnaTagATune (MTAT) (Law et al., 2009) to evaluate the 29-second
long music clips generated by our models. MTAT contains
25,863 music clips, 21,638 of which are associated with
multiple tags from a vocabulary of 188 music tags. We
only utilize these 21,638 examples for evaluation, for each
of which we concatenate the music tags with into a single
string that we use as the associated text prompt. During
evaluation, we generate a single 29-second long audio clip
for the prompt associated with each of the 21,638 examples.
Second, we use AudioSet-Music-Eval, the music portion of
AudioSet (Gemmeke et al., 2017). There are 1,482 music
related examples in the evaluation split of AudioSet, where
each 10-second clip is associated with labels from the nontrivial part of the music subtree of AudioSet ontology. For
each example in this set, we use the concatenated labels
as the text prompt to generate a 30-second long clip, the
middle 10-second portion of which is used for evaluation.
Lastly, we evaluate on MusicCaps (Agostinelli et al.,
2023) which consists of 5.5K 10-second clips from AudioSet paired with rater written captions. We use the rater
1
tfhub.dev/google/vggish/1
2
tfhub.dev/google/nonsemantic-speech-benchmark/trill/3
Table 5. The FAD between the reference dataset audio and the
generated audio with prompts from the reference dataset. Three
audio encoders, VGG, Trill and MuLan have been used to measure
FAD. A lower value indicates better proximity of quality.
Dataset/Model FADVGG FADTrill FADMuLan
MusicCaps (Agostinelli et al., 2023)
Riffusion (Forsgren & Martiros, 2022) 13.371 0.763 0.487
Mubert (MubertAI, 2022) 9.620 0.449 0.366
MusicLM (Agostinelli et al., 2023) 4.0 0.44 -
Noise2Music Waveform 2.134 0.405 0.110
Noise2Music Spectrogram 3.840 0.474 0.180
AudioSet-Music-Eval
Noise2Music Waveform 2.240 0.252 0.193
Noise2Music Spectrogram 3.498 0.323 0.276
MagnaTagATune
Noise2Music Waveform 3.554 0.352 0.235
Noise2Music Spectrogram 5.553 0.419 0.346
captions as text prompts, and report the metrics with the
middle 10-second excerpts of the generated audio samples.
4.4. Evaluation results
In Table 5, we report the FAD of our models on the three
evaluation datasets, and compare them with baseline models
from Riffusion3
and Mubert4
. In Table 6, we report the
average audio-text and audio-audio MuLan similarity scores
between the generated audio and the evaluation datasets.
We also include the metrics computed for the ground truth
audio, as well as the shuffled ground truth.
The evaluation metrics should be interpreted with care, since
our result has potential advantages over the baselines presented. First, there is a possibility that our training data
distribution is closer to the evaluation datasets compared
to the baselines. Also, one may suspect that the MuLanbased metrics might be biased towards our models, since the
MuLan model has been used to pseudo-label our data. The
reader should thus be cautious to draw conclusions about the
effectiveness of the methods used in this paper compared
to those of the baselines based on these metrics. The metrics, however, are indeed representative of the performance
of the trained models themselves in the AudioSet domain
and provides a quantitative measure of final model performance. This ideally should hold true for the MuLan-based
metrics as well, if we assume that the MuLan model, much
like CLIP (Radford et al., 2021), has learned an un-biased,
faithful representation of text and audio data.
We also conduct human listening tests to measure the semantic alignment, whose setup is identical to that used in
3We query Mubert API at github.com/MubertAI as of Dec 24,
2022 to generate 10-second audio clips given the text prompts in
the evaluation datasets.
4We ran inference with riffusion-model-v1 provided by
github.com/riffusion/riffusion-app as of Dec 24, 2022 to generate 10-second audio clips.
Noise2Music
Table 6. The average MuLan similarity between the generated audio and either the text prompt or the ground truth audio for each
evaluation set. A higher value indicates better semantic alignment.
Dataset/Model audio â†” gt-text audio â†” gt-audio
MusicCaps (Agostinelli et al., 2023)
Ground Truth Audio 0.452 (1.000)
Randomly Shuffled Audio 0.248 0.278
Riffusion (Forsgren & Martiros, 2022) 0.342 0.312
Mubert (MubertAI, 2022) 0.323 0.280
MusicLM (Agostinelli et al., 2023) 0.51 -
Noise2Music Waveform 0.478 0.489
Noise2Music Spectrogram 0.434 0.464
AudioSet-Music-Eval
Ground Truth Audio 0.470 (1.000)
Randomly Shuffled Audio 0.274 0.265
Noise2Music Waveform 0.563 0.429
Noise2Music Spectrogram 0.490 0.389
MagnaTagATune
Ground Truth Audio 0.498 (1.000)
Randomly Shuffled Audio 0.277 0.315
Noise2Music Waveform 0.518 0.479
Noise2Music Spectrogram 0.459 0.444
Table 7. The number of wins in pair-wise comparisons of the human listening study. Higher indicates better semantic alignment.
MusicCaps Noise2Music Waveform MusicLM Mubert Riffusion
959 718 692 254 308
(Agostinelli et al., 2023) with now five sources, listed in
Table 7. The participants of the test are presented with a text
caption from the MusicCaps evaluation set and 10-second
clips from two different sources, then asked which clip is
better described by the text of the caption on a 5-point Likert
scale. We collect 3k ratings, with each source involved in
1.2k pair-wise comparisons. The the head-to-head comparisons between each pair of sources is evenly distributed. In
Table 7, we report the total number of â€œwinsâ€ each model
achieved among the 1.2k comparisons it has been subject
to. Our waveform model shows comparable performance to
MusicLM, while being behind the ground truth audio.
4.5. Inference parameter ablations
We vary inference parameters of the models and observe
its effects. We note that we have conducted ablations with
model checkpoints that are slightly less-trained compared to
the checkpoints used to produce the evaluation numbers in
the previous subsection. The ablations are conducted with
respect to the base parameters as listed in Table 3.
In Figure 2, we depict how FAD measured with VGG and
the MuLan similarity score change as the denoising step
schedule and the CFG scale are varied during inference.
Only one parameter is varied at a time, and all other parameters stay fixed at the baseline values.
We find an overall correlation between the FAD metric and
the similarity score, except in the case of the cascader, where
FAD can get worse while the similarity score improves. We
Figure 2. We plot how FADVGG and the MuLan similarity score
vary as inference parameters are adjusted. The CFG parameters
take values from [1, 2, 5, 10, 15], while â€œBâ€ack-heavy, â€œUâ€niform
and â€œFâ€ront-heavy denoising step schedules have been applied.
also find that there is an optimal CFG scale, and too big
of a CFG scale hurts the generation quality. It can also be
seen that the generator CFG scale is a bigger factor than
the denoising schedule of the generator, while the impact of
cascader denoising schedule is extremely large.
Figure 3. Quality metrics of the generated examples plotted against
computational cost parameterized by inference time.
4.6. Inference cost and performance
In Figure 3, we plot the quality metrics against the inference
cost measured by the inference time. We reduce/increase
the number of inference steps of the generator or the cascader/vocoder and inverse-proportionally scale the step sizes
in the inference schedule. We find that the effect of increasing the inference cost of the generator is mixed while
the generative quality generally improves with more cascader/vocoder inference steps.
5. Qualitative analysis
Content representation: We present generation examples
at google-research.github.io/noise2music#table-2, to illustrate that the model is able to ground the music aspects
represented in the text prompt. In particular, we find that
the genre, instrument, mood, vocal traits, and era of music
implied in the text is manifested in the generated music.
Noise2Music
Creative prompts: While our models often struggle to produce high quality audio from out-of-distribution prompts,
they are nevertheless able to generate some interesting examples. In google-research.github.io/noise2music#table-3,
we have collected examples of creative prompts for which
the model was able to generate quality music.
6. Discussion
Spectrogram vs. waveform approach: The spectrogram
and waveform approaches have their comparative advantages. The spectrogram models employed in this work are
much cheaper to train and serve compared to the waveform models, and are more scalable in time length. This
is because the sequence length of the spectrogram is much
shorter than that of a low-fi waveform. In addition, the
spectrogram contains high-frequency information which is
missing in the low-fidelity audio. Meanwhile, the waveform
model produces interpretable representations at every step
of the generation process, making the model easy to debug
and tune. This is partially responsible for our ability to train
the waveform models with more ease.
Future directions: While we have demonstrated the potential of text prompt based music generation, there is much
room for improvement beyond our work. Increasing model
interpretability, further improving text-audio alignment, reducing training and inference cost, and scaling up the generation length of the audio are just a few directions in which
our work needs to be improved. Another interesting direction is to fine-tune the models trained in this work for diverse
audio tasks including music completion and modification,
as was done for images by Saharia et al. (2022a).
7. Broader Impact
We believe our work has the potential to grow into a useful
tool for artists and content creators that can further enrich
their creative pursuits. To live up to this promise, more work
is needed with musicians and other stakeholders to develop
models into a meaningful co-creation tool.
We acknowledge the limitations of the proposed model. In
particular, large generative models learn to imitate patterns
and biases inherent in the training sets, and in our case, the
model can propagate the potential biases built in the text and
music corpora used to train our models. Such biases can be
hard to detect as they manifest in often subtle, unpredictable
ways, which are not fully captured by our current evaluation
benchmarks. Demeaning or other harmful language may be
generated in model outputs, due to learned associations or
by chance.
Beyond this, we recognize that musical genres are complex
and key musical attributes are contextual and change over
time. Training data reflect a limited corpus of musical samples and genres, given uneven recording and digitization of
samples from global musical cultures. How music is categorized and labeled can essentialize genres; and these labels
may be constructed and applied without the participation
of communities. When readers examine the released generation examples in the accompanied website, we caution
readers not to presume each sample can generalize to an
entire musical genre or one label can capture the diversity of
musical genres produced within a region (i.e. â€œLatin musicâ€
contains a broad range of cultures and styles). Moreover,
musical samples may sound â€œauthenticâ€ to those outside
these communities, as nuances in musical traditions need
trained ears/cultural knowledge to recognize. In generating
vocals, there may be possible caricatures, 11mock accents,â€
parodies, or other demeaning linguistic harms (e.g., â€œmock
Black singingâ€ in a request for â€œsoulful vocalsâ€ or â€œmock
Spanishâ€ in a Latin music request) that arise in text prompts
requesting cultural or religious musical genres, or genres
that emerged as part of the political struggles of certain
communities (e.g., Black American music, Nueva cancion, Â´
Chicano folk, Brazilian Tropicalismo, Sufi Qaw).
As is with any other technology, the result of our research
can be misused or abused. We acknowledge the risk of
potential misappropriation when the created content exactly
matches examples in training data. In accordance with responsible model development practices, duplication checks
are a built-in part of our current pipeline of producing and
releasing examples, and will continue to be for any future
work.
Efforts for identifying potential safety issues and addressing
them are important components for improving these generative models. Until there is a more clear understanding of the
limitations and risks, we do not intend to release the model.
Acknowledgements
We are grateful to Aren Jansen for building MuLan, which is
an indispensable component of this project. We give thanks
to Austin Tarango, Fernando Diaz, Kathy Meier-Hellstern,
Molly FitzMorris, and Renee Shelby for helping us incorporate important responsible AI practices around this project.
We acknowledge support from Blake Cunningham, Cara
Adams, for giving us advice along the project and assisting
us with the publication process. We appreciate valuable
feedback and support from Alex Ku, Andrea Agostinelli,
Ankur Bapna, Chen Liang, Ed Chi, Ekin Dogus Cubuk, Erica Moreira, Esteban Real, Heiga Zen, Jaehoon Lee, James
Qin, Nathan Park, Stephen Kelly, Thang Luoung, Weizhe
Hua, Ye Jia, Yifeng Lu, Yonghui Wu, Yu Zhang, Yuma
Koizumi. Special thanks to authors of MusicLM for helpful
discussions and cooperation, and especially for sharing their
evaluation set and manuscript before publication.
Noise2Music
References
Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici,
G., Varadarajan, B., and Vijayanarasimhan, S. Youtube8m: A large-scale video classification benchmark, 2016.
URL <a class=link href=https://arxiv.org/abs/1609.08675 target=_blank rel=noopener>https://arxiv.org/abs/1609.08675</a>.
Agostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti,
M., Caillon, A., Huang, Q., Jansen, A., Roberts, A.,
Tagliasacchi, M., Sharifi, M., Zeghidour, N., and Frank,
C. Musiclm: Generating music from text. arxiv preprint
arxiv:2301.11325, 2023.
Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E.,
Pietquin, O., Sharifi, M., Teboul, O., Grangier, D.,
Tagliasacchi, M., and Zeghidour, N. Audiolm: a language modeling approach to audio generation, 2022. URL
<a class=link href=https://arxiv.org/abs/2209.03143 target=_blank rel=noopener>https://arxiv.org/abs/2209.03143</a>.
Briot, J.-P. From artificial neural networks to deep learning
for music generation: history, concepts and trends. Neural
Computing and Applications, 33(1):39â€“65, 2021.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877â€“1901, 2020.
Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and
Chan, W. Wavegrad: Estimating gradients for waveform
generation. In International Conference on Learning
Representations, 2021.
Chen, Z., Tan, X., Wang, K., Pan, S., Mandic, D., He, L.,
and Zhao, S. Infergrad: Improving diffusion models for
vocoder by considering inference in training. In ICASSP
2022-2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 8432â€“8436.
IEEE, 2022.
Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,
and Sutskever, I. Jukebox: A generative model for music.
arXiv preprint arXiv:2005.00341, 2020.
Forsgren, S. and Martiros, H. Riffusion - Stable diffusion
for real-time music generation. 2022. URL https:
//riffusion.com/about.
Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A.,
Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M.
Audio set: An ontology and human-labeled dataset for
audio events. In 2017 IEEE international conference on
acoustics, speech and signal processing (ICASSP), pp.
776â€“780. IEEE, 2017.
Hershey, S., Chaudhuri, S., Ellis, D. P. W., Gemmeke, J. F.,
Jansen, A., Moore, C., Plakal, M., Platt, D., Saurous,
R. A., Seybold, B., Slaney, M., Weiss, R., and Wilson,
K. Cnn architectures for large-scale audio classification.
In International Conference on Acoustics, Speech and
Signal Processing (ICASSP). 2017. URL https://
arxiv.org/abs/1609.09430.
Ho, J. and Salimans, T. Classifier-free diffusion guidance.
arXiv preprint arXiv:2207.12598, 2022.
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840â€“6851, 2020.
Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,
A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,
et al. Imagen video: High definition video generation
with diffusion models. arXiv preprint arXiv:2210.02303,
2022a.
Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and
Salimans, T. Cascaded diffusion models for high fidelity
image generation. J. Mach. Learn. Res., 23:47â€“1, 2022b.
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M.,
and Fleet, D. J. Video diffusion models. arXiv preprint
arXiv:2204.03458, 2022c.
Huang, Q., Jansen, A., Lee, J., Ganti, R., Li, J. Y., and
Ellis, D. P. W. Mulan: A joint embedding of music audio
and natural language. In Proceedings of the the 23rd
International Society for Music Information Retrieval
Conference (ISMIR), 2022.
Kilgour, K., Zuluaga, M., Roblek, D., and Sharifi,
M. Fr\â€™echet audio distance: A metric for evaluating music enhancement algorithms. arXiv preprint
arXiv:1812.08466, 2018.
Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B.
Diffwave: A versatile diffusion model for audio synthesis.
In International Conference on Learning Representations,
2021.
Kreuk, F., Synnaeve, G., Polyak, A., Singer, U., Defossez, Â´
A., Copet, J., Parikh, D., Taigman, Y., and Adi, Y. Audiogen: Textually guided audio generation. arXiv preprint
arXiv:2209.15352, 2022.
Law, E., West, K., Mandel, M. I., Bay, M., and Downie, J. S.
Evaluation of algorithms using games: The case of music
tagging. In ISMIR, pp. 387â€“392, 2009.
Manco, I., Benetos, E., Quinton, E., and Fazekas, G. Contrastive audio-language learning for music, 2022. URL
<a class=link href=https://arxiv.org/abs/2208.12208 target=_blank rel=noopener>https://arxiv.org/abs/2208.12208</a>.
MubertAI. <a class=link href=https://github.com/mubertai/mubert-text-tomusic target=_blank rel=noopener>https://github.com/mubertai/mubert-text-tomusic</a>. 2022.
Noise2Music
Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion
probabilistic models. In International Conference on
Machine Learning, pp. 8162â€“8171. PMLR, 2021.
Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudinov, M. Grad-tts: A diffusion probabilistic model for
text-to-speech. In International Conference on Machine
Learning, pp. 8599â€“8608. PMLR, 2021.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. In International Conference on
Machine Learning, pp. 8748â€“8763. PMLR, 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring
the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res., 21(140):1â€“67, 2020.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,
M. Hierarchical text-conditional image generation with
clip latents. arXiv preprint arXiv:2204.06125, 2022.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer, B. High-resolution image synthesis with latent
diffusion models, 2021.
Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans,
T., Fleet, D., and Norouzi, M. Palette: Image-to-image
diffusion models. In ACM SIGGRAPH 2022 Conference
Proceedings, pp. 1â€“10, 2022a.
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,
E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S.,
Lopes, R. G., et al. Photorealistic text-to-image diffusion
models with deep language understanding. arXiv preprint
arXiv:2205.11487, 2022b.
Schneider, F., Jin, Z., and Scholkopf, B. Mo Â¨ usai: Text- Ë†
to-music generation with long-context latent diffusion,
2023.
Shor, J., Jansen, A., Maor, R., Lang, O., Tuval, O., de Chaumont Quitry, F., Tagliasacchi, M., Shavitt, I., Emanuel,
D., and Haviv, Y. Towards Learning a Universal NonSemantic Representation of Speech. In Proc. Interspeech
2020, pp. 140â€“144, 2020. doi: 10.21437/Interspeech.
2020-1242.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on
Machine Learning, pp. 2256â€“2265. PMLR, 2015.
Song, Y. and Ermon, S. Generative modeling by estimating
gradients of the data distribution. Advances in Neural
Information Processing Systems, 32, 2019.
Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,
Du, Y., et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
Wang, Y., Stanton, D., Zhang, Y., Ryan, R.-S., Battenberg, E., Shor, J., Xiao, Y., Jia, Y., Ren, F., and Saurous,
R. A. Style tokens: Unsupervised style modeling, control
and transfer in end-to-end speech synthesis. In International Conference on Machine Learning, pp. 5180â€“5189.
PMLR, 2018.
Wu, S. and Shi, Z. It\Ë† otts and it\Ë† owave: Linear stochastic
differential equation is all you need for audio generation.
arXiv preprint arXiv:2105.07583, 2021.
Xu, Y., Lee, H., Chen, D., Hechtman, B. A., Huang, Y.,
Joshi, R., Krikun, M., Lepikhin, D., Ly, A., Maggioni, M.,
Pang, R., Shazeer, N., Wang, S., Wang, T., Wu, Y., and
Chen, Z. GSPMD: general and scalable parallelization for
ML computation graphs. CoRR, abs/2105.04663, 2021.
URL <a class=link href=https://arxiv.org/abs/2105.04663 target=_blank rel=noopener>https://arxiv.org/abs/2105.04663</a>.
Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y.,
and Yu, D. Diffsound: Discrete diffusion model for textto-sound generation. arXiv preprint arXiv:2207.09983,
2022.
Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z.,
Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive models for content-rich text-to-image
generation. arXiv preprint arXiv:2206.10789, 2022.
Noise2Music
A. Diffusion models
In this section, we review some relevant information for diffusion models and set up the notation used in the main text of the
paper. We follow (Saharia et al., 2022b) in our presentation.
A diffusion model assumes a set-up where a sample x from a distribution corrupted by a Gaussian diffusion process with a
noise schedule, represented by a monotonically increasing standard deviation Ïƒt at time t. More precisely, the distribution
for the corrupted sample xt at time t conditioned on x or xs for s &lt; t is given by:
q(xt|x) = N (Î±tx, Ïƒ2
t
I), q(xt|xs) = N ((Î±t/Î±s)xs, Ïƒ2
t|s
I). (2)
We can define a set of variables that will make the equations cleaner:
Î±t =
q
1 âˆ’ Ïƒ
2
t
, Î»t = ln(Î±
2
t /Ïƒ2
t
), Ïƒ2
t|s = (1 âˆ’ e
Î»tâˆ’Î»s
)Ïƒ
2
t
, ÏƒËœ
2
s|t = (1 âˆ’ e
Î»tâˆ’Î»s
)Ïƒ
2
s
. (3)
The time variables s and t are assumed to be in the range [0, 1].
As seen in the first equation, the randomness of the corrupted sample xt is encoded in a single noise vector  âˆ¼ N (0, I).
The aim of the diffusion model is to model this noise vector, given the corrupted sample, the time t and the context c:
Î¸(xt, c, t).
Once the model is trained, we sample noise at time t = 1, i.e., x1 âˆ¼ N (0, I), and reverse the diffusion process to produce a
â€œcleanâ€ sample x0 from the original distribution. We employ ancestral (or DDPM) sampling (Ho et al., 2020) to do so. In
this sampling method, we select some time steps 0 = t0 &lt; Â· Â· Â· &lt; tN = 1 and reverse the diffusion process by applying the
update rule to obtain xs from xt:
xs =
Î±s
Î±t
xt âˆ’ (1 âˆ’ e
Î»tâˆ’Î»s
) Â·
Î±s
Î±t
Â· Ïƒt Â· Î¸(xt, c, t) + ËœÏƒ
1âˆ’Î³
s|t
Â· Ïƒ
Î³
t|s
Â· Ëœ, (4)
where s = tNâˆ’n and t = tNâˆ’n+1 at the n-th update step. Here, Ëœ is a random standard normal vector sampled at each
inference step. Î³ is a hyperparameter that controls the stochasticity of the diffusion process. As Î³ increases, a larger variance
is introduced at a given inference step since Ïƒt|s > ÏƒËœs|t
for t > s.
A.1. Noise schedules
We utilize two noise schedulesâ€”the linear (Ho et al., 2020) and cosine (Nichol & Dhariwal, 2021) schedules.
The linear schedule (Ho et al., 2020) is defined to be such that Ïƒ
2
t
increases in a linear fashion with respect to time:
Ïƒ
2
t = (Ïƒ
2
1 âˆ’ Ïƒ
2
0
)t + Ïƒ
2
0
. (5)
We use the initial and final values Ïƒ
2
0 = 0.0001 and Ïƒ
2
1 = 0.02 in this work.
We use a slight variant of the cosine schedule (Nichol & Dhariwal, 2021):
Î±t = cos(at + b) (6)
where a = arctan e
10 âˆ’ arctan e
âˆ’10 and b = arctan e
âˆ’10 are taken so that Î»t=0 = 20 and Î»t=1 = âˆ’20.
A.2. Denoising step schedules
Here we list the actual denoising step schedules used at inference time for the generator and cascader/vocoder models. These
schedules have been hand-tuned by trial and error as explained in section 4.3.1. The numerical values of the time step sizes
for each model and schedule used for ablation is given in python notation in Table 8. The schedules used for final evaluation
are italicized. Notice for the spectrogram generator, the back-heavy schedule has been modified further for evaluation.
Meanwhile, a plot of the denoising time reached at a given inference step for each model schedule is plotted in Figure 4.
B. Model architecture
We present some details on the architecture of the models used in the paper. The overall structure of the U-Net has been
depicted in Figure 1. After an entry convolutional layer is applied to the input, the input is passed through a series of
Noise2Music
Table 8. The denoising time steps values for denoising schedules. The schedules selected for use for each model are italicized.
Model Schedule Time steps
Waveform generator Front-heavy front heavy = [0.01 / 200] * 200 + [0.04 / 400] * 400 + [0.15 / 200] * 200 + [0.3 / 150] * 150 + [0.5 / 50] * 50
Back-heavy back heavy = front heavy[::-1]
Uniform uniform = [1.0 / 1000] * 1000
Waveform cascader Front-heavy front heavy = [0.05 / 400] * 400 + [0.15 / 200] * 200 + [0.3 / 150] * 150 + [0.5 / 50] * 50
Back-heavy back heavy = front heavy[::-1]
Uniform uniform = [1.0 / 800] * 800
Spectrogram generator Front-heavy front heavy = [0.01 / 400] * 400 + [0.04 / 800] * 800 + [0.15 / 400] * 400 + [0.3 / 300] * 300 + [0.5 / 100] * 100
Back-heavy back heavy = front heavy[::-1]
Uniform uniform = [1.0 / 1000] * 1000
Back-heavy back heavy eval = [0.3 / 50] * 50 + [0.3 / 150] * 150 + [0.2 / 300] * 300 + [0.2 / 500] * 500
Spectrogram vocoder Front-heavy front heavy = [0.05 / 50] * 50 + [0.15 / 30] * 30 + [0.3 / 15] * 15 + [0.5 / 5] * 5
Back-heavy back heavy = front heavy[::-1]
Uniform uniform = [1.0 / 100] * 100
Super-resolution cascader Front-heavy front heavy = [0.05 / 400] * 400 + [0.15 / 200] * 200 + [0.3 / 150] * 150 + [0.5 / 50] * 50
Figure 4. The denoising time reached at a given inference step for the four denoising time step schedules used in the paper. A front-heavy
schedule expends most of its steps near t = 0 while a back-heavy schedule expends most of its steps near t = 1.
down-sampling and up-sampling layers with convolutional blocks in between. Here we expand upon how the convolutional
blocks in the down-sampling/up-sampling portion of the U-Nets are structured, and explain how each network utilized in the
generation pipeline is configured.
B.1. Down-sampling and up-sampling blocks
The model architecture closely follows that of the efficient U-Net (Saharia et al., 2022b), with two-dimensional convolutional
layers replaced by one-dimensional convolutional layers. There are small differences, which we review here.
As seen in Figure 1, the U-Net model is a mirror image of itself. Given the model depth D, the model employs D downsampling and D up-sampling layers, which we may label by I = 1, Â· Â· Â· , D. Each down-sampling layer is a one-dimensional
convolutional layer with stride SI . Thus, denoting the feature length and feature dimension of the forward-propagated input
at depth-I to be TI and CI , the down-sampling layer takes an input with (length, channel) dimensions (TIâˆ’1, CIâˆ’1) and
maps it to an output with dimensions (TI , CI ) with TI = TIâˆ’1/SI . The up-sampling layer does the mirror operation, where
an input with (TI , CI ) dimensions is mapped to an output with (TIâˆ’1, CIâˆ’1) dimensions, with up-sampling stride SI .
Between the down-sampling and up-sampling layers, â€œconvolutional blocksâ€ of uniform dimension are used. The exact
same block is used for both the down-sampling and the up-sampling portion of the U-Net, and the same number of blocks
are employed at the same depth. The structure of a block is depicted in Figure 5. While all blocks interact with the
time embedding vectorâ€”obtained by converting the float into a vector via positional embedding and applying a linear
layerâ€”through the â€œcombine embeddingâ€ layer, the self attention and the cross attention layers are only turned on for
selected depths, or not even used at all in some cases. The â€œcombine embeddingâ€ layer applies a fully connected layer to the
time embedding to compute a channel-wise scaling and bias vector, which is applied to the input sequence. The self and
cross attention layers include the standard post-attention residual layer with a hidden layer of twice the dimension of CI .
Meanwhile, the entry convolutional layer and the exit convolutional layer both have kernel size 3. The â€œzerothâ€ channel
width of the input to the first down-sampling layer and the output of the last up-sampling layer are both set to a â€œbase model
dimensionâ€. The time embedding dimension is also set to this value.
Noise2Music
Figure 5. The structure of the convolutional blocks that form the base unit of operation in the 1D U-Nets. The self and cross attention
layers are optional.
Let us now summarize the key hyperparameters that determine the architecture of the network.
â€¢ Base model dimension
â€¢ Convolutional kernel size
â€¢ Depth of the network D
â€¢ Length-D list of down-sampling factors
â€¢ Length-D list of number of blocks used at each depth
â€¢ Length-D list of bools indicating whether self/cross-attention is used at a given depth
â€¢ Number of heads used for attention
The rest of the architecture is fixed.
B.2. Model specifications
The architectural parameters of the models used in this paper can be summarized by Table 9.
Table 9. Architectural parameters for the models used in this work.
Parameter Waveform generator Waveform cascader Spectrogram generator Spectrogram vocoder Super-resolution cascader
Base dimension 256 256 256 128 256
Kernel Size 7 7 9 5 7
Depth 6 5 5 4 4
Strides [4, 4, 4, 4, 4, 4] [4, 4, 4, 4, 4] [4, 3, 2, 2, 2] [2, 4, 4, 5] [4, 4, 4, 4]</p><h1 id=channels-256--1-1-2-3-4-4-256--1-1-2-3-4-256--1-2-2-3-4-128--1-1-2-4-256--1-1-2-2>Channels 256 * [1, 1, 2, 3, 4, 4] 256 * [1, 1, 2, 3, 4] 256 * [1, 2, 2, 3, 4] 128 * [1, 1, 2, 4] 256 * [1, 1, 2, 2]</h1><h1 id=blocks-3-3-3-4-4-4-3-3-4-6-6-6-6-6-6-6-4-4-4-4-4-4-4-4>Blocks [3, 3, 3, 4, 4, 4] [3, 3, 4, 6, 6] [6, 6, 6, 6, 6] [4, 4, 4, 4] [4, 4, 4, 4]</h1><p>Self Attention [F, F, F, T, T, T] [F, F, F, F, F] [F, F, F, T, T] [F, F, F, F] [F, F, F, F]
Cross Attention [F, F, F, T, T, T] [F, F, F, T, T] [F, F, F, T, T] [F, F, F, F] [F, F, F, F]
Attention Heads 8 8 8 - -
C. Prompt template to prime LaMDA model to generate music descriptive text
â€œWalking on Sunshineâ€ by Katrina & The Waves : The song is a pop / rock song. It has a happy, upbeat mood, with a driving
bassline and a simple, repetitive drumbeat. The song is backed by a synthesizer and a guitar.
â€œBorn This Wayâ€ by Lady Gaga : The electropop song is backed by rumbling synth sounds, a humming bass and additional
chorus percussion, with sole organ toward the end. The female singer has a confident and strong voice.
Noise2Music
â€œTill I Collapseâ€ by Eminem : The song is a hip hop song. It has a strong, aggressive mood, with a powerful, energetic beat.
â€œMy Heart Will Go Onâ€ by Celine Dion : The pop song has a sad and heartbroken mood. It contains heavy emphasis on the
instrumental arranging. Usage of Tin Whistle is prominent, backed by melodic use of strings and rhythm guitars. The song
features both acoustic and electronic instrumentation. The female singerâ€™s vocal performance is emotional demanding.
â€œMe Tooâ€ by Meghan Trainor : The pop song is a mid-tempo pop song. It has a happy, upbeat mood, with a driving bassline
and a simple, repetitive drumbeat. The song is backed by a synthesizer and a guitar.
â€œNuvole Biancheâ€ by Einaudi : The song is a piano-based classical piece. It has a gentle, melancholy mood, with a soothing,
slow pace.
â€œDonâ€™t Stop Me Nowâ€ by Queen : The energetic rock song builds on a piano, bass guitar, and drums. The singers are excited,
ready to go, and uplifting.
â€œStrawberry Swingâ€ by Coldplay : The alternative/indie song contains influences from afro-pop and highlife music, and
is built around finger-picked, distortion-free guitars with a heavy bassline and psychedelic synths. Itâ€™s a mid-tempo track,
featuring echoing guitars, piano ballad-inspired melodies and bittersweet, anthemic falsetto vocals.
â€œMad Worldâ€ by Gary Jules : The new wave / synth-pop song is backing its male singer with only a set of piano chords, a
mellotron imitating a cello, very light touches of electric piano, and modest use of a vocoder on the chorus.
â€œA Change is Gonna Comeâ€ by Sam Cooke : The soul / R&amp;B song has a reflective, nostalgic mood. The male singerâ€™s voice
is clearly in the foreground, backed by horns, strings, and the timpani carrying the bridge. The French horn conveys a sense
of melancholy.
â€œSmells Like Teen Spiritâ€ by Nirvana : The alternative rock/hard rock song has quiet verses with wobbly, chorused guitar,
followed by big, loud hardcore-inspired choruses. The overall mood is rebellious and pumped up. The guitar chords are
double tracked to create a more powerful sound.
â€œStrawberry Fields Foreverâ€ by The Beatles : The psychadelic rock song features a reverse-recorded instrumentation,
Mellotron flute sounds, an Indian swarmandal, tape loops and a fade-out/fade-in coda, as well as a cello and brass
arrangement. The vocals are slightly dissonant adding a bittersweet and ominous quality.
{title} by {artist} :
D. AudioSet music labels
AudioSet labels are licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.
/m/0z9c,/m/0mkg,/m/042v gx,/m/0fd3y,/t/dd00036,/m/025td0t,/m/0192l,/m/018j2,/m/0bm02,/m/018vs,/m/02cz 7,/m/0395lw,
/m/0gg8l,/m/0155w,/m/0l14 3,/m/01kcd,/m/015vgc,/m/01xqw,/m/02bk07,/m/0l14jd,/m/02mscn,/m/0140xf,/m/01wy6,/m/0ggq0m,
/m/01lyv,/m/0239kh,/m/01qbl,/m/0ggx5q,/m/02bxd,/m/026z9,/m/02fsn,/m/0283d,/m/02hnl,/m/02k mr,/m/026t6,/m/07s72n,
/m/02sgy,/m/08cyft,/m/02lkt,/m/03xq f,/m/0m0jc,/t/dd00035,/m/0326g,/m/0l14j ,/m/02w4v,/m/0319l,/m/02x8m,/t/dd00032,
/m/0dwtp,/m/0mbct,/m/0dls3,/m/0342h,/m/03gvt,/t/dd00031,/m/03qjg,/m/03m5k,/m/03q5t,/m/03lty,/m/0glt670,/m/03mb9,
/m/05rwpb,/m/03 d0,/m/03r5q ,/m/05148p4,/m/07pkxdp,/m/0j45pbj,/m/04rzd,/m/0dwsp,/m/06j64v,/m/05fw6t,/m/0164x2,
/m/028sqc,/m/0dq0md,/m/0g293,/m/02v2lh,/m/05pd6,/m/013y1f,/m/0l14md,/m/05r5c,/m/0fx80y,/m/064t9,/m/0dl5d,/m/05w3f,
/m/05r6t,/m/05r5wn,/m/06cqb,/m/06j6l,/m/03t3fj,/m/07sbbz2,/m/06by7,/t/dd00033,/m/0ln16,/m/06ncr,/t/dd00037,
/m/01hgjl,/m/0l14l2,/m/0l14t7,/m/0jtg0,/m/06rqw,/m/06rvn,/m/0gywn,/m/0l14gg,/m/06w87,/m/0l156b,/m/02qmj0d,
/m/07s0s5r,/m/015y n,/m/0l14qv,/m/01p970,/m/07brj,/m/01glhc,/m/07gxw,/t/dd00034,/m/02cjck,/m/07kc ,/m/011k j,
/m/02p0sh1,/m/07lnk,/m/07c6l,/m/07gql,/m/016622,/m/07xzm,/m/0dwt5,/m/01z7dr,/m/07y 7,/m/0y4f8,/m/04wptg,/m/085jw,
/m/01sm1g,/m/01bns</p><p>è¿™ç¯‡è®ºæ–‡çš„ä¸»è¦è´¡çŒ®</p><hr><p><strong>Cursor</strong></p><p>è¿™ç¯‡è®ºæ–‡ã€ŠNoise2Music: Text-conditioned Music Generation with Diffusion Modelsã€‹çš„<strong>ä¸»è¦è´¡çŒ®</strong>å¯ä»¥æ€»ç»“ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š</p><hr><h3 id=1-æå‡ºäº†noise2musicç³»ç»ŸåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬æ¡ä»¶éŸ³ä¹ç”Ÿæˆ>1. <strong>æå‡ºäº†Noise2Musicç³»ç»Ÿï¼šåŸºäºæ‰©æ•£æ¨¡å‹çš„æ–‡æœ¬æ¡ä»¶éŸ³ä¹ç”Ÿæˆ</strong></h3><ul><li>è¯¥ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºï¼ˆtext promptï¼‰ç”Ÿæˆé«˜è´¨é‡ã€é•¿è¾¾30ç§’çš„éŸ³ä¹ç‰‡æ®µã€‚</li><li>é‡‡ç”¨äº†<strong>çº§è”æ‰©æ•£æ¨¡å‹ï¼ˆcascaded diffusion modelsï¼‰</strong>ï¼Œåˆ†é˜¶æ®µç”ŸæˆéŸ³ä¹ï¼Œæå‡äº†éŸ³é¢‘è´¨é‡å’Œå¯æ§æ€§ã€‚</li></ul><hr><h3 id=2-åˆ›æ–°çš„ä¸¤é˜¶æ®µç”Ÿæˆæµç¨‹>2. <strong>åˆ›æ–°çš„ä¸¤é˜¶æ®µç”Ÿæˆæµç¨‹</strong></h3><ul><li><strong>ç”Ÿæˆå™¨æ¨¡å‹ï¼ˆGenerator Modelï¼‰</strong>ï¼šé¦–å…ˆæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆä¸­é—´è¡¨ç¤ºï¼ˆå¦‚ä½ä¿çœŸéŸ³é¢‘æˆ–log-melè°±å›¾ï¼‰ã€‚</li><li><strong>çº§è”æ¨¡å‹ï¼ˆCascader Modelï¼‰</strong>ï¼šå†æ ¹æ®ä¸­é—´è¡¨ç¤ºï¼ˆå’Œå¯é€‰çš„æ–‡æœ¬ï¼‰ç”Ÿæˆé«˜ä¿çœŸéŸ³é¢‘ã€‚</li><li>æœ€åé€šè¿‡<strong>è¶…åˆ†è¾¨ç‡æ¨¡å‹</strong>å°†éŸ³é¢‘æå‡åˆ°24kHzé«˜è´¨é‡ã€‚</li></ul><hr><h3 id=3-å¤šç§ä¸­é—´è¡¨ç¤ºçš„æ¢ç´¢>3. <strong>å¤šç§ä¸­é—´è¡¨ç¤ºçš„æ¢ç´¢</strong></h3><ul><li>æ¢ç´¢äº†ä¸¤ç§ä¸­é—´è¡¨ç¤ºæ–¹å¼ï¼š<strong>ä½ä¿çœŸéŸ³é¢‘</strong>å’Œ<strong>log-melè°±å›¾</strong>ï¼Œå¹¶æ¯”è¾ƒäº†å„è‡ªçš„ä¼˜ç¼ºç‚¹ã€‚</li></ul><hr><h3 id=4-å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„è‡ªåŠ¨æ„å»º>4. <strong>å¤§è§„æ¨¡é«˜è´¨é‡è®­ç»ƒæ•°æ®çš„è‡ªåŠ¨æ„å»º</strong></h3><ul><li>åˆ©ç”¨<strong>å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚LaMDAï¼‰<strong>å’Œ</strong>éŸ³ä¹-æ–‡æœ¬è”åˆåµŒå…¥æ¨¡å‹ï¼ˆMuLanï¼‰</strong>ï¼Œè‡ªåŠ¨ä¸ºå¤§é‡éŸ³ä¹ç‰‡æ®µç”Ÿæˆé«˜è´¨é‡ã€ç»†ç²’åº¦çš„æ–‡æœ¬æè¿°ï¼ˆpseudo-labelingï¼‰ï¼Œæ„å»ºäº†çº¦15ä¸‡å°æ—¶çš„éŸ³ä¹-æ–‡æœ¬å¯¹æ•°æ®é›†ã€‚</li><li>è¿˜å‘å¸ƒäº†<strong>MuLaMCap</strong>æ•°æ®é›†ï¼ŒåŒ…å«40ä¸‡å¯¹éŸ³ä¹-æ–‡æœ¬å¯¹ï¼Œä¸°å¯Œäº†éŸ³ä¹ç”Ÿæˆå’Œç†è§£é¢†åŸŸçš„æ•°æ®èµ„æºã€‚</li></ul><hr><h3 id=5-æ¨¡å‹æ¶æ„ä¸æŠ€æœ¯åˆ›æ–°>5. <strong>æ¨¡å‹æ¶æ„ä¸æŠ€æœ¯åˆ›æ–°</strong></h3><ul><li>é‡‡ç”¨<strong>1D Efficient U-Net</strong>ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„ä¸»å¹²ï¼Œé€‚é…éŸ³é¢‘åºåˆ—å»ºæ¨¡ã€‚</li><li>ä½¿ç”¨<strong>cross-attention</strong>æœºåˆ¶ï¼Œå°†æ–‡æœ¬æç¤ºä¿¡æ¯æœ‰æ•ˆæ³¨å…¥åˆ°æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li><li>å¼•å…¥<strong>classifier-free guidance</strong>æå‡æ–‡æœ¬ä¸ç”ŸæˆéŸ³ä¹çš„è¯­ä¹‰å¯¹é½ã€‚</li></ul><hr><h3 id=6-ç³»ç»Ÿæ€§è¯„æµ‹ä¸å¯¹æ¯”>6. <strong>ç³»ç»Ÿæ€§è¯„æµ‹ä¸å¯¹æ¯”</strong></h3><ul><li>é‡‡ç”¨<strong>Frechet Audio Distance (FAD)</strong> å’Œ <strong>MuLanç›¸ä¼¼åº¦åˆ†æ•°</strong>ç­‰æŒ‡æ ‡ï¼Œç³»ç»Ÿè¯„æµ‹äº†ç”ŸæˆéŸ³ä¹çš„éŸ³è´¨å’Œæ–‡æœ¬-éŸ³é¢‘å¯¹é½åº¦ã€‚</li><li>ä¸Riffusionã€Mubertã€MusicLMç­‰ä¸»æµæ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ï¼Œå±•ç¤ºäº†Noise2Musicçš„ä¼˜è¶Šæ€§ã€‚</li><li>è¿›è¡Œäº†å¤§è§„æ¨¡çš„äººå·¥å¬æ„Ÿæµ‹è¯•ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†æ¨¡å‹çš„è¡¨ç°ã€‚</li></ul><hr><h3 id=7-å¯¹éŸ³ä¹ç”Ÿæˆé¢†åŸŸçš„æ¨åŠ¨>7. <strong>å¯¹éŸ³ä¹ç”Ÿæˆé¢†åŸŸçš„æ¨åŠ¨</strong></h3><ul><li>è¯æ˜äº†æ‰©æ•£æ¨¡å‹ä¸ä»…èƒ½æ§åˆ¶éŸ³ä¹çš„åŸºæœ¬å±æ€§ï¼ˆå¦‚é£æ ¼ã€ä¹å™¨ã€èŠ‚å¥ï¼‰ï¼Œè¿˜èƒ½æ•æ‰æ›´ç»†è‡´çš„è¯­ä¹‰ï¼ˆå¦‚æ°›å›´ã€æƒ…æ„Ÿã€æ´»åŠ¨ç­‰ï¼‰ã€‚</li><li>ä¸ºåç»­éŸ³ä¹ç”Ÿæˆã€éŸ³ä¹æ£€ç´¢ã€éŸ³ä¹æ ‡æ³¨ç­‰ä»»åŠ¡æä¾›äº†æ–°çš„æ–¹æ³•å’Œæ•°æ®åŸºç¡€ã€‚</li></ul><hr><h2 id=æ€»ç»“ä¸€å¥è¯-1>æ€»ç»“ä¸€å¥è¯</h2><p><strong>Noise2Music</strong>é¦–æ¬¡å°†çº§è”æ‰©æ•£æ¨¡å‹å¤§è§„æ¨¡åº”ç”¨äºæ–‡æœ¬æ¡ä»¶éŸ³ä¹ç”Ÿæˆï¼Œæå‡ºäº†é«˜æ•ˆçš„æ•°æ®è‡ªåŠ¨æ ‡æ³¨æ–¹æ³•ï¼Œæ˜¾è‘—æå‡äº†ç”ŸæˆéŸ³ä¹çš„è´¨é‡å’Œæ–‡æœ¬å¯¹é½åº¦ï¼Œå¹¶ä¸ºç¤¾åŒºè´¡çŒ®äº†å¤§è§„æ¨¡éŸ³ä¹-æ–‡æœ¬æ•°æ®é›†å’Œç³»ç»Ÿæ€§è¯„æµ‹ã€‚</p><p>å¦‚éœ€æ›´è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚æˆ–æŸä¸€éƒ¨åˆ†çš„æ·±å…¥è§£è¯»ï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼</p><hr><p><strong>User</strong></p><p>è®­ç»ƒéœ€è¦å¤šå°‘ç®—åŠ›ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>è®ºæ–‡ä¸­<strong>æ²¡æœ‰ç›´æ¥ç»™å‡ºè®­ç»ƒNoise2Musicæ‰€éœ€çš„æ€»ç®—åŠ›ï¼ˆå¦‚GPU/TPUæ•°é‡ã€è®­ç»ƒæ€»æ—¶é•¿ç­‰ï¼‰</strong>ï¼Œä½†æ ¹æ®æ–‡ä¸­ç»†èŠ‚å’Œç±»ä¼¼å¤§æ¨¡å‹çš„ç»éªŒï¼Œå¯ä»¥åšå‡ºè¾ƒä¸ºåˆç†çš„æ¨æµ‹å’Œåˆ†æï¼š</p><hr><h2 id=è®ºæ–‡ä¸­ç›¸å…³ä¿¡æ¯>è®ºæ–‡ä¸­ç›¸å…³ä¿¡æ¯</h2><ol><li><strong>æ¨¡å‹è§„æ¨¡</strong><ul><li>ä¸»ä½“æ¨¡å‹å‚æ•°é‡åœ¨<strong>æ•°äº¿çº§åˆ«</strong>ï¼ˆå¦‚Waveform generator 7.2äº¿å‚æ•°ï¼ŒSpectrogram generator 7.4äº¿å‚æ•°ï¼‰ã€‚</li></ul></li><li><strong>æ•°æ®è§„æ¨¡</strong><ul><li>è®­ç»ƒæ•°æ®çº¦<strong>15ä¸‡å°æ—¶</strong>çš„éŸ³ä¹éŸ³é¢‘ï¼Œè‡ªåŠ¨æ ‡æ³¨æ–‡æœ¬æè¿°ã€‚</li><li>éŸ³é¢‘é‡‡æ ·ç‡æœ€é«˜24kHzï¼Œç‰‡æ®µé•¿åº¦30ç§’ã€‚</li></ul></li><li><strong>è®­ç»ƒç»†èŠ‚</strong><ul><li>æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰ï¼š2048ï¼ˆsuper-resolutionæ¨¡å‹ä¸º4096ï¼‰ã€‚</li><li>ä¼˜åŒ–å™¨ï¼šAdamï¼Œcosineå­¦ä¹ ç‡è°ƒåº¦ï¼Œè®­ç»ƒæ­¥æ•°æœ€é«˜250ä¸‡æ­¥ã€‚</li><li>è®­ç»ƒå’Œæ¨ç†å‡åœ¨<strong>Google Cloud TPU v4</strong>ä¸Šå®Œæˆï¼Œæ¨ç†æ—¶æ¯æ¬¡ç”¨4å—TPU v4å¹¶è¡Œã€‚</li></ul></li></ol><hr><h2 id=ç»éªŒæ¨ç®—>ç»éªŒæ¨ç®—</h2><ul><li><strong>æ•°æ®é‡å·¨å¤§</strong>ï¼š15ä¸‡å°æ—¶éŸ³é¢‘ï¼Œå‡è®¾æ¯ä¸ªæ ·æœ¬30ç§’ï¼Œçº¦1800ä¸‡ä¸ªæ ·æœ¬ã€‚</li><li><strong>å¤§batch + é«˜é‡‡æ ·ç‡ + é•¿åºåˆ—</strong>ï¼šå¯¹å†…å­˜å’Œå¸¦å®½è¦æ±‚æé«˜ã€‚</li><li><strong>æ¨¡å‹å‚æ•°é‡</strong>ï¼šå•æ¨¡å‹7äº¿å‚æ•°ï¼Œä¸”æœ‰å¤šä¸ªæ¨¡å‹ï¼ˆç”Ÿæˆå™¨ã€cascaderã€vocoderç­‰ï¼‰ã€‚</li></ul><h3 id=å‚è€ƒç±»ä¼¼é¡¹ç›®>å‚è€ƒç±»ä¼¼é¡¹ç›®</h3><ul><li><strong>MusicLM</strong>ï¼ˆGoogleï¼Œ2023ï¼‰ï¼šè®­ç»ƒç”¨åˆ°<strong>æ•°ç™¾åˆ°ä¸Šåƒå—TPU v4</strong>ï¼Œè®­ç»ƒæ—¶é—´æ•°å‘¨ã€‚</li><li><strong>Imagen/Stable Diffusion</strong>ç­‰å¤§æ¨¡å‹ï¼šé€šå¸¸éœ€è¦æ•°ååˆ°ä¸Šç™¾å—A100/TPU v4ï¼Œè®­ç»ƒæ—¶é—´1-2å‘¨ã€‚</li></ul><hr><h2 id=ç²—ç•¥ä¼°ç®—>ç²—ç•¥ä¼°ç®—</h2><ul><li><strong>å•æ¨¡å‹è®­ç»ƒ</strong>ï¼šå¦‚æœç”¨8-16å—TPU v4è®­ç»ƒä¸€ä¸ª7äº¿å‚æ•°çš„æ‰©æ•£æ¨¡å‹ï¼Œbatch size 2048ï¼Œè®­ç»ƒ250ä¸‡æ­¥ï¼Œé€šå¸¸éœ€è¦<strong>å‡ å¤©åˆ°ä¸€å‘¨</strong>ã€‚</li><li><strong>å…¨æµç¨‹è®­ç»ƒ</strong>ï¼ˆå¤šæ¨¡å‹+å¤§æ•°æ®ï¼‰ï¼šå®é™…éœ€è¦<strong>å‡ ååˆ°ä¸Šç™¾å—TPU v4</strong>ï¼Œè®­ç»ƒæ—¶é—´å¯èƒ½åœ¨<strong>1-2å‘¨</strong>ç”šè‡³æ›´ä¹…ã€‚</li><li><strong>å­˜å‚¨éœ€æ±‚</strong>ï¼šåŸå§‹éŸ³é¢‘+ä¸­é—´è¡¨ç¤º+æ¨¡å‹æƒé‡ï¼Œè‡³å°‘æ•°åTBã€‚</li></ul><hr><h2 id=è®ºæ–‡åŸæ–‡ç›¸å…³æè¿°>è®ºæ–‡åŸæ–‡ç›¸å…³æè¿°</h2><blockquote><p>We serve the models on Google Cloud TPU V4, where each service request generates four 30-second music clips. We apply GSPMD (Xu et al., 2021) to partition the model on four TPU V4 devices, reducing the serving time by more than 50%.</p></blockquote><hr><h2 id=æ€»ç»“-2>æ€»ç»“</h2><ul><li><strong>è®­ç»ƒNoise2Musicéœ€è¦çš„ç®—åŠ›éå¸¸å¤§</strong>ï¼Œå¤§è‡´é‡çº§ä¸º<strong>å‡ ååˆ°ä¸Šç™¾å—TPU v4</strong>ï¼ˆæˆ–ç­‰æ•ˆA100 GPUï¼‰ï¼Œè®­ç»ƒæ—¶é—´<strong>1-2å‘¨</strong>ç”šè‡³æ›´ä¹…ã€‚</li><li>å…·ä½“æ¶ˆè€—å–å†³äºå®é™…å¹¶è¡Œåº¦ã€æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è°ƒä¼˜ç­‰ç»†èŠ‚ã€‚</li><li>è¿™ç±»å¤§æ¨¡å‹è®­ç»ƒé€šå¸¸åªæœ‰å¤§å…¬å¸/ç ”ç©¶æœºæ„å…·å¤‡æ¡ä»¶å®Œæˆã€‚</li></ul><hr><p>å¦‚éœ€æ›´è¯¦ç»†çš„æ¨ç®—å…¬å¼æˆ–æƒ³äº†è§£å°è§„æ¨¡å¤ç°çš„å»ºè®®ï¼Œä¹Ÿå¯ä»¥ç»§ç»­é—®æˆ‘ï¼</p><hr><p><strong>User</strong></p><p>æ•°æ®é›†æ˜¯ä»€ä¹ˆï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>Noise2Music è®ºæ–‡ä¸­ç”¨åˆ°çš„æ•°æ®é›†ä¸»è¦åŒ…æ‹¬<strong>è‡ªå»ºçš„å¤§è§„æ¨¡éŸ³ä¹-æ–‡æœ¬å¯¹æ•°æ®é›†</strong>ï¼Œä»¥åŠç”¨äºè¯„æµ‹çš„å…¬å¼€æ•°æ®é›†ã€‚ä¸‹é¢è¯¦ç»†è¯´æ˜ï¼š</p><hr><h2 id=1-è®­ç»ƒæ•°æ®é›†è‡ªå»º>1. <strong>è®­ç»ƒæ•°æ®é›†ï¼ˆè‡ªå»ºï¼‰</strong></h2><h3 id=æ•°æ®æ¥æº><strong>æ•°æ®æ¥æº</strong></h3><ul><li><strong>åŸå§‹éŸ³é¢‘</strong>ï¼šæ”¶é›†äº†çº¦ 6,800,000 é¦–éŸ³ä¹éŸ³é¢‘æ–‡ä»¶ï¼ˆçº¦ 15 ä¸‡å°æ—¶ï¼‰ï¼Œæ¯é¦–éŸ³é¢‘åˆ‡åˆ†ä¸ºå¤šä¸ª 30 ç§’ç‰‡æ®µã€‚</li><li><strong>éŸ³é¢‘é‡‡æ ·ç‡</strong>ï¼š16kHzï¼ˆä¸»æ¨¡å‹ï¼‰ï¼Œ24kHzï¼ˆè¶…åˆ†è¾¨ç‡æ¨¡å‹ï¼‰ã€‚</li></ul><h3 id=æ–‡æœ¬æ ‡ç­¾æè¿°å¦‚ä½•è·å¾—><strong>æ–‡æœ¬æ ‡ç­¾ï¼ˆæè¿°ï¼‰å¦‚ä½•è·å¾—ï¼Ÿ</strong></h3><ul><li>ç”±äºå…¬å¼€çš„éŸ³ä¹-æ–‡æœ¬å¯¹æ•°æ®ç¨€ç¼ºï¼Œä½œè€…é‡‡ç”¨äº†**è‡ªåŠ¨ä¼ªæ ‡ç­¾ï¼ˆpseudo-labelingï¼‰**æ–¹æ³•ï¼š<ol><li><strong>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLaMDAï¼‰</strong>ï¼šç”Ÿæˆå¤§é‡éŸ³ä¹æè¿°æ€§æ–‡æœ¬ï¼ˆå¦‚é£æ ¼ã€æƒ…æ„Ÿã€ä¹å™¨ã€åœºæ™¯ç­‰ï¼‰ã€‚</li><li><strong>MuLanï¼ˆéŸ³ä¹-æ–‡æœ¬è”åˆåµŒå…¥æ¨¡å‹ï¼‰</strong>ï¼šå°†æ¯ä¸ªéŸ³é¢‘ç‰‡æ®µä¸å€™é€‰æ–‡æœ¬æè¿°è¿›è¡ŒåŒ¹é…ï¼Œé€‰å‡ºæœ€ç›¸å…³çš„æè¿°ä½œä¸ºä¼ªæ ‡ç­¾ã€‚</li><li><strong>è¡¥å……æ ‡ç­¾</strong>ï¼šè¿˜åˆ©ç”¨äº†æ­Œæ›²å…ƒæ•°æ®ï¼ˆå¦‚æ­Œåã€è‰ºæœ¯å®¶ã€æµæ´¾ç­‰ï¼‰å’Œäººå·¥æ ‡æ³¨çš„çŸ­æ ‡ç­¾ã€‚</li></ol></li></ul><h3 id=æœ€ç»ˆæ•°æ®é›†è§„æ¨¡><strong>æœ€ç»ˆæ•°æ®é›†è§„æ¨¡</strong></h3><ul><li><strong>çº¦ 15 ä¸‡å°æ—¶</strong>çš„éŸ³ä¹éŸ³é¢‘ï¼Œæ¯ä¸ªç‰‡æ®µé…æœ‰å¤šä¸ªé«˜è´¨é‡æ–‡æœ¬æè¿°ã€‚</li><li>è®ºæ–‡è¿˜æ„å»ºäº†ä¸€ä¸ªåä¸º <strong>MuLaMCap</strong> çš„éŸ³ä¹-æ–‡æœ¬å¯¹æ•°æ®é›†ï¼ŒåŒ…å«çº¦ 40 ä¸‡å¯¹éŸ³ä¹-æ–‡æœ¬å¯¹ï¼ˆåŸºäº AudioSet éŸ³ä¹å­é›†è‡ªåŠ¨æ ‡æ³¨ï¼‰ã€‚</li></ul><hr><h2 id=2-è¯„æµ‹æ•°æ®é›†å…¬å¼€>2. <strong>è¯„æµ‹æ•°æ®é›†ï¼ˆå…¬å¼€ï¼‰</strong></h2><p>Noise2Music åœ¨ä»¥ä¸‹å…¬å¼€æ•°æ®é›†ä¸Šè¿›è¡Œè¯„æµ‹ï¼š</p><ol><li><strong>MusicCaps</strong><ul><li>ç”± Google å‘å¸ƒï¼ŒåŒ…å« 5,500 ä¸ª 10 ç§’éŸ³ä¹ç‰‡æ®µï¼Œæ¯ä¸ªç‰‡æ®µé…æœ‰äººå·¥å†™çš„è¯¦ç»†æ–‡æœ¬æè¿°ã€‚</li></ul></li><li><strong>AudioSet-Music-Eval</strong><ul><li>AudioSet çš„éŸ³ä¹å­é›†ï¼ŒåŒ…å« 1,482 ä¸ª 10 ç§’éŸ³ä¹ç‰‡æ®µï¼Œæ¯ä¸ªç‰‡æ®µé…æœ‰æ ‡ç­¾ï¼ˆå¦‚é£æ ¼ã€ä¹å™¨ç­‰ï¼‰ã€‚</li></ul></li><li><strong>MagnaTagATune (MTAT)</strong><ul><li>åŒ…å« 21,638 ä¸ªéŸ³ä¹ç‰‡æ®µï¼Œæ¯ä¸ªç‰‡æ®µé…æœ‰å¤šä¸ªéŸ³ä¹æ ‡ç­¾ï¼ˆå¦‚æµæ´¾ã€æƒ…æ„Ÿã€ä¹å™¨ç­‰ï¼‰ã€‚</li></ul></li></ol><hr><h2 id=3-mulamcap-æ•°æ®é›†è®ºæ–‡è´¡çŒ®>3. <strong>MuLaMCap æ•°æ®é›†ï¼ˆè®ºæ–‡è´¡çŒ®ï¼‰</strong></h2><ul><li><strong>MuLaMCap</strong> æ˜¯ä½œè€…åŸºäº AudioSet éŸ³ä¹å­é›†è‡ªåŠ¨æ ‡æ³¨å¾—åˆ°çš„éŸ³ä¹-æ–‡æœ¬å¯¹æ•°æ®é›†ã€‚</li><li>åŒ…å«çº¦ 40 ä¸‡å¯¹éŸ³ä¹ç‰‡æ®µå’Œæ–‡æœ¬æè¿°ï¼Œæ–‡æœ¬æè¿°æ¥è‡ªå¤§è¯­è¨€æ¨¡å‹å’Œäººå·¥æ ‡ç­¾ï¼Œè¯­ä¹‰ä¸°å¯Œã€ç»†ç²’åº¦ã€‚</li></ul><hr><h2 id=æ€»ç»“-3>æ€»ç»“</h2><ul><li><strong>è®­ç»ƒæ•°æ®</strong>ï¼šä¸»è¦æ˜¯è‡ªå»ºçš„å¤§è§„æ¨¡éŸ³ä¹éŸ³é¢‘+è‡ªåŠ¨ç”Ÿæˆçš„æ–‡æœ¬æè¿°ï¼ˆä¼ªæ ‡ç­¾ï¼‰ï¼Œçº¦ 15 ä¸‡å°æ—¶ï¼Œ680 ä¸‡é¦–éŸ³é¢‘ã€‚</li><li><strong>è¯„æµ‹æ•°æ®</strong>ï¼šMusicCapsã€AudioSet-Music-Evalã€MagnaTagATuneã€‚</li><li><strong>è´¡çŒ®æ•°æ®</strong>ï¼šMuLaMCapï¼ˆ40 ä¸‡å¯¹éŸ³ä¹-æ–‡æœ¬å¯¹ï¼Œå…¬å¼€å¯ç”¨ï¼‰ã€‚</li></ul><p>å¦‚éœ€äº†è§£ä¼ªæ ‡ç­¾ç”Ÿæˆæµç¨‹ã€æ•°æ®é¢„å¤„ç†ç»†èŠ‚æˆ–æƒ³è·å–å…¬å¼€æ•°æ®é›†é“¾æ¥ï¼Œå¯ä»¥ç»§ç»­é—®æˆ‘ï¼</p><hr><p><strong>User</strong></p><p>OpenMusicçš„ç°ä»£åŒ–å¼€æºæ›¿ä»£å“ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>ä½ é—®çš„â€œOpenMusicçš„ç°ä»£åŒ–å¼€æºæ›¿ä»£å“â€â€”â€”è¿™é‡Œçš„ <strong>OpenMusic</strong> é€šå¸¸æŒ‡çš„æ˜¯ç”± IRCAM å¼€å‘çš„<strong>å¯è§†åŒ–éŸ³ä¹ä½œæ›²ç¯å¢ƒ</strong>ï¼Œä¸»è¦ç”¨äºç®—æ³•ä½œæ›²ã€éŸ³ä¹ç†è®ºå®éªŒã€å£°éŸ³è®¾è®¡ç­‰ã€‚OpenMusic ä»¥å…¶å›¾å½¢åŒ–ç¼–ç¨‹å’Œçµæ´»çš„éŸ³ä¹æ•°æ®å¤„ç†è‘—ç§°ï¼Œä½†å®ƒçš„ç•Œé¢å’ŒæŠ€æœ¯æ ˆè¾ƒä¸ºè€æ—§ï¼Œä¸”ä¸»è¦é¢å‘ Mac å¹³å°ã€‚</p><p>å¦‚æœä½ åœ¨å¯»æ‰¾<strong>ç°ä»£åŒ–ã€å¼€æºã€è·¨å¹³å°</strong>çš„æ›¿ä»£å“ï¼Œä¸‹é¢æ˜¯ä¸€äº›ä¸»æµé€‰æ‹©ï¼Œæ¶µç›–äº†<strong>å¯è§†åŒ–ç¼–ç¨‹ã€ç®—æ³•ä½œæ›²ã€éŸ³ä¹ç”Ÿæˆã€MIDI/éŸ³é¢‘å¤„ç†</strong>ç­‰æ–¹å‘ï¼š</p><hr><h2 id=1-pure-data-pd>1. <strong>Pure Data (Pd)</strong></h2><ul><li><strong>ç®€ä»‹</strong>ï¼šæä¸ºæµè¡Œçš„å¼€æºå¯è§†åŒ–éŸ³é¢‘ç¼–ç¨‹ç¯å¢ƒï¼Œé€‚åˆå®æ—¶éŸ³é¢‘ã€MIDIã€ç®—æ³•ä½œæ›²ã€äº¤äº’è‰ºæœ¯ç­‰ã€‚</li><li><strong>ä¼˜ç‚¹</strong>ï¼šè·¨å¹³å°ï¼ˆWindows/Mac/Linuxï¼‰ã€ç¤¾åŒºæ´»è·ƒã€å¯æ‰©å±•æ€§å¼ºã€‚</li><li><strong>ç½‘å€</strong>ï¼š<a class=link href=https://puredata.info/ target=_blank rel=noopener>https://puredata.info/</a></li></ul><hr><h2 id=2-openmusic-om>2. <strong>OpenMusic++ (OM#)</strong></h2><ul><li><strong>ç®€ä»‹</strong>ï¼šOpenMusic çš„ç°ä»£åŒ–é‡å†™ç‰ˆï¼Œç›®æ ‡æ˜¯æˆä¸ºè·¨å¹³å°ã€ç°ä»£åŒ–çš„ OpenMusic æ›¿ä»£å“ã€‚</li><li><strong>ä¼˜ç‚¹</strong>ï¼šæ”¯æŒ Windows/Mac/Linuxï¼Œç•Œé¢æ›´ç°ä»£ï¼Œå…¼å®¹ OpenMusic çš„ patchã€‚</li><li><strong>ç½‘å€</strong>ï¼š<a class=link href=https://github.com/openmusic-project/om# target=_blank rel=noopener>https://github.com/openmusic-project/om#</a></li></ul><hr><h2 id=3-ossia-score>3. <strong>ossia score</strong></h2><ul><li><strong>ç®€ä»‹</strong>ï¼šé¢å‘å®æ—¶äº¤äº’å’Œå¤šåª’ä½“åœºæ™¯çš„å¼€æºåˆ›ä½œç¯å¢ƒï¼Œæ”¯æŒéŸ³é¢‘ã€MIDIã€OSCã€DMX ç­‰å¤šç§åè®®ã€‚</li><li><strong>ä¼˜ç‚¹</strong>ï¼šç°ä»£åŒ– UIï¼Œè·¨å¹³å°ï¼Œé€‚åˆå¤æ‚çš„äº¤äº’å¼éŸ³ä¹/å¤šåª’ä½“ä½œå“ã€‚</li><li><strong>ç½‘å€</strong>ï¼š<a class=link href=https://ossia.io/score/ target=_blank rel=noopener>https://ossia.io/score/</a></li></ul><hr><h2 id=4-supercollider>4. <strong>SuperCollider</strong></h2><ul><li><strong>ç®€ä»‹</strong>ï¼šå¼ºå¤§çš„éŸ³é¢‘åˆæˆå’Œç®—æ³•ä½œæ›²å¹³å°ï¼Œè™½ç„¶ä¸»è¦æ˜¯ä»£ç ç¼–ç¨‹ï¼Œä½†æœ‰ä¸°å¯Œçš„ç¤¾åŒºå’Œå¯è§†åŒ–å·¥å…·ï¼ˆå¦‚ <a class=link href=https://tidalcycles.org/ target=_blank rel=noopener>tidalcycles.org</a>ï¼‰ã€‚</li><li><strong>ä¼˜ç‚¹</strong>ï¼šæå¼ºçš„å®æ—¶éŸ³é¢‘å¤„ç†èƒ½åŠ›ï¼Œè·¨å¹³å°ï¼Œç¤¾åŒºæ´»è·ƒã€‚</li><li><strong>ç½‘å€</strong>ï¼š<a class=link href=https://supercollider.github.io/ target=_blank rel=noopener>https://supercollider.github.io/</a></li></ul><hr><h2 id=5-nodebox>5. <strong>NodeBox</strong></h2><ul><li><strong>ç®€ä»‹</strong>ï¼šé¢å‘åˆ›æ„ç¼–ç çš„å¯è§†åŒ–ç¼–ç¨‹ç¯å¢ƒï¼Œæ”¯æŒéŸ³ä¹ã€å›¾å½¢ã€åŠ¨ç”»ç­‰ç”Ÿæˆã€‚</li><li><strong>ä¼˜ç‚¹</strong>ï¼šç°ä»£åŒ–ã€æ˜“ç”¨ï¼Œé€‚åˆè‰ºæœ¯å®¶å’Œè®¾è®¡å¸ˆã€‚</li><li><strong>ç½‘å€</strong>ï¼š<a class=link href=https://www.nodebox.net/ target=_blank rel=noopener>https://www.nodebox.net/</a></li></ul><hr><h2 id=6-lilypond--pythonmusic21>6. <strong>LilyPond + Python/Music21</strong></h2><ul><li><strong>ç®€ä»‹</strong>ï¼šLilyPond ç”¨äºä¹è°±æ’ç‰ˆï¼ŒMusic21 æ˜¯ MIT å¼€å‘çš„éŸ³ä¹åˆ†æä¸ç”Ÿæˆ Python åº“ï¼Œé€‚åˆç®—æ³•ä½œæ›²å’ŒéŸ³ä¹å­¦ç ”ç©¶ã€‚</li><li><strong>ä¼˜ç‚¹</strong>ï¼šå®Œå…¨å¼€æºï¼ŒPython ç”Ÿæ€ï¼Œé€‚åˆæ‰¹é‡å¤„ç†å’Œè‡ªåŠ¨åŒ–ã€‚</li><li><strong>ç½‘å€</strong>ï¼š<a class=link href=http://lilypond.org/ target=_blank rel=noopener>http://lilypond.org/</a>, <a class=link href=https://web.mit.edu/music21/ target=_blank rel=noopener>https://web.mit.edu/music21/</a></li></ul><hr><h2 id=7-orca>7. <strong>Orca</strong></h2><ul><li><strong>ç®€ä»‹</strong>ï¼šæç®€ã€åˆ›æ„å‹çš„æ–‡æœ¬å¼éŸ³ä¹ç¼–ç¨‹ç¯å¢ƒï¼Œé€‚åˆ live coding å’Œå®éªŒéŸ³ä¹ã€‚</li><li><strong>ä¼˜ç‚¹</strong>ï¼šæç®€ã€è·¨å¹³å°ã€ç¤¾åŒºæ´»è·ƒã€‚</li><li><strong>ç½‘å€</strong>ï¼š<a class=link href=https://github.com/hundredrabbits/Orca target=_blank rel=noopener>https://github.com/hundredrabbits/Orca</a></li></ul><hr><h2 id=8-inscore>8. <strong>INScore</strong></h2><ul><li><strong>ç®€ä»‹</strong>ï¼šä¸“æ³¨äºäº¤äº’å¼ä¹è°±å’Œå¤šåª’ä½“å±•ç¤ºçš„å¼€æºå¹³å°ï¼Œé€‚åˆå®éªŒæ€§éŸ³ä¹å’Œå¯è§†åŒ–ã€‚</li><li><strong>ç½‘å€</strong>ï¼š<a class=link href=https://inscore.sourceforge.io/ target=_blank rel=noopener>https://inscore.sourceforge.io/</a></li></ul><hr><h2 id=æ€»ç»“-4>æ€»ç»“</h2><ul><li><strong>å¦‚æœä½ æƒ³è¦å’Œ OpenMusic æœ€æ¥è¿‘çš„ä½“éªŒ</strong>ï¼Œæ¨èå…³æ³¨ <strong>OpenMusic++ (OM#)</strong>ã€‚</li><li><strong>å¦‚æœæ›´æ³¨é‡å®æ—¶éŸ³é¢‘å’Œäº¤äº’</strong>ï¼Œ<strong>Pure Data</strong> å’Œ <strong>ossia score</strong> æ˜¯å¾ˆå¥½çš„é€‰æ‹©ã€‚</li><li><strong>å¦‚æœå–œæ¬¢ä»£ç å’Œç®—æ³•ä½œæ›²</strong>ï¼Œ<strong>SuperCollider</strong>ã€<strong>Music21</strong>ã€<strong>Orca</strong> éƒ½å€¼å¾—å°è¯•ã€‚</li></ul><p>å¦‚éœ€æŸä¸ªå·¥å…·çš„è¯¦ç»†ä»‹ç»ã€å¯¹æ¯”ã€æˆ–å…¥é—¨æ•™ç¨‹ï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼</p><hr><p><strong>User</strong></p><p>Skip to main content
What type of model architecture is best suited for generative music? [D] : r/MachineLearning</p><p>r/MachineLearning
Current search is within r/MachineLearning</p><p>Remove r/MachineLearning filter and expand search to all of Reddit
Search in r/MachineLearning
Advertise on Reddit</p><p>Open chat
Create
Create post
Open inbox</p><p>User Avatar
Expand user menu
Skip to NavigationSkip to Right Sidebar</p><p>Back
r/MachineLearning icon
Go to MachineLearning
r/MachineLearning
â€¢
10 mo. ago
redditTee123</p><p>What type of model architecture is best suited for generative music? [D]
Discussion
In a previous life before $$$ became a necessity & I started software, I was a musician. I played guitar, piano, drums, have a solid understanding of music theory, and produced a few of my own tracks.</p><p>Not that much of that likely matters for building my own generative music model, but it definitely means my interest is there :)</p><p>Would anyone have recs where to start, specifically what type of model architecture? GPT is suggesting RNNs, Transformers, or VAEs. Has anyone experimented with various models & could give some guidance on architecture selection + how to process data appropriately?</p><p>Upvote
20</p><p>Downvote</p><p>7
Go to comments</p><p>Share
Share
Join the conversation
Sort by:</p><p>Best</p><p>Search Comments
Expand comment search
Comments Section
not_particulary
â€¢
10mo ago
Start with a transformer, maybe on midi files?</p><p>Upvote
8</p><p>Downvote</p><p>Reply
reply</p><p>Award</p><p>Share
Share</p><p>mvreich
â€¢
10mo ago
Look into diffusion models and audio diffusion. Just off the top off my head, it should be very easy to train a text/lyric-to-music generative model following the generalized approach in Rombach&rsquo;s LDM paper. KL-regularized audio autoencoder + CLAP prompt embedding + transformer-based diffusion model.</p><p>Upvote
5</p><p>Downvote</p><p>Reply
reply</p><p>Award</p><p>Share
Share</p><p>[deleted]
â€¢
10mo ago
u/AsianMusicProgrammer avatar
AsianMusicProgrammer
â€¢
10mo ago
I&rsquo;d recommend checking out MidiTok for getting started with Transformed-based approaches</p><p>Upvote
4</p><p>Downvote</p><p>Reply
reply</p><p>Award</p><p>Share
Share</p><p>u/veganshakzuka avatar
veganshakzuka
â€¢
10mo ago
I&rsquo;d look into Jamba.</p><p>Upvote
2</p><p>Downvote</p><p>Reply
reply</p><p>Award</p><p>Share
Share</p><p>Karioth1
â€¢
10mo ago
Hardest part will be securing enough compute to train. But I would go on Google scholar and just search â€œmusic generation transformerâ€ and see whatâ€™s there. I would probably go for a symbolic approach train on midi-like tokens. But there are audio to audio models as well.</p><p>Upvote
2</p><p>Downvote</p><p>Reply
reply</p><p>Award</p><p>Share
Share</p><p>Community Info Section
r/MachineLearning
Join
Machine Learning
Beginners -> /r/mlquestions or /r/learnmachinelearning , AGI -> /r/singularity, career advices -> /r/cscareerquestions, datasets -> r/datasets</p><p>Show more
Created Jul 29, 2009
Public
3M
Members
245
Online
Top 1%
Rank by size
Rules
1
No Spam
2
No Self-Promotion
3
No Marketing Campaigns (SEO)
4
No Disrespectful Behavior
5
No arXiv Links without Body Text
6
No Low-Effort, Beginner Questions
Recommended Subreddits
r/MLQuestions icon
r/MLQuestions
78,080 members
r/learnmachinelearning
524,379 members
r/reinforcementlearning
62,154 members
r/deeplearning
196,806 members
r/ArtificialInteligence icon
r/ArtificialInteligence
1,497,748 members
r/artificial icon
r/artificial
1,102,093 members
r/computervision icon
r/computervision
118,778 members
r/Automate
143,899 members
r/OpenAI icon
r/OpenAI
2,378,821 members
r/Anthropic icon
r/Anthropic
22,511 members
Moderators
Message Mods
u/kunjaan avatar
u/kunjaan
u/cavedave
Mod to the stars
u/MTGTraner avatar
u/MTGTraner
HD Hlynsson
u/hardmaru avatar
u/hardmaru
Google Brain
u/programmerChilli avatar
u/programmerChilli
Researcher
u/AutoModerator avatar
u/AutoModerator
u/olaf_nij avatar
u/olaf_nij
u/BeatLeJuce
Researcher
View all moderators
Reddit Rules
Privacy Policy
User Agreement
Reddit, Inc. Â© 2025. All rights reserved.</p><p>Collapse Navigation</p><p>è®¨è®ºäº†ä»€ä¹ˆï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>ä½ é—®çš„è¿™ä¸ª Reddit è®¨è®ºå¸–ä¸»è¦å›´ç»•**â€œå“ªç§æ¨¡å‹æ¶æ„æœ€é€‚åˆç”ŸæˆéŸ³ä¹ï¼ˆgenerative musicï¼‰â€**å±•å¼€ï¼Œä¸‹é¢æ˜¯è®¨è®ºçš„ä¸»è¦å†…å®¹æ€»ç»“ï¼š</p><hr><h2 id=1-ä¸»æµå»ºè®®transformer>1. <strong>ä¸»æµå»ºè®®ï¼šTransformer</strong></h2><ul><li>å¤šä½ç”¨æˆ·å»ºè®®<strong>ä» Transformer æ¶æ„å…¥æ‰‹</strong>ï¼Œå°¤å…¶æ˜¯åœ¨ MIDIï¼ˆç¬¦å·éŸ³ä¹ï¼‰æ•°æ®ä¸Šè®­ç»ƒã€‚</li><li>Transformer åœ¨åºåˆ—å»ºæ¨¡ï¼ˆå¦‚æ–‡æœ¬ã€MIDIéŸ³ç¬¦åºåˆ—ï¼‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå·²æˆä¸ºéŸ³ä¹ç”Ÿæˆé¢†åŸŸçš„ä¸»æµé€‰æ‹©ã€‚</li></ul><hr><h2 id=2-æ‰©æ•£æ¨¡å‹diffusion-models>2. <strong>æ‰©æ•£æ¨¡å‹ï¼ˆDiffusion Modelsï¼‰</strong></h2><ul><li>æœ‰äººæ¨èå…³æ³¨<strong>æ‰©æ•£æ¨¡å‹ï¼ˆdiffusion modelsï¼‰<strong>å’Œ</strong>éŸ³é¢‘æ‰©æ•£ï¼ˆaudio diffusionï¼‰</strong>ï¼Œå°¤å…¶æ˜¯ç»“åˆè‡ªç¼–ç å™¨ï¼ˆautoencoderï¼‰å’Œæ–‡æœ¬/éŸ³é¢‘è”åˆåµŒå…¥ï¼ˆå¦‚ CLAPï¼‰ã€‚</li><li>è¿™ç§æ–¹æ³•é€‚åˆåš<strong>æ–‡æœ¬åˆ°éŸ³ä¹</strong>çš„ç”Ÿæˆï¼ˆtext-to-musicï¼‰ï¼Œå¹¶ä¸”å¯ä»¥å€Ÿé‰´å›¾åƒé¢†åŸŸçš„ latent diffusion modelï¼ˆLDMï¼‰æ€è·¯ã€‚</li></ul><hr><h2 id=3-å…¶ä»–æ¶æ„>3. <strong>å…¶ä»–æ¶æ„</strong></h2><ul><li>ä¹Ÿæœ‰äººæåˆ°<strong>RNN</strong>ã€<strong>VAE</strong>ç­‰ä¼ ç»Ÿæ¶æ„ï¼Œä½†æ™®éè®¤ä¸º Transformer å’Œ Diffusion æ›´ç°ä»£ã€æ›´æœ‰æ•ˆã€‚</li></ul><hr><h2 id=4-æ•°æ®å¤„ç†å»ºè®®>4. <strong>æ•°æ®å¤„ç†å»ºè®®</strong></h2><ul><li>å¤šäººå»ºè®®<strong>å…ˆç”¨ MIDI æˆ–ç±»ä¼¼çš„ç¬¦å·æ•°æ®</strong>ï¼Œå› ä¸ºéŸ³é¢‘ç›´æ¥å»ºæ¨¡éš¾åº¦å’Œç®—åŠ›éœ€æ±‚æ›´é«˜ã€‚</li><li>æ¨èå·¥å…·å¦‚<strong>MidiTok</strong>ï¼Œç”¨äºå¤„ç†å’Œç¼–ç  MIDI æ•°æ®ï¼Œæ–¹ä¾¿ Transformer ç­‰æ¨¡å‹è®­ç»ƒã€‚</li></ul><hr><h2 id=5-ç®—åŠ›ä¸æ•°æ®>5. <strong>ç®—åŠ›ä¸æ•°æ®</strong></h2><ul><li>æœ‰äººæé†’<strong>è®­ç»ƒå¤§æ¨¡å‹éœ€è¦å¤§é‡ç®—åŠ›</strong>ï¼Œå»ºè®®å…ˆæŸ¥é˜…ç›¸å…³è®ºæ–‡ï¼Œäº†è§£å½“å‰ä¸»æµåšæ³•å’Œæ•°æ®é›†ã€‚</li></ul><hr><h2 id=6-å…¶ä»–èµ„æº>6. <strong>å…¶ä»–èµ„æº</strong></h2><ul><li>æœ‰äººæåˆ°å¯ä»¥æŸ¥é˜… Google Scholar ä¸Šå…³äºâ€œmusic generation transformerâ€çš„è®ºæ–‡ï¼Œè·å–æœ€æ–°è¿›å±•ã€‚</li><li>ä¹Ÿæœ‰ç”¨æˆ·æ¨èå…³æ³¨ Jambaï¼ˆä¸€ä¸ªéŸ³ä¹ç”Ÿæˆç›¸å…³é¡¹ç›®ï¼‰ã€‚</li></ul><hr><h2 id=æ€»ç»“ä¸€å¥è¯-2><strong>æ€»ç»“ä¸€å¥è¯</strong></h2><ul><li><strong>Transformer</strong> æ˜¯å½“å‰éŸ³ä¹ç”Ÿæˆé¢†åŸŸçš„ä¸»æµæ¶æ„ï¼Œé€‚åˆå¤„ç† MIDI ç­‰ç¬¦å·æ•°æ®ã€‚</li><li><strong>æ‰©æ•£æ¨¡å‹</strong>ï¼ˆDiffusionï¼‰åœ¨éŸ³é¢‘ç”Ÿæˆå’Œæ–‡æœ¬åˆ°éŸ³ä¹æ–¹å‘ä¹Ÿå¾ˆæœ‰å‰æ™¯ã€‚</li><li>æ•°æ®å¤„ç†å’Œç®—åŠ›æ˜¯å®é™…è½åœ°æ—¶çš„é‡è¦è€ƒè™‘å› ç´ ã€‚</li></ul><p>å¦‚éœ€å…·ä½“æ¨¡å‹ã€ä»£ç æˆ–è®ºæ–‡æ¨èï¼Œå¯ä»¥ç»§ç»­é—®æˆ‘ï¼</p><hr><p><strong>User</strong></p><p>Jamba:
A Hybrid Transformer-Mamba Language Model
Opher Lieberâˆ— Barak Lenzâˆ— Hofit Bata Gal Cohen Jhonathan Osin
Itay Dalmedigos Erez Safahi Shaked Meirom Yonatan Belinkov
Shai Shalev-Shwartz Omri Abend Raz Alon Tomer Asida
Amir Bergman Roman Glozman Michael Gokhman Avshalom Manevich
Nir Ratner Noam Rozen Erez Schwartz Mor Zusman Yoav Shoham
Abstract
We present Jamba, a new base large language model based on a novel hybrid
Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba
interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both
model families. MoE is added in some of these layers to increase model capacity
while keeping active parameter usage manageable. This flexible architecture allows
resource- and objective-specific configurations. In the particular configuration we
have implemented, we end up with a powerful model that fits in a single 80GB
GPU. Built at large scale, Jamba provides high throughput and small memory
footprint compared to vanilla Transformers, and at the same time state-of-the-art
performance on standard language model benchmarks and long-context evaluations.
Remarkably, the model presents strong results for up to 256K tokens context length.
We study various architectural decisions, such as how to combine Transformer and
Mamba layers, and how to mix experts, and show that some of them are crucial
in large scale modeling. We also describe several interesting properties of these
architectures which the training and evaluation of Jamba have revealed, and plan to
release checkpoints from various ablation runs, to encourage further exploration
of this novel architecture. We make the weights of our implementation of Jamba
publicly available under a permissive license.
Model: <a class=link href=https://huggingface.co/ai21labs/Jamba-v0.1 target=_blank rel=noopener>https://huggingface.co/ai21labs/Jamba-v0.1</a>
1 Introduction
We introduce Jamba, a new publicly available large language model. Jamba is based on a novel
hybrid architecture, which combines Transformer layers [51] with Mamba layers [17], a recent
state-space model [18, 19], as well as a mixture-of-experts (MoE) module [14, 46]. Jamba thus
combines two orthogonal architectural designs that together give it improved performance and higher
throughput, while maintaining a manageable memory footprint. The 7B-based Jamba model (12B
active parameters, 52B total available parameters) we are releasing was designed to fit in a single
80GB GPU, but the Jamba architecture supports other design choices, depending on oneâ€™s hardware
and performance requirements.
âˆ—Equal contribution.
arXiv:2403.19887v2 [cs.CL] 3 Jul 2024
The fundamental novelty of Jamba is its hybrid Transformer-Mamba architecture (though see mention
below of recent related efforts). Despite the immense popularity of the Transformer as the predominant
architecture for language models, it suffers from two main drawbacks. First, its high memory and
compute requirements hinders the processing of long contexts, where the key-value (KV) cache size
becomes a limiting factor. Second, its lack of a single summary state entails slow inference and low
throughput, since each generated token performs a computation on the entire context. In contrast,
older recurrent neural network (RNN) models, which summarize an arbitrarily long context in a
single hidden state, do not suffer from these limitations. RNN models have their own shortcomings,
however. They are costly to train since training cannot be parallelized across time steps. And they
struggle with long distance relationships, which the hidden state captures to only a limited extent.
Recent state space models (SSMs) like Mamba are more efficient to train than RNNs and are more
capable at handling long distance relationships, but still lag behind the performance of comparably
sized Transformer language models. Taking advantage of both model families, Jamba combines
Transformer and Mamba layers, at a certain ratio. Varying the ratio of Transformer/Mamba layers
allows balancing memory usage, efficient training, and long context capabilities.
A few other recent attempts to combine Attention and SSM modules are worth noting. [55] mixes
an S4 layer [18] with a local attention layer, followed by a sequence of local attention layers; it
shows experiments with small models and simple tasks. [17] reports that interleaving Mamba and
attention layers is only slightly better than pure Mamba in terms of perplexity, with models up to
1.3B parameters. [37] starts with an SSM layer followed by chunk-based Transformers, with models
up to 1.3B showing improved perplexity. [13] adds an SSM layer before the self-attention in a
Transformer layer, while [43] adds the SSM after the self-attention, both showing improvements on
speech recognition. [36] replaces the MLP layers in the Transformer by Mamba layers, and shows
benefits in simple tasks. These efforts are different from Jamba both in the particular way in which
the SSM component is mixed with the attention one, and in the scale of implementation. Closest are
perhaps H3 [15], a specially designed SSM that enables induction capabilities, and a generalization
called Hyena [39]. The former proposed a hybrid architecture that replaces the second and middle
layers with self-attention, and was implemented with up to 2.7B parameters and 400B training tokens.
However, as shown in [17], its perfomance lags that of pure Mamba. Based on Hyena, StripedHyena
[40] interleaves attention and SSM layers in a 7B parameter model. However, it lags behind the
Attention-only Mistral-7B [23]. All of this renders Jamba the first production-grade Attention-SSM
hybrid model. Scaling the hybrid Jamba architecture required overcoming several obstacles, which
we dicsuss in Section 6.
Jamba also includes MoE layers [14, 46], which allow increasing the model capacity (total number of
available parameters) without increasing compute requirements (number of active parameters). MoE
is a flexible approach that enables training extremely large models with strong performance [24]. In
Jamba, MoE is applied to some of the MLP layers. The more MoE layers, and the more experts in
each MoE layer, the larger the total number of model parameters. In contrast, the more experts we use
at each forward pass, the larger the number of active parameters as well as the compute requirement.
In our implementation of Jamba, we apply MoE at every other layer, with 16 experts and the top-2
experts used at each token (a more detailed discussion of the model architecture is provided below).
We evaluated our implementation of Jamba on a wide range of benchmarks and found it performs
comparably to Mixtral-8x7B [24], which has a similar number of parameters, and also to the larger
Llama-2 70B [50]. In addition, our model supports a context length of 256K tokens â€“ the longest
supported context length for production-grade publicly available models. On long-context evaluations,
Jamba outperformes Mixtral on most of the evaluated datasets. At the same time, Jamba is extremely
efficient; for example, its throughput is 3x that of Mixtral-8x7B for long contexts. Moreover, our
model fits in a single GPU (with 8bit weights) even with contexts of over 128K tokens, which is
impossible with similar-size attention-only models such as Mixtral-8x7B.
Somewhat unusual for a new architecture, we release Jamba (12B active parameters, 52B total available parameters) under Apache 2.0 license: <a class=link href=https://huggingface.co/ai21labs/Jamba-v0.1 target=_blank rel=noopener>https://huggingface.co/ai21labs/Jamba-v0.1</a>.
We do so since we feel that the novel architecture of Jamba calls for further study, experimentation,
and optimization by the community. Our design was based on various ablation experiments we
conducted to explore the effect of different tradeoffs and design choices, and insights gleaned from
those. These ablations were performed at scales of up to 7B parameters, and training runs of up to
250B tokens. We plan to release model checkpoints from these runs.
2
RMSNorm
Attention
RMSNorm
MLP
RMSNorm
Attention
RMSNorm
MoE
RMSNorm
Mamba
RMSNorm
MLP
RMSNorm
Mamba
RMSNorm
MoE
Transformer layer Attention MoE layer
Mamba layer Mamba MoE layer
(a) Jamba block (b) Different types of layers
Mamba layer
Mamba layer
Mamba MoE layer
Transformer layer
Mamba layer
Mamba MoE layer
Mamba MoE layer
Mamba MoE layer
Figure 1: (a) A single Jamba block. (b) Different types of layers. The implementation shown here is
with l = 8, a : m = 1 : 7 ratio of attention-to-Mamba layers, and MoE applied every e = 2 layers.
Important notice: The Jamba model released is a pretrained base model, which did not go through
alignment or instruction tuning, and does not have moderation mechanisms. It should not be used in
production environments or with end users without additional adaptation.
2 Model Architecture
Jamba is a hybrid decoder architecture that mixes Transformer layers [51] with Mamba layers [17], a
recent state-space model (SSM) [18, 19], in addition to a mixture-of-experts (MoE) module [14, 46].
We call the combination of these three elements a Jamba block. See Figure 1 for an illustration.
Combining Transformer, Mamba, and MoE elements allows flexibility in balancing among the
sometimes conflicting objectives of low memory usage, high throughput, and high quality. In terms
of memory usage, note that comparing the total size of the model parameters can be misleading.
In an MoE model, the number of active parameters that participate in any given forward step may
be much smaller than the total number of parameters. Another important consideration is the KV
cache â€“ the memory required to store the attention keys and values in the context. When scaling
Transformer models to long contexts, the KV cache becomes a limiting factor. Trading off attention
layers for Mamba layers reduces the total size of the KV cache. Our architecture aims to provide
3
not only a small number of active parameters but also an 8x smaller KV cache compared to a vanilla
Transformer. Table 1 compares Jamba with recent publicly available models, showing its advantage
in maintaining a small KV cache even with 256K token contexts.
Available params Active params KV cache (256K context, 16bit)
LLAMA-2 6.7B 6.7B 128GB
Mistral 7.2B 7.2B 32GB
Mixtral 46.7B 12.9B 32GB
Jamba 52B 12B 4GB
Table 1: Comparison of Jamba and recent open models in terms of total available parameters, active
parameters, and KV cache memory on long contexts. Jamba provides a substantial reduction in the
KV cache memory requirements.
In terms of throughput, with short sequences, attention operations take up a small fraction of the
inference and training FLOPS [7]. However, with long sequences, attention hogs most of the compute.
In contrast, Mamba layers are more compute-efficient. Thus, increasing the ratio of Mamba layers
improves throughput especially for long sequences.
Here is a description of the main configuration, which provides improved performance and efficiency.
Section 6 contains results from ablation experiments supporting the design choices.
The basic component is a Jamba block, which may be repeated in sequence. Each Jamba block is a
combination of Mamba or Attention layers. Each such layer contains either an attention or a Mamba
module, followed by a multi-layer perceptron (MLP). The different possible types of layers are shown
in Figure 1(b).2 A Jamba block contains l layers, which are mixed at a ratio of a : m, meaning a
attention layers for every m Mamba layers.
In Jamba, some of the MLPs may be replaced by MoE layers, which helps increase the model capacity
while keeping the active number of parameters, and thus the compute, small. The MoE module may
be applied to MLPs every e layers. When using MoE, there are n possible experts per layer, with a
router choosing the top K experts at each token. In summary, the different degrees of freedom in the
Jamba architecture are:
â€¢ l: The number of layers.
â€¢ a : m: ratio of attention-to-Mamba layers.
â€¢ e: how often to use MoE instead of a single MLP.
â€¢ n: total number of experts per layer.
â€¢ K: number of top experts used at each token.
Given this design space, Jamba provides flexibility in preferring certain properties over others. For
example, increasing m and decreasing a, that is, increasing the ratio of Mamba layers at the expense
of attention layers, reduces the required memory for storing the key-value cache. This reduces the
overall memory footprint, which is especially important for processing long sequences. Increasing the
ratio of Mamba layers also improves throughput, especially at long sequences. However, decreasing
a might lower the modelâ€™s capabilities.
Additionally, balancing n, K, and e affects the relationship between active parameters and total
available parameters. A larger n increases the model capacity at the expense of memory footprint,
while a larger K increases the active parameter usage and the compute requirement. In contrast, a
larger e decreases the model capacity, while decreasing both compute (when K>1) and memory
requirements, and allowing for less communication dependencies (decreasing memory transfers as
well as inter-GPU communication during expert-parallel training and inference).
Jambaâ€™s implementation of Mamba layers incorporate several normalizations that help stabilize
training in large model scales. In particular, we apply RMSNorm [53] in the Mamba layers.
2The figure shows a potential Attention MoE layer, which our architecture does not use, but future variants
could.
4
We found that with the Mamba layer, positional embeddings or mechanisms like RoPE [47] are not
necessary, and so we do not use any explicit positional information.
Other architecture details are standard, including grouped-query attention (GQA), SwiGLU activation
function [7, 45, 50], and load balancing for the MoE [14]. The vocabulary size is 64K. The tokenizer
is trained with BPE [16, 33, 44] and each digit is a separate token [7]. We also remove the dummy
space used in Llama and Mistral tokenizers for more consistent and reversible tokenization.
3 Reaping the Benefits
3.1 Jamba Implementation for a Single 80GB GPU
The specific configuration in our implementation was chosen to fit in a single 80GB GPU, while
achieving best performance in the sense of quality and throughput. In our implementation we have a
sequence of 4 Jamba blocks. Each Jamba block has the following configuration:
â€¢ l = 8: The number of layers.
â€¢ a : m = 1 : 7: ratio attention-to-Mamba layers.
â€¢ e = 2: how often to use MoE instead of a single MLP.
â€¢ n = 16: total number of experts.
â€¢ K = 2: number of top experts used at each token.
The a : m = 1 : 7 ratio was chosen according to preliminary ablations, as shown in Section 6, since
this ratio was the most compute-efficient variant amongst the best performing variants in terms of
quality.
The configuration of the experts was chosen to enable the model to fit in a single 80GB GPU (with
int8 weights), while including sufficient memory for the inputs. In particular, n and e were balanced
to have an average of âˆ¼8 experts per layer. In addition, we balanced n, K, and e to allow for
high quality, while keeping both compute requirements and communication dependencies (memory
transfers) checked. Accordingly, we chose to replace the MLP module with MoE on every other
layer, as well as have a total of 16 experts, two of which are used at each token. These choices were
inspired by prior work on MoE [8, 54] and verified in preliminary experiments.
Figure 2 shows the maximal context length that fits a single 80GB GPU with our Jamba implementation compared to Mixtral 8x7B and Llama-2-70B. Jamba provides 2x the context length of Mixtral
and 7x that of Llama-2-70B.
Context Length
0K
20K
40K
60K
80K
100K
120K
140K
Llama-2 70B Mixtral 8x7B Jamba
Context length fitting a single 80GB A100 GPU
Figure 2: Comparison of maximum context length fitting in a single A100 80GB GPU. Jamba enables
2x the context length of Mixtral and 7x that of Llama-2-70B.
Overall, our Jamba implementation was successfully trained on context lengths of up to 1M tokens.
The released model supports lengths of up to 256K tokens.
5
3.2 Throughput Analysis
For concreteness, we present results of the throughput in two specific settings.3
In the first setting, we
have varying batch size, a single A100 80 GB GPU, int8 quantization, 8K context length, generating
output of 512 tokens. As Figure 3a shows, Jamba allows processing of large batches, leading to a 3x
increase in throughput (tokens/second) over Mixtral, which does not fit with a batch of 16 despite
having a similar number of active parameters.
In the second setting, we have a single batch, 4 A100 GPUs, no quantization, varying context lengths,
generating output of 512 tokens. As demonstrated in Figure 3b, at small context lengths all models
have a similar throughput. Jamba excels at long contexts; with 128K tokens its throughput is 3x that
of Mixtral. Note that this is despite the fact that Jamba has not yet enjoyed optimizations of the kind
the community has developed for pure Transformer models over the past six years. We can expect
the throughut gap to increase as such optimizations are developed also for Jamba.
Batch size
Throughput (t/s)
0
500
1000
1500
2000
1 2 4 8 16
Llama-2 13B Llama-2 70B Mixtral 8x7B Jamba
Throughput (single GPU)
(a) Throughput at different batch sizes (single A100
GPU, 8K context length). Jamba allows processing
large batches, with a throughput 3x greater than Mixtral.
Context Window
Throughput (t/s)
0
500
1000
1500
2000
1K 2K 4K 8K 16K 32K 64K 128K
Llama-2 13B Llama-2 70B Mixtral 8x7B Jamba
Throughput (4 A100 GPUs)
(b) Throughput at different context lengths (single
batch, 4 A100 GPUs). With a context of 128K tokens, Jamba obtains 3x the throughput of Mixtral, while
Llama-2-70B does not fit with this long context.
Figure 3: Comparison of throughput (tokens/second) with Jamba and recent open models.
4 Training Infrastructure and Dataset
The model was trained on NVIDIA H100 GPUs. We used an in-house proprietary framework
allowing efficient large-scale training including FSDP, tensor parallelism, sequence parallelism, and
expert parallelism.
Jamba is trained on an in-house dataset that contains text data from the Web, books, and code, with
the last update in March 2024. Our data processing pipeline includes quality filters and deduplication.
5 Evaluation
In general we approach benchmarks cautiously, as they correlate only partially with what matters
in real applications, and furthermore invite gaming the system in order to boast vanity numbers.
Nevertheless, we present several indicative results.
5.1 Academic Benchmarks
We report results with a wide range of standard academic benchmarks:
Common sense reasoning: HellaSwag (10-shot) [52], WinoGrande (5-shot) [42], ARC-E (0-shot)
and ARC-Challenge (25-shot) [10], and PIQA (zero-shot) [3].
Reading Comprehension: BoolQ (10-shots) [9] and QuAC (zero-shot) [6].
Others: GSM8K (3-shot CoT) [11], HumanEval (pass@1) [5], Natural Questions closed-book (NQ;
5-shot) [27], and TruthfulQA (zero-shot) [30].
Aggregate benchmarks: MMLU (5-shot) [21] and BBH (3-shot) [48].
3Referring to end-to-end throughput (encoding+decoding). The results should be taken relatively rather than
absolutely, as they are without possible optimizations.
6
Reasoning
HellaSwag WinoGrande ARC-E ARC-C PIQA NQ TruthfulQA
Llama-2 13B 80.7 72.8 77.3 59.4 80.5 37.7 37.4
Llama-2 70B 85.3 80.2 80.2 67.3 82.8 46.9 44.9
Gemma 81.2 72.3 81.5 53.2 81.2 32.6 44.8
Mixtral 86.7 81.2 77.6 66 83 44.8 46.8
Jamba 87.1 82.5 73.5 64.4 83.2 45.9 46.4
Comprehension Aggregate
BoolQ QuAC GSM8K HumanEval MMLU BBH
Llama-2 13B 81.7 42.7 34.7 18.3 54.8 39.4
Llama-2 70B 85 42.4 55.3 29.9 69.8 51.2
Gemma 87.2 39.2 54.5 32.3 64.3 55.1
Mixtral 88.4 40.9 60.4 34.8 70.6 50.3
Jamba 88.2 40.9 59.9 29.3 67.4 45.4
Table 2: Comparison of Jamba with other publicly available models. Jamba obtains similar performance with much better throughput.
Table 2 compares Jamba to several publicly available models on common academic benchmarks for
evaluating language models. We compare with Llama-2 13B [50], which has about the same number
of active paramters as our model, Llama-2 70B, which is larger than our model, Gemma [49], which
has 7B parameters, and Mixtral [24], which has about the same number of active and total parameters
as our model.
In most tasks, Jamba performs comparably to leading publicly available models of similar or larger
size, including Llama-2 70B and Mixtral. At the same time, our model has a smaller number of total
available parameters than Llama-2 (52B compared to 70B). Moreover, as a sparse model, Jamba
has only 12B active parameters, similar to Mixtralâ€™s 12.9B active parameters. However, as a fullyattentional model, Mixtral has a large memory footprint with long sequences, requiring 32GB for the
KV cache with 256K tokens. In contrast, thanks to its hybrid Attention-Mamba architecture, Jambaâ€™s
KV cache takes only 4GB even at such a long context (Section 2). Importantly, our Jamba achieves
such a strong performance while having much better throughput than Llama-2 70B and Mixtral, up
to 3x improvement (Section 3.2).
In summary, Jamba demostrates the ability of hybrid architectures to reach the performance of
state-of-the-art Transformer models of the same size class, while having the benefits of an SSM.
5.2 Long-Context Evaluations
We have successfully trained Jamba models with context lengths of up to 1M tokens. The released
model handles context lengths of up to 256K tokens. In this section, we evaluate it on synthetic and
naturalistic benchmarks that test its long-context capabilities.
5.2.1 Needle-in-a-haystack
As Figure 4 shows, Jamba has excellent performance in the needle-in-a-haystack evaluation, which
requires retrieving a simple statement planted in a long context window [25]. This result is noteworthy
especially given that our implementation of Jamba uses only 4 attention layers.
5.2.2 Naturalistic long-context evaluation
We evaluate Jambaâ€™s ability to handle long contexts in two settings. First, we evaluate the model
on several classification tasks commonly used for assessing in-context learning, with an increasing
number of few-shot examples. In particular, we use the four datasets with the largest label space
from [41], which showed that such tasks benefit most from using more few-shot examples: Trec-Fine
(fine-grained question type classification, 50 labels; [29]), NLU Intent (intent classification in natural
language understanding, 68 labels; [31]), Banking77 (intent classification in the banking domain, 77
7
2K
4K
8K
32K
128K
256K
Tokens
0.0
10.0
20.0
30.0
40.0
50.0
60.0
70.0
80.0
90.0
100.0
Depth %
Retrieval Across Context Lengths (&ldquo;Needle-in-a-haystack&rdquo;)
0.0
0.2
0.4
0.6
0.8
1.0
Score
Figure 4: A needle-in-a-haystack evaluation showing Jambaâ€™s ability to recall statements placed in
the middle of contexts of up to 256K tokens length.
labels; [4]), and CLINC150 (intent classification, 150 labels; [28]). In each case we add examples
up to a context length of 128K tokens. Figure 5 shows the results, comparing Jamba to Mixtral. In
Trec-Fine and Banking77, Jamba outperforms Mixtral, especially with a large number of few-shot
examples. In NLU Intent and CLINC150, the two models are on par.
0 1000 2000 3000 4000</p><h1 id=samples>samples</h1><p>0.0
0.2
0.4
0.6
0.8
1.0
greedy_em
greedy_em vs. # samples for TREC-Fine
Jamba
Mixtral
(a) TREC Fine-grained.
0 1000 2000 3000 4000 5000</p><h1 id=samples-1>samples</h1><p>0.0
0.2
0.4
0.6
0.8
1.0
greedy_em
greedy_em vs. # samples for NLU Intent
Jamba
Mixtral
(b) NLU Intent.
0 500 1000 1500 2000 2500 3000 3500</p><h1 id=samples-2>samples</h1><p>0.0
0.2
0.4
0.6
0.8
1.0
greedy_em
greedy_em vs. # samples for Banking77
Jamba
Mixtral
(c) Banking77.
0 1000 2000 3000 4000</p><h1 id=samples-3>samples</h1><p>0.0
0.2
0.4
0.6
0.8
1.0
greedy_em
greedy_em vs. # samples for CLINC150
Jamba
Mixtral
(d) CLINC150.
Figure 5: Comparison of Jamba and Mixtral on few-shot classification with a large number of
few-shot examples. Results are exact match with greedy decoding.
Second, we use question-answering datasets, consisting of long inputs. To this end, we repurpose
five of the longest-context datasets from L-Eval [2], by structuring them in a few-shot format (we
use 3-shots in these experiments). Specifically, we evaluated the models on the following datasets:
NarrativeQA (QA on narratives; [26]), LongFQA (finance; [2]), Natural Questions (NQ; Wikipedia;
8
[27]), CUAD (law; [22]), and SFiction (science fiction). The average input length in these datasets
ranges from 6K to 62K tokens. These lengths are further highly expanded by the few-shot format.
Table 3 summarizes the results, in terms of F1.4
Jamba outperforms Mixtral on most of the datasets
as well as on average. In addition, as these long-context tasks require substantial computation, here
Jambaâ€™s efficiency shines, with much better throughput with long contexts (Section 3.2).
LongFQA CUAD NarrativeQA NQ SFiction Avg
Mixtral 0.42 0.46 0.29 0.58 0.42 0.43
Jamba 0.44 0.44 0.30 0.60 0.40 0.44
Table 3: Results (F1) on long-context QA benchmarks, with a 3-shot format.
6 Ablations and Insights
This section discusses ablation experiments we ran for different design choices in our implementation
of the Jamba architecture. First we show the benefit of combining attention and Mamba layers,
at which ratio they should be combined, and how to interleave them. We investigate cases where
pure Mamba fails, suggesting that it struggles to develop in-context learning capabilities, while
the Attention-Mamba hybrid exhibits in-context learning similar to vanilla Transformers. Then we
show the benefit of adding MoE on top of a hybrid Attention-Mamba model. Finally, we share two
additional learnings that we found useful: explicit positional information is not needed in Jamba, and
Mamba layers necessitate special normalization to stabilize training at large scale.5
For these ablations, we report the following measures, which exhibit informative performance even at
small data or model scale.
â€¢ Academic benchmarks: HellaSwag (10-shot) [52], WinoGrande (5-shot) [42], Natural
Questions closed-book (NQ; 5-shot) [27].
â€¢ HuggingFace OpenLLM leaderboard (OLLM) [12]: a summary statistic of several datasets.
We report results with our reproduction.
â€¢ Perplexity evaluations: we report log-prob (per byte) on texts from three domains: C4,
Books, and code.
6.1 Benefits of combining Attention and Mamba
We first investigate the ratio of Attention to Mamba layers (a : m), with 1.3B parameters models
trained for 250B tokens. As Table 4 shows, the hybrid Jamba model outperforms the pure attention
or Mamba models. The ratio of attention-to-Mamba layers may be 1:3 or 1:7 with virtually no
performance difference. Figure 6 shows the training loss of these models, where Jamba exhibits
improved loss during training. Given that a 1:7 ratio is more compute-efficient and shows similar
performance, we opt for it in our larger-scale experiments.
Hella
Swag
Wino
Grande
log-prob
OLLM NQ C4 Books Code
Attention 36.4 62.4 59.6 14.5 -0.543 -0.659 -0.331
Mamba 36.1 62.6 59.4 14.5 -0.543 -0.661 -0.334
Jamba (a : m = 1 : 3, no MoE) 37.2 65.1 61.7 16.5 -0.533 -0.649 -0.321
Jamba (a : m = 1 : 7, no MoE) 37.2 65.1 61.7 16.0 -0.533 -0.650 -0.321
Table 4: Results on academic benchmarks and log-probability evaluations showing an improved
performance of Attention-Mamba (no MoE) compared to vanilla Attention and Mamba models.
There is no substantial difference between 1:3 and 1:7 ratios of Attention-to-Mamba layers. Models
are 1.3B parameters, trained for 250B tokens.
4
F1 score is the recommended metric in L-Eval [2]. In addition, our setup calibrates the length of the few-shot
completions to approximately match the length of the test completion, increasing the credibility of F1.
5
In all the ablations, â€œpure Mambaâ€ refers to models with Mamba layers interleaved with MLP layers.
9
Figure 6: Training loss curves for pure Attention, pure Mamba, and Attention-Mamba hybrids (no
MoE), with ratios a : m of 1:3 and 1:7. All models are 1.3B parameters. The two hybrids achieve
better loss throughout this training run, without any noticeable difference between the different
Attention/Mamba ratios.
Next, we compare performance of vanilla Transformer, vanilla Mamba, and Attention-Mamba hybrid
models, at 7B model size, after training on 50B tokens. As Table 5 shows, the pure Mamba model is
quite competitive, but lags slightly behind pure Attention. The hybrid Attention-Mamba (without
MoE) outperforms the pure models while obtaining better throughput than the vanilla Transformer
(Section 3.2).
Hella
Swag
Wino
Grande
log-prob
OLLM NQ C4 Books Code
Attention 36.1 60.4 59.7 13.7 -0.555 -0.666 -0.347
Mamba 35.3 60.2 55.8 14.0 -0.554 -0.667 -0.355
Jamba (a : m = 1 : 7, no MoE) 36.6 62.5 58.8 15.4 -0.547 -0.658 -0.340
Table 5: Results on academic benchmarks and log-prob evaluations, comparing pure Attention, pure
Mamba, and Attention-Mamba hybrid (no MoE). Models are 7B parameters, trained for 50B tokens.
Figure 7 shows the training loss of the three architectures. While the pure Transformer and Mamba
models have a similar convergence, the hybrid Jamba (no MoE) has a lower loss throughout this run.
Figure 7: Training loss curves for pure Attention, pure Mamba, and an Attention-Mamba hybrid (no
MoE). All models are 7B parameters. The hybrid achieves better loss throughout this training run.
10
6.2 Why does the Combination Work?
The pure Mamba model showed fairly good results in most tasks early on, including in general
perplexity evaluations. However, it performed substantially worse than the pure Attention model
in three common benchmark tasks: IMDB [32], QuAC [6], and NarrativeQA [26]. In contrast, the
hybrid Attention-Mamba performed similarly to the Attention model on these datasets. Table 6 shows
the results for 1.3B models after 250B tokens.
IMDB QuAC NarrativeQA
Attention 84.1 27.9 45.8
Mamba 48.8 20.2 27.7
Attention-Mamba 90.9 26.6 43.7
Table 6: Mamba performs poorly on certain datasets, while the Attention-Mamba hybrid performs on
par with the Attention model.
Looking into these results further, we found out that the pure Mamba model often does not follow the
correct format. For instance, in the IMDB dataset, answer choices are â€œPositiveâ€ or â€œNegativeâ€. While
the Attention model adheres to this format, the pure Mamba model often produces other answers,
such as â€œVery Goodâ€, â€œVery Positiveâ€, â€œFunnyâ€, â€œBadâ€, â€œPoorâ€, and â€œ3/10â€. While these may be
considered correct answers, the difficulty of Mamba to adhere to the format suggests a potential
problem. Indeed, to perform successful in-context learning, it is important for models to capture the
input-output format [34]. The hybrid Attention-Mamba model follows the format successfully, just
like the pure Attention model.
We hypothesize that this phenomenon points to a limitation of SSMs â€“ a potential difficulty in
in-context learning (ICL). Indeed, the ability to perform ICL has been linked to the emergence of socalled induction heads in Transformer language models during training, which perform approximate
copying operations that are supportive of ICL [35]. We conjecture that the lack of an attention
mechanism in the pure Mamba model makes it difficult for it to learn in-context. While Mamba
may learn to copy and perform simple ICL when explicitly trained to do so ([17, 36], it is not
clear if ICL is an emergent capability in SSM as is typical of Transformer models. In contrast, the
hybrid Attentionâ€“Mamba model does perform successful ICL, even when only 1 out of 8 layers is an
Attention one.
As anecdotal evidence of an emergent induction mechanism, we visualize in Figure 8 the attention
of an example head from a 1.3B Attention-Mamba hybrid model (no MoE), on an IMDB example
where the pure Mamba failed and the hybrid succeeded. Clearly, the attention from the last token
(â€œ:â€) is focused on the labels from the few-shot examples. We have found 12 such heads in our hybrid
model, in all three attention layers (which correspond to layers 4, 12, 20 in the model).
Future work can further investigate the emergence of ICL in hybrid models at large scale. Our
released checkpoints would hopefully facilitate such investigations. Finally, very recent work has
attempted to extract attention-like scores from state-space models like Mamba [1], which opens
another direction to search for induction capabilities in state-space models.
6.3 The Effect of Mixture-of-Experts (MoE)
Recent work has shown that MoE improves Transformer language models while keeping compute
manageable [24].6 However, it is not clear if MoE integrates well with state-space models at a
large scale, and specifically with our hybrid Attentionâ€“Mamba architecture. Indeed, Table 7 shows
that MoE improves the performance of the hybrid Attention-Mamba architecture at large scale (7B
parameters trained on 50B tokens). The MoE variant has n = 16 total experts, K = 2 experts used at
each token, and MoE is applied every e = 2 layers, as described in Section 3.1.
6There is also initial evidence that MoE helps Mamba layers, albeit at small model and data scale [38].
11
[â€¦]
[â€¦]
[â€¦]
[â€¦]
Figure 8: Example induction head (H3, first attention layer) from a hybrid Attention-Mamba model.
Highlighted words reflect strong attention from the last token, â€œ:â€, just before the model is about to
predict the label. We see that the attention is focused on label tokens from the few-shot examples.
Hella
Swag
Wino
Grande
log-prob
OLLM NQ C4 Books Code
Jamba (no MoE) 36.6 62.5 58.8 15.4 -0.547 -0.658 -0.340
Jamba+MoE 38.1 66.0 61.2 18.9 -0.534 -0.645 -0.326
Table 7: Mixture-of-experts improves the Attention-Mamba hybrid.
6.4 Stabilizing Mamba at large scale
When training Jamba models of up to 1.3B parameters, we observed stable training without special
problems. However, when scaling to the largest model released here (7B-based, which has 12B/52B
active/total parameters), we encountered large loss spikes. Investigating this revealed that inner parts
of the Mamba layers suffer from large activation values, leading to the spikes. We therefore added
RMSNorm [53] to internal activations. As Figure 9 shows, this stabilized training and prevented
additional loss spikes.
6.5 Jamba does not Require Explicit Positional Information
Table 8 shows results of the Jamba architecture (with MoE) with no positional information and when
applying RoPE [47] in the attention layers (1.3B parameter models, 250B tokens). The results are
similar, suggesting that explicit positional information may not be required for the hybrid architecture.
Presumably, the Mamba layers, which are placed before attention layers, provide implicit position
information.7
7
Some prior evidence suggested that Transformer decoder models do not need positional encodings [20].
However, all existing large scale models do use some sort of explicit position information.
12
Figure 9: Adding RMSNorm to Mamba layers prevents loss spikes.
Hella
Swag
Wino
Grande
Narrative
QA
log-prob
OLLM ARC-C NQ BoolQ C4 Books Code
Jamba 39.6 71.5 64.2 40.7 50.5 22.2 68.9 -0.516 -0.623 -0.299
Jamba+RoPE 40.1 71.8 65.5 40.4 46.2 22.2 67.9 -0.516 -0.623 -0.299
Table 8: Comparison of Jamba with and without explicit positional information.
7 Conclusion
We presented Jamba, a novel architecture which combines Attention and Mamba layers, with MoE
modules, and an open implementation of it, reaching state-of-the-art performance and supporting
long contexts. We showed how Jamba provides flexibility for balancing performance and memory
requirements, while maintaining a high throughput. We experimented with several design choices
such as the ratio of Attention-to-Mamba layers and discussed some discoveries made during the
development process, which will inform future work on hybrid attentionâ€“state-space models. To
facilitate such research, we plan to release model checkpoints from smaller-scale training runs.
The largest model we provide with this release has 12B active and 52B total available parameters,
supporting context lengths of up to 256K tokens and fitting in a single 80GB GPU even when
processing 140K-token texts.
References
[1] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models. arXiv
preprint arXiv:2403.01590, 2024.
[2] Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng
Qiu. L-Eval: Instituting standardized evaluation for long context language models. arXiv
preprint arXiv:2307.11088, 2023.
[3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical
commonsense in natural language. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pages 7432â€“7439, 2020.
[4] IÃ±igo Casanueva, Tadas Temcinas, Daniela Gerz, Matthew Henderson, and Ivan Vuli Ë‡ c. Efficient Â´
intent detection with dual sentence encoders. In Proceedings of the 2nd Workshop on Natural
Language Processing for Conversational AI, pages 38â€“45, 2020.
[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
13
[6] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang,
and Luke Zettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, pages 2174â€“2184, 2018.
[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1â€“
113, 2023.
[8] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan
Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified
scaling laws for routed language models. In International conference on machine learning,
pages 4057â€“4086. PMLR, 2022.
[9] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and
Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In
Proceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),
pages 2924â€“2936, 2019.
[10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning
challenge. arXiv preprint arXiv:1803.05457, 2018.
[11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[12] Hugging Face. Open LLM leaderboard. <a class=link href=https://huggingface.co/spaces/ target=_blank rel=noopener>https://huggingface.co/spaces/</a>
HuggingFaceH4/open_llm_leaderboard, 2024.
[13] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar,
Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, and Mark J. F. Gales. Multi-head state
space model for speech recognition. In Proceedings of INTERSPEECH 2023, pages 241â€“245,
2023.
[14] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and efficient sparsity. Journal of Machine Learning Research,
23(120):1â€“39, 2022.
[15] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re.
Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh
International Conference on Learning Representations, 2022.
[16] Philip Gage. A new algorithm for data compression. The C Users Journal, 12(2):23â€“38, 1994.
[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
arXiv preprint arXiv:2312.00752, 2023.
[18] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured
state spaces. In International Conference on Learning Representations, 2021.
[19] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher RÃ©.
Combining recurrent, convolutional, and continuous-time models with linear state space layers.
Advances in neural information processing systems, 34:572â€“585, 2021.
[20] Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models
without positional encodings still learn positional information. In Findings of the Association
for Computational Linguistics: EMNLP 2022, pages 1382â€“1390, 2022.
[21] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. In International
Conference on Learning Representations, 2020.
14
[22] Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. CUAD: An expert-annotated NLP
dataset for legal contract review. In Thirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1), 2021.
[23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
[24] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,
et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.
[25] Greg Kamradt. Needle in a haystack - pressure testing llms. <a class=link href=https://github.com/ target=_blank rel=noopener>https://github.com/</a>
gkamradt/LLMTest_NeedleInAHaystack/, 2023.
[26] Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge.
Transactions of the Association for Computational Linguistics, 6:317â€“328, 2018.
[27] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a
benchmark for question answering research. Transactions of the Association for Computational
Linguistics, 7:452â€“466, 2019.
[28] Stefan Larson, Anish Mahendran, Joseph J Peper, Christopher Clarke, Andrew Lee, Parker
Hill, Jonathan K Kummerfeld, Kevin Leach, Michael A Laurenzano, Lingjia Tang, et al. An
evaluation dataset for intent classification and out-of-scope prediction. In Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
1311â€“1316, 2019.
[29] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International
Conference on Computational Linguistics, 2002.
[30] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic
human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214â€“3252, Dublin, Ireland, May 2022.
Association for Computational Linguistics.
[31] Xingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser. Benchmarking natural language understanding services for building conversational agents. In Increasing Naturalness and
Flexibility in Spoken Dialogue Interaction: 10th International Workshop on Spoken Dialogue
Systems, pages 165â€“183. Springer, 2021.
[32] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting
of the association for computational linguistics: Human language technologies, pages 142â€“150,
2011.
[33] Sabrina J Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias GallÃ©,
Arun Raja, Chenglei Si, Wilson Y Lee, BenoÃ®t Sagot, et al. Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP. arXiv preprint
arXiv:2112.10508, 2021.
[34] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning
work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, pages 11048â€“11064, 2022.
[35] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning
and induction heads. arXiv preprint arXiv:2209.11895, 2022.
15
[36] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak,
Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative
study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024.
[37] Jonathan Pilault, Mahan Fathi, Orhan Firat, Christopher Pal, Pierre-Luc Bacon, and Ross
Goroshin. Block-state transformers. In Thirty-seventh Conference on Neural Information
Processing Systems, 2023.
[38] Maciej PiÃ³ro, Kamil Ciebiera, Krystian KrÃ³l, Jan Ludziejewski, and Sebastian Jaszczur.
MoE-Mamba: Efficient selective state space models with mixture of experts. arXiv preprint
arXiv:2401.04081, 2024.
[39] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher RÃ©. Hyena hierarchy: Towards larger convolutional
language models. In International Conference on Machine Learning, pages 28043â€“28078.
PMLR, 2023.
[40] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and
Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing
Models. <a class=link href=https://github.com/togethercomputer/stripedhyena target=_blank rel=noopener>https://github.com/togethercomputer/stripedhyena</a>, 2023.
[41] Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas,
Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for
large language models. In Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 6383â€“6402, 2023.
[42] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An
adversarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 34, pages 8732â€“8740, 2020.
[43] George Saon, Ankit Gupta, and Xiaodong Cui. Diagonal state space augmented transformers
for speech recognition. In ICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 1â€“5. IEEE, 2023.
[44] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare
words with subword units. In Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1715â€“1725, 2016.
[45] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
[46] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. In International Conference on Learning Representations, 2017.
[47] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
[48] Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won
Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging BIGBench tasks and whether chain-of-thought can solve them. In Findings of the Association for
Computational Linguistics: ACL 2023, pages 13003â€“13051, 2023.
[49] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya
Pathak, Laurent Sifre, Morgane RiviÃ¨re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open
models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.
[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
16
[52] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can
a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages 4791â€“4800, 2019.
[53] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural
Information Processing Systems, 32, 2019.
[54] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer,
and William Fedus. ST-MoE: Designing stable and transferable sparse expert models. arXiv
preprint arXiv:2202.08906, 2022.
[55] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng
Gao. Efficient long sequence modeling via state space augmented transformer. arXiv preprint
arXiv:2212.08136, 2022.
17
mambaæ˜¯ä»€ä¹ˆï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p><strong>Mamba</strong> æ˜¯ä¸€ç§<strong>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆState Space Model, SSMï¼‰</strong>ï¼Œä¸“ä¸ºé«˜æ•ˆå¤„ç†é•¿åºåˆ—ä»»åŠ¡è€Œè®¾è®¡ï¼Œæ˜¯è¿‘å¹´æ¥åºåˆ—å»ºæ¨¡é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æ–°æ¶æ„ã€‚å®ƒç”± Albert Gu å’Œ Tri Dao ç­‰äººåœ¨ 2023 å¹´æå‡ºï¼Œè®ºæ–‡ä¸º <a class=link href=https://arxiv.org/abs/2312.00752 target=_blank rel=noopener>Mamba: Linear-time Sequence Modeling with Selective State Spaces</a>ã€‚</p><hr><h2 id=1-mamba-çš„æ ¸å¿ƒæ€æƒ³>1. <strong>Mamba çš„æ ¸å¿ƒæ€æƒ³</strong></h2><ul><li><strong>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMï¼‰</strong>ï¼š<br>ä¼ ç»Ÿçš„ SSMï¼ˆå¦‚çº¿æ€§åŠ¨æ€ç³»ç»Ÿï¼‰é€šè¿‡â€œçŠ¶æ€â€æ¥è®°å¿†å’Œå¤„ç†åºåˆ—ä¿¡æ¯ï¼Œç†è®ºä¸Šå¯ä»¥æ•æ‰å¾ˆé•¿çš„ä¾èµ–å…³ç³»ã€‚</li><li><strong>é«˜æ•ˆé•¿åºåˆ—å»ºæ¨¡</strong>ï¼š<br>Mamba é€šè¿‡ç‰¹æ®Šçš„ç»“æ„å’Œä¼˜åŒ–ï¼Œä½¿å¾— SSM èƒ½åƒ Transformer ä¸€æ ·é«˜æ•ˆåœ°å¹¶è¡Œè®­ç»ƒï¼Œä½†æ¨ç†å’Œå†…å­˜æ¶ˆè€—è¿œä½äº Transformerï¼Œå°¤å…¶é€‚åˆè¶…é•¿åºåˆ—ï¼ˆå¦‚10ä¸‡tokenä»¥ä¸Šï¼‰ã€‚</li><li><strong>é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´ï¼ˆSelective State Spaceï¼‰</strong>ï¼š<br>Mamba å¼•å…¥äº†â€œé€‰æ‹©æ€§â€æœºåˆ¶ï¼Œèƒ½åŠ¨æ€å†³å®šå“ªäº›ä¿¡æ¯éœ€è¦è¢«è®°å¿†å’Œä¼ é€’ï¼Œæå‡äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ•ˆç‡ã€‚</li></ul><hr><h2 id=2-mamba-ä¸-transformer-çš„åŒºåˆ«>2. <strong>Mamba ä¸ Transformer çš„åŒºåˆ«</strong></h2><ul><li><strong>Transformer</strong> ä¾èµ–è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰ï¼Œæ¯ä¸ª token éƒ½è¦å’Œæ‰€æœ‰ token äº¤äº’ï¼Œè®¡ç®—å’Œå†…å­˜éšåºåˆ—é•¿åº¦äºŒæ¬¡å¢é•¿ï¼Œé•¿åºåˆ—æ—¶æ•ˆç‡ä½ä¸‹ã€‚</li><li><strong>Mamba</strong> é‡‡ç”¨ SSM ç»“æ„ï¼Œæ¨ç†æ—¶è®¡ç®—å’Œå†…å­˜éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼Œæå¤§æå‡äº†é•¿åºåˆ—å¤„ç†èƒ½åŠ›ã€‚</li><li><strong>Mamba</strong> è¿˜å¯ä»¥å’Œ Transformer ç»“åˆï¼ˆå¦‚ Jambaï¼‰ï¼Œå…¼é¡¾é•¿è·ç¦»å»ºæ¨¡èƒ½åŠ›å’Œé«˜æ•ˆæ¨ç†ã€‚</li></ul><hr><h2 id=3-mamba-çš„ä¼˜åŠ¿>3. <strong>Mamba çš„ä¼˜åŠ¿</strong></h2><ul><li><strong>çº¿æ€§å¤æ‚åº¦</strong>ï¼šæ¨ç†å’Œå†…å­˜æ¶ˆè€—éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼Œé€‚åˆè¶…é•¿æ–‡æœ¬ã€éŸ³é¢‘ã€DNAç­‰ä»»åŠ¡ã€‚</li><li><strong>å¹¶è¡Œè®­ç»ƒ</strong>ï¼šé€šè¿‡å·§å¦™è®¾è®¡ï¼ŒMamba è®­ç»ƒæ—¶å¯ä»¥åƒ Transformer ä¸€æ ·å¹¶è¡Œï¼Œæ•ˆç‡é«˜ã€‚</li><li><strong>å¯ä¸æ³¨æ„åŠ›æœºåˆ¶æ··åˆ</strong>ï¼šå¦‚ Jamba è¿™æ ·ï¼Œå°† Mamba å±‚å’Œ Transformer å±‚äº¤æ›¿ï¼Œå…¼é¡¾æ€§èƒ½å’Œæ•ˆç‡ã€‚</li></ul><hr><h2 id=4-åº”ç”¨åœºæ™¯>4. <strong>åº”ç”¨åœºæ™¯</strong></h2><ul><li><strong>é•¿æ–‡æœ¬/ä»£ç /æ–‡æ¡£å»ºæ¨¡</strong></li><li><strong>éŸ³é¢‘ã€è¯­éŸ³ã€åŸºå› åºåˆ—ç­‰é•¿åºåˆ—ä»»åŠ¡</strong></li><li><strong>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é•¿ä¸Šä¸‹æ–‡æ”¯æŒ</strong></li></ul><hr><h2 id=5-ä¸€å¥è¯æ€»ç»“>5. <strong>ä¸€å¥è¯æ€»ç»“</strong></h2><blockquote><p><strong>Mamba æ˜¯ä¸€ç§é«˜æ•ˆçš„çŠ¶æ€ç©ºé—´åºåˆ—å»ºæ¨¡æ¶æ„ï¼Œèƒ½ä»¥çº¿æ€§å¤æ‚åº¦å¤„ç†è¶…é•¿åºåˆ—ï¼Œæ˜¯ Transformer çš„æœ‰åŠ›è¡¥å……å’Œæ›¿ä»£æ–¹æ¡ˆä¹‹ä¸€ã€‚</strong></p></blockquote><hr><p>å¦‚éœ€æ›´è¯¦ç»†çš„åŸç†ã€ä»£ç ç¤ºä¾‹æˆ–ä¸ Transformer çš„å¯¹æ¯”åˆ†æï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼</p><hr><p><strong>User</strong></p><p>å›¾ç‰‡è½¬æ–‡å­—</p><hr><p><strong>Cursor</strong></p><p>ä»£ç å›¾ç‰‡çš„æ–‡å­—å†…å®¹å¦‚ä¸‹ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=kr>const</span> <span class=nx>Store</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>jwt</span><span class=o>:</span> <span class=kc>null</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nx>get</span> <span class=nx>loggedIn</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=k>this</span><span class=p>.</span><span class=nx>jwt</span> <span class=o>!==</span> <span class=kc>null</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>proxiedStore</span> <span class=o>=</span> <span class=k>new</span> <span class=nb>Proxy</span><span class=p>(</span><span class=nx>Store</span><span class=p>,</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>set</span><span class=o>:</span> <span class=p>(</span><span class=nx>target</span><span class=p>,</span> <span class=nx>prop</span><span class=p>,</span> <span class=nx>value</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>prop</span><span class=o>==</span><span class=s2>&#34;jwt&#34;</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>target</span><span class=p>[</span><span class=nx>prop</span><span class=p>]</span> <span class=o>=</span> <span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=nx>localStorage</span><span class=p>.</span><span class=nx>setItem</span><span class=p>(</span><span class=s2>&#34;jwt&#34;</span><span class=p>,</span> <span class=nx>value</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>});</span>
</span></span></code></pre></div><p>å¦‚éœ€è¿›ä¸€æ­¥è§£é‡Šä»£ç å«ä¹‰ï¼Œè¯·éšæ—¶å‘ŠçŸ¥ï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=kr>const</span> <span class=nx>Store</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>jwt</span><span class=o>:</span> <span class=kc>null</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nx>get</span> <span class=nx>loggedIn</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=k>this</span><span class=p>.</span><span class=nx>jwt</span> <span class=o>!==</span> <span class=kc>null</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kr>const</span> <span class=nx>proxiedStore</span> <span class=o>=</span> <span class=k>new</span> <span class=nb>Proxy</span><span class=p>(</span><span class=nx>Store</span><span class=p>,</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>set</span><span class=o>:</span> <span class=p>(</span><span class=nx>target</span><span class=p>,</span> <span class=nx>prop</span><span class=p>,</span> <span class=nx>value</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>prop</span><span class=o>==</span><span class=s2>&#34;jwt&#34;</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>target</span><span class=p>[</span><span class=nx>prop</span><span class=p>]</span> <span class=o>=</span> <span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=nx>localStorage</span><span class=p>.</span><span class=nx>setItem</span><span class=p>(</span><span class=s2>&#34;jwt&#34;</span><span class=p>,</span> <span class=nx>value</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>});</span>
</span></span></code></pre></div><hr><p><strong>User</strong></p><p>å›¾ç‰‡è½¬æ–‡å­—</p><hr><p><strong>Cursor</strong></p><p>å›¾ç‰‡ä¸­çš„ä»£ç å†…å®¹å¦‚ä¸‹ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=kr>const</span> <span class=nx>errors</span> <span class=o>=</span> <span class=p>[];</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>name</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>4</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter your complete name&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>password</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>7</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter a password with at least 7 characters&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>email</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>4</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter your complete email&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>password</span><span class=o>!=</span><span class=nx>passwordConfirmation</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Passwords don&#39;t match&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>errors</span><span class=p>.</span><span class=nx>length</span><span class=o>==</span><span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>response</span> <span class=o>=</span> <span class=kr>await</span> <span class=nx>API</span><span class=p>.</span><span class=nx>register</span><span class=p>(</span><span class=nx>name</span><span class=p>,</span> <span class=nx>email</span><span class=p>,</span> <span class=nx>password</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>response</span><span class=p>.</span><span class=nx>success</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nx>app</span><span class=p>.</span><span class=nx>Store</span>
</span></span><span class=line><span class=cl>        <span class=nx>app</span><span class=p>.</span><span class=nx>Router</span><span class=p>.</span><span class=nx>go</span><span class=p>(</span><span class=s2>&#34;/account/&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nx>app</span><span class=p>.</span><span class=nx>showError</span><span class=p>(</span><span class=nx>response</span><span class=p>.</span><span class=nx>message</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nx>app</span><span class=p>.</span><span class=nx>showError</span><span class=p>(</span><span class=nx>errors</span><span class=p>.</span><span class=nx>join</span><span class=p>(</span><span class=s2>&#34;. &#34;</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>å¦‚éœ€è¿›ä¸€æ­¥è§£é‡Šä»£ç å«ä¹‰ï¼Œè¯·éšæ—¶å‘ŠçŸ¥ï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=kr>const</span> <span class=nx>errors</span> <span class=o>=</span> <span class=p>[];</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>name</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>4</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter your complete name&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>password</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>7</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter a password with at least 7 characters&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>email</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>4</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter your complete email&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>password</span><span class=o>!=</span><span class=nx>passwordConfirmation</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Passwords don&#39;t match&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=nx>errors</span><span class=p>.</span><span class=nx>length</span><span class=o>==</span><span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>response</span> <span class=o>=</span> <span class=kr>await</span> <span class=nx>API</span><span class=p>.</span><span class=nx>register</span><span class=p>(</span><span class=nx>name</span><span class=p>,</span> <span class=nx>email</span><span class=p>,</span> <span class=nx>password</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>response</span><span class=p>.</span><span class=nx>success</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nx>app</span><span class=p>.</span><span class=nx>Store</span>
</span></span><span class=line><span class=cl>        <span class=nx>app</span><span class=p>.</span><span class=nx>Router</span><span class=p>.</span><span class=nx>go</span><span class=p>(</span><span class=s2>&#34;/account/&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nx>app</span><span class=p>.</span><span class=nx>showError</span><span class=p>(</span><span class=nx>response</span><span class=p>.</span><span class=nx>message</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nx>app</span><span class=p>.</span><span class=nx>showError</span><span class=p>(</span><span class=nx>errors</span><span class=p>.</span><span class=nx>join</span><span class=p>(</span><span class=s2>&#34;. &#34;</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><hr><p><strong>User</strong></p><p>formdata åç«¯Goè¯­è¨€å¦‚ä½•å¤„ç†ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>åœ¨ Go è¯­è¨€åç«¯å¤„ç†å‰ç«¯é€šè¿‡ <code>formdata</code>ï¼ˆé€šå¸¸æ˜¯ <code>multipart/form-data</code>ï¼Œæ¯”å¦‚è¡¨å•ä¸Šä¼ æ–‡ä»¶æˆ–æ™®é€šè¡¨å•æ•°æ®ï¼‰æ—¶ï¼Œå¸¸ç”¨çš„æ–¹å¼å¦‚ä¸‹ï¼š</p><hr><h2 id=1-å¤„ç†æ™®é€šè¡¨å•æ•°æ®>1. å¤„ç†æ™®é€šè¡¨å•æ•°æ®</h2><p>å¦‚æœå‰ç«¯ç”¨ <code>formdata</code> å‘é€æ™®é€šé”®å€¼å¯¹ï¼ˆå¦‚ç”¨æˆ·åã€å¯†ç ç­‰ï¼‰ï¼ŒGo åç«¯å¯ä»¥è¿™æ ·å¤„ç†ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>func</span><span class=w> </span><span class=nf>handler</span><span class=p>(</span><span class=nx>w</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>ResponseWriter</span><span class=p>,</span><span class=w> </span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>http</span><span class=p>.</span><span class=nx>Request</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è§£æè¡¨å•ï¼ˆåŒ…æ‹¬ multipart/form-data å’Œ application/x-www-form-urlencodedï¼‰</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>ParseMultipartForm</span><span class=p>(</span><span class=mi>32</span><span class=w> </span><span class=o>&lt;&lt;</span><span class=w> </span><span class=mi>20</span><span class=p>)</span><span class=w> </span><span class=c1>// 32MB æœ€å¤§å†…å­˜</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>http</span><span class=p>.</span><span class=nf>Error</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=p>.</span><span class=nf>Error</span><span class=p>(),</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>StatusBadRequest</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è·å–è¡¨å•å­—æ®µ</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>username</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormValue</span><span class=p>(</span><span class=s>&#34;username&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>password</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormValue</span><span class=p>(</span><span class=s>&#34;password&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Fprintf</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=s>&#34;username: %s, password: %s&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>username</span><span class=p>,</span><span class=w> </span><span class=nx>password</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><p>å¦‚æœåªæ˜¯æ™®é€šè¡¨å•ï¼ˆéæ–‡ä»¶ï¼‰ï¼Œä¹Ÿå¯ä»¥ç”¨ <code>r.ParseForm()</code>ï¼Œä½† <code>ParseMultipartForm</code> å…¼å®¹æ€§æ›´å¥½ã€‚</p><hr><h2 id=2-å¤„ç†æ–‡ä»¶ä¸Šä¼ >2. å¤„ç†æ–‡ä»¶ä¸Šä¼ </h2><p>å¦‚æœå‰ç«¯ç”¨ <code>formdata</code> ä¸Šä¼ æ–‡ä»¶ï¼ŒGo åç«¯è¿™æ ·å¤„ç†ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>func</span><span class=w> </span><span class=nf>uploadHandler</span><span class=p>(</span><span class=nx>w</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>ResponseWriter</span><span class=p>,</span><span class=w> </span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>http</span><span class=p>.</span><span class=nx>Request</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// é™åˆ¶æœ€å¤§ä¸Šä¼ å¤§å°ï¼ˆå¦‚10MBï¼‰</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>r</span><span class=p>.</span><span class=nf>ParseMultipartForm</span><span class=p>(</span><span class=mi>10</span><span class=w> </span><span class=o>&lt;&lt;</span><span class=w> </span><span class=mi>20</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è·å–æ–‡ä»¶å’Œæ–‡ä»¶å¤´</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>file</span><span class=p>,</span><span class=w> </span><span class=nx>handler</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormFile</span><span class=p>(</span><span class=s>&#34;file&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>http</span><span class=p>.</span><span class=nf>Error</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=p>.</span><span class=nf>Error</span><span class=p>(),</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>StatusBadRequest</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>defer</span><span class=w> </span><span class=nx>file</span><span class=p>.</span><span class=nf>Close</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è¯»å–æ–‡ä»¶å†…å®¹æˆ–ä¿å­˜åˆ°æœ¬åœ°</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>f</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>os</span><span class=p>.</span><span class=nf>Create</span><span class=p>(</span><span class=s>&#34;./uploads/&#34;</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=nx>handler</span><span class=p>.</span><span class=nx>Filename</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>http</span><span class=p>.</span><span class=nf>Error</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=p>.</span><span class=nf>Error</span><span class=p>(),</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>StatusInternalServerError</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>defer</span><span class=w> </span><span class=nx>f</span><span class=p>.</span><span class=nf>Close</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>io</span><span class=p>.</span><span class=nf>Copy</span><span class=p>(</span><span class=nx>f</span><span class=p>,</span><span class=w> </span><span class=nx>file</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Fprintf</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=s>&#34;File uploaded successfully: %s&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>handler</span><span class=p>.</span><span class=nx>Filename</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><hr><h2 id=3-ç»“åˆè¡¨å•å­—æ®µå’Œæ–‡ä»¶>3. ç»“åˆè¡¨å•å­—æ®µå’Œæ–‡ä»¶</h2><p>ä½ å¯ä»¥åŒæ—¶è·å–è¡¨å•å­—æ®µå’Œæ–‡ä»¶ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>func</span><span class=w> </span><span class=nf>uploadHandler</span><span class=p>(</span><span class=nx>w</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>ResponseWriter</span><span class=p>,</span><span class=w> </span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>http</span><span class=p>.</span><span class=nx>Request</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>r</span><span class=p>.</span><span class=nf>ParseMultipartForm</span><span class=p>(</span><span class=mi>10</span><span class=w> </span><span class=o>&lt;&lt;</span><span class=w> </span><span class=mi>20</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è·å–æ–‡æœ¬å­—æ®µ</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>username</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormValue</span><span class=p>(</span><span class=s>&#34;username&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è·å–æ–‡ä»¶</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>file</span><span class=p>,</span><span class=w> </span><span class=nx>handler</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormFile</span><span class=p>(</span><span class=s>&#34;avatar&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// ...åŒä¸Š</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Fprintf</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=s>&#34;User: %s, File: %s&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>username</span><span class=p>,</span><span class=w> </span><span class=nx>handler</span><span class=p>.</span><span class=nx>Filename</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><hr><h2 id=4-è·¯ç”±æ³¨å†Œç¤ºä¾‹>4. è·¯ç”±æ³¨å†Œç¤ºä¾‹</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>http</span><span class=p>.</span><span class=nf>HandleFunc</span><span class=p>(</span><span class=s>&#34;/upload&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>uploadHandler</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>http</span><span class=p>.</span><span class=nf>ListenAndServe</span><span class=p>(</span><span class=s>&#34;:8080&#34;</span><span class=p>,</span><span class=w> </span><span class=kc>nil</span><span class=p>)</span><span class=w>
</span></span></span></code></pre></div><hr><h2 id=5-æ³¨æ„äº‹é¡¹>5. æ³¨æ„äº‹é¡¹</h2><ul><li>å‰ç«¯ <code>formdata</code> çš„ key è¦å’Œåç«¯è·å–çš„å­—æ®µåä¸€è‡´ã€‚</li><li>æ–‡ä»¶ä¸Šä¼ æ—¶ï¼Œ<code>r.FormFile("file")</code> çš„ <code>"file"</code> è¦å’Œå‰ç«¯ <code>&lt;input type="file" name="file"></code> æˆ– <code>formdata.append("file", ...)</code> çš„ key ä¸€è‡´ã€‚</li><li>å¦‚æœåªå¤„ç†æ™®é€šè¡¨å•æ•°æ®ï¼ˆéæ–‡ä»¶ï¼‰ï¼Œå¯ä»¥ç”¨ <code>r.ParseForm()</code>ã€‚</li></ul><hr><p>å¦‚éœ€æ›´è¯¦ç»†çš„ä¾‹å­ï¼ˆå¦‚ç”¨ Ginã€Echo ç­‰æ¡†æ¶ï¼‰ï¼Œæˆ–æœ‰ç‰¹æ®Šéœ€æ±‚ï¼Œæ¬¢è¿è¡¥å……è¯´æ˜ï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>func</span><span class=w> </span><span class=nf>handler</span><span class=p>(</span><span class=nx>w</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>ResponseWriter</span><span class=p>,</span><span class=w> </span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>http</span><span class=p>.</span><span class=nx>Request</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è§£æè¡¨å•ï¼ˆåŒ…æ‹¬ multipart/form-data å’Œ application/x-www-form-urlencodedï¼‰</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>ParseMultipartForm</span><span class=p>(</span><span class=mi>32</span><span class=w> </span><span class=o>&lt;&lt;</span><span class=w> </span><span class=mi>20</span><span class=p>)</span><span class=w> </span><span class=c1>// 32MB æœ€å¤§å†…å­˜</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>http</span><span class=p>.</span><span class=nf>Error</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=p>.</span><span class=nf>Error</span><span class=p>(),</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>StatusBadRequest</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è·å–è¡¨å•å­—æ®µ</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>username</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormValue</span><span class=p>(</span><span class=s>&#34;username&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>password</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormValue</span><span class=p>(</span><span class=s>&#34;password&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Fprintf</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=s>&#34;username: %s, password: %s&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>username</span><span class=p>,</span><span class=w> </span><span class=nx>password</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>func</span><span class=w> </span><span class=nf>uploadHandler</span><span class=p>(</span><span class=nx>w</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>ResponseWriter</span><span class=p>,</span><span class=w> </span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>http</span><span class=p>.</span><span class=nx>Request</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// é™åˆ¶æœ€å¤§ä¸Šä¼ å¤§å°ï¼ˆå¦‚10MBï¼‰</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>r</span><span class=p>.</span><span class=nf>ParseMultipartForm</span><span class=p>(</span><span class=mi>10</span><span class=w> </span><span class=o>&lt;&lt;</span><span class=w> </span><span class=mi>20</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è·å–æ–‡ä»¶å’Œæ–‡ä»¶å¤´</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>file</span><span class=p>,</span><span class=w> </span><span class=nx>handler</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormFile</span><span class=p>(</span><span class=s>&#34;file&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>http</span><span class=p>.</span><span class=nf>Error</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=p>.</span><span class=nf>Error</span><span class=p>(),</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>StatusBadRequest</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>defer</span><span class=w> </span><span class=nx>file</span><span class=p>.</span><span class=nf>Close</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è¯»å–æ–‡ä»¶å†…å®¹æˆ–ä¿å­˜åˆ°æœ¬åœ°</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>f</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>os</span><span class=p>.</span><span class=nf>Create</span><span class=p>(</span><span class=s>&#34;./uploads/&#34;</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=nx>handler</span><span class=p>.</span><span class=nx>Filename</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>if</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>!=</span><span class=w> </span><span class=kc>nil</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nx>http</span><span class=p>.</span><span class=nf>Error</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=p>.</span><span class=nf>Error</span><span class=p>(),</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>StatusInternalServerError</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=k>return</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=p>}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>defer</span><span class=w> </span><span class=nx>f</span><span class=p>.</span><span class=nf>Close</span><span class=p>()</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>io</span><span class=p>.</span><span class=nf>Copy</span><span class=p>(</span><span class=nx>f</span><span class=p>,</span><span class=w> </span><span class=nx>file</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Fprintf</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=s>&#34;File uploaded successfully: %s&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>handler</span><span class=p>.</span><span class=nx>Filename</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>func</span><span class=w> </span><span class=nf>uploadHandler</span><span class=p>(</span><span class=nx>w</span><span class=w> </span><span class=nx>http</span><span class=p>.</span><span class=nx>ResponseWriter</span><span class=p>,</span><span class=w> </span><span class=nx>r</span><span class=w> </span><span class=o>*</span><span class=nx>http</span><span class=p>.</span><span class=nx>Request</span><span class=p>)</span><span class=w> </span><span class=p>{</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>r</span><span class=p>.</span><span class=nf>ParseMultipartForm</span><span class=p>(</span><span class=mi>10</span><span class=w> </span><span class=o>&lt;&lt;</span><span class=w> </span><span class=mi>20</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è·å–æ–‡æœ¬å­—æ®µ</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>username</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormValue</span><span class=p>(</span><span class=s>&#34;username&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// è·å–æ–‡ä»¶</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>file</span><span class=p>,</span><span class=w> </span><span class=nx>handler</span><span class=p>,</span><span class=w> </span><span class=nx>err</span><span class=w> </span><span class=o>:=</span><span class=w> </span><span class=nx>r</span><span class=p>.</span><span class=nf>FormFile</span><span class=p>(</span><span class=s>&#34;avatar&#34;</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=c1>// ...åŒä¸Š</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nx>fmt</span><span class=p>.</span><span class=nf>Fprintf</span><span class=p>(</span><span class=nx>w</span><span class=p>,</span><span class=w> </span><span class=s>&#34;User: %s, File: %s&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>username</span><span class=p>,</span><span class=w> </span><span class=nx>handler</span><span class=p>.</span><span class=nx>Filename</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>}</span><span class=w>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=nx>http</span><span class=p>.</span><span class=nf>HandleFunc</span><span class=p>(</span><span class=s>&#34;/upload&#34;</span><span class=p>,</span><span class=w> </span><span class=nx>uploadHandler</span><span class=p>)</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nx>http</span><span class=p>.</span><span class=nf>ListenAndServe</span><span class=p>(</span><span class=s>&#34;:8080&#34;</span><span class=p>,</span><span class=w> </span><span class=kc>nil</span><span class=p>)</span><span class=w>
</span></span></span></code></pre></div><hr><p><strong>User</strong></p><p>å›¾ç‰‡è½¬æ–‡å­—</p><hr><p><strong>Cursor</strong></p><p>å›¾ç‰‡ä¸­çš„ä»£ç å†…å®¹å¦‚ä¸‹ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=nx>login</span><span class=o>:</span> <span class=kr>async</span> <span class=p>(</span><span class=nx>event</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nx>event</span><span class=p>.</span><span class=nx>preventDefault</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>email</span> <span class=o>=</span> <span class=nb>document</span><span class=p>.</span><span class=nx>getElementById</span><span class=p>(</span><span class=s2>&#34;login-email&#34;</span><span class=p>).</span><span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>password</span> <span class=o>=</span> <span class=nb>document</span><span class=p>.</span><span class=nx>getElementById</span><span class=p>(</span><span class=s2>&#34;login-password&#34;</span><span class=p>).</span><span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>errors</span> <span class=o>=</span> <span class=p>[];</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>password</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>7</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter a password with at least 7 characters&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>email</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>4</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter your complete email&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>errors</span><span class=p>.</span><span class=nx>length</span><span class=o>==</span><span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kr>const</span> <span class=nx>response</span> <span class=o>=</span> <span class=kr>await</span> <span class=nx>API</span><span class=p>.</span><span class=nx>login</span><span class=p>(</span><span class=nx>email</span><span class=p>,</span> <span class=nx>password</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=nx>response</span><span class=p>.</span><span class=nx>success</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=nx>app</span><span class=p>.</span><span class=nx>Store</span><span class=p>.</span><span class=nx>jwt</span> <span class=o>=</span> <span class=nx>response</span><span class=p>.</span><span class=nx>jwt</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=nx>app</span><span class=p>.</span><span class=nx>Router</span><span class=p>.</span><span class=nx>go</span><span class=p>(</span><span class=s2>&#34;/account/&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=nx>app</span><span class=p>.</span><span class=nx>showError</span><span class=p>(</span><span class=nx>response</span><span class=p>.</span><span class=nx>message</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nx>app</span><span class=p>.</span><span class=nx>showError</span><span class=p>(</span><span class=nx>errors</span><span class=p>.</span><span class=nx>join</span><span class=p>(</span><span class=s2>&#34;. &#34;</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>å¦‚éœ€è¿›ä¸€æ­¥è§£é‡Šä»£ç å«ä¹‰ï¼Œè¯·éšæ—¶å‘ŠçŸ¥ï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=nx>login</span><span class=o>:</span> <span class=kr>async</span> <span class=p>(</span><span class=nx>event</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nx>event</span><span class=p>.</span><span class=nx>preventDefault</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>email</span> <span class=o>=</span> <span class=nb>document</span><span class=p>.</span><span class=nx>getElementById</span><span class=p>(</span><span class=s2>&#34;login-email&#34;</span><span class=p>).</span><span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>password</span> <span class=o>=</span> <span class=nb>document</span><span class=p>.</span><span class=nx>getElementById</span><span class=p>(</span><span class=s2>&#34;login-password&#34;</span><span class=p>).</span><span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kr>const</span> <span class=nx>errors</span> <span class=o>=</span> <span class=p>[];</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>password</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>7</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter a password with at least 7 characters&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>email</span><span class=p>.</span><span class=nx>length</span> <span class=o>&lt;</span> <span class=mi>4</span><span class=p>)</span> <span class=nx>errors</span><span class=p>.</span><span class=nx>push</span><span class=p>(</span><span class=s2>&#34;Enter your complete email&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>errors</span><span class=p>.</span><span class=nx>length</span><span class=o>==</span><span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kr>const</span> <span class=nx>response</span> <span class=o>=</span> <span class=kr>await</span> <span class=nx>API</span><span class=p>.</span><span class=nx>login</span><span class=p>(</span><span class=nx>email</span><span class=p>,</span> <span class=nx>password</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=nx>response</span><span class=p>.</span><span class=nx>success</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=nx>app</span><span class=p>.</span><span class=nx>Store</span><span class=p>.</span><span class=nx>jwt</span> <span class=o>=</span> <span class=nx>response</span><span class=p>.</span><span class=nx>jwt</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=nx>app</span><span class=p>.</span><span class=nx>Router</span><span class=p>.</span><span class=nx>go</span><span class=p>(</span><span class=s2>&#34;/account/&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=nx>app</span><span class=p>.</span><span class=nx>showError</span><span class=p>(</span><span class=nx>response</span><span class=p>.</span><span class=nx>message</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=nx>app</span><span class=p>.</span><span class=nx>showError</span><span class=p>(</span><span class=nx>errors</span><span class=p>.</span><span class=nx>join</span><span class=p>(</span><span class=s2>&#34;. &#34;</span><span class=p>));</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><hr><p><strong>User</strong></p><p>Uncaught (in promise) TypeError: &lsquo;set&rsquo; on proxy: trap returned falsish for property &lsquo;jwt&rsquo;
at Object.login (app.js:95:31)
login @ app.js:95
await in login
onsubmit @ login:140</p><p>è¿™ä¸ªé”™è¯¯æ˜¯ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>è¿™ä¸ªé”™è¯¯ä¿¡æ¯ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Uncaught (in promise) TypeError: &#39;set&#39; on proxy: trap returned falsish for property &#39;jwt&#39;
</span></span><span class=line><span class=cl>    at Object.login (app.js:95:31)
</span></span></code></pre></div><p><strong>æ„æ€æ˜¯ï¼š</strong><br>ä½ ç”¨ Proxy åŒ…è£¹äº†ä¸€ä¸ªå¯¹è±¡ï¼ˆå¦‚ <code>Store</code>ï¼‰ï¼Œå¹¶è‡ªå®šä¹‰äº† <code>set</code> æ–¹æ³•ï¼ˆtrapï¼‰ï¼Œä½†ä½ çš„ <code>set</code> æ–¹æ³•<strong>æ²¡æœ‰è¿”å› <code>true</code></strong>ï¼Œå¯¼è‡´ Proxy è®¤ä¸ºè®¾ç½®å±æ€§å¤±è´¥ï¼Œäºæ˜¯æŠ›å‡ºäº†è¿™ä¸ªé”™è¯¯ã€‚</p><hr><h2 id=è¯¦ç»†è§£é‡Š>è¯¦ç»†è§£é‡Š</h2><p>åœ¨ JavaScript çš„ Proxy é‡Œï¼Œ<code>set</code> trap å¿…é¡»è¿”å›ä¸€ä¸ªå¸ƒå°”å€¼ï¼ˆ<code>true</code> è¡¨ç¤ºè®¾ç½®æˆåŠŸï¼Œ<code>false</code> è¡¨ç¤ºå¤±è´¥ï¼‰ã€‚<br>å¦‚æœä½ æ²¡æœ‰æ˜¾å¼è¿”å› <code>true</code>ï¼Œé»˜è®¤è¿”å› <code>undefined</code>ï¼ˆå³â€œfalsishâ€ï¼‰ï¼Œå°±ä¼šæŠ¥è¿™ä¸ªé”™ã€‚</p><p><strong>ä½ çš„ä»£ç ç±»ä¼¼è¿™æ ·ï¼š</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-js data-lang=js><span class=line><span class=cl><span class=kr>const</span> <span class=nx>proxiedStore</span> <span class=o>=</span> <span class=k>new</span> <span class=nb>Proxy</span><span class=p>(</span><span class=nx>Store</span><span class=p>,</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>set</span><span class=o>:</span> <span class=p>(</span><span class=nx>target</span><span class=p>,</span> <span class=nx>prop</span><span class=p>,</span> <span class=nx>value</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>prop</span> <span class=o>==</span> <span class=s2>&#34;jwt&#34;</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>target</span><span class=p>[</span><span class=nx>prop</span><span class=p>]</span> <span class=o>=</span> <span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=nx>localStorage</span><span class=p>.</span><span class=nx>setItem</span><span class=p>(</span><span class=s2>&#34;jwt&#34;</span><span class=p>,</span> <span class=nx>value</span><span class=p>);</span>
</span></span><span class=line><span class=cl>      <span class=c1>// è¿™é‡Œç¼ºå°‘ return true
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// è¿™é‡Œä¹Ÿç¼ºå°‘ return true/false
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>});</span>
</span></span></code></pre></div><hr><h2 id=è§£å†³æ–¹æ³•>è§£å†³æ–¹æ³•</h2><p><strong>åœ¨ <code>set</code> trap ç»“å°¾åŠ ä¸Š <code>return true;</code>ï¼š</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-js data-lang=js><span class=line><span class=cl><span class=kr>const</span> <span class=nx>proxiedStore</span> <span class=o>=</span> <span class=k>new</span> <span class=nb>Proxy</span><span class=p>(</span><span class=nx>Store</span><span class=p>,</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>set</span><span class=o>:</span> <span class=p>(</span><span class=nx>target</span><span class=p>,</span> <span class=nx>prop</span><span class=p>,</span> <span class=nx>value</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>prop</span> <span class=o>==</span> <span class=s2>&#34;jwt&#34;</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>target</span><span class=p>[</span><span class=nx>prop</span><span class=p>]</span> <span class=o>=</span> <span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=nx>localStorage</span><span class=p>.</span><span class=nx>setItem</span><span class=p>(</span><span class=s2>&#34;jwt&#34;</span><span class=p>,</span> <span class=nx>value</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>target</span><span class=p>[</span><span class=nx>prop</span><span class=p>]</span> <span class=o>=</span> <span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=kc>true</span><span class=p>;</span> <span class=c1>// å¿…é¡»è¿”å› true
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>});</span>
</span></span></code></pre></div><hr><h2 id=æ€»ç»“-5>æ€»ç»“</h2><ul><li>Proxy çš„ <code>set</code> trap å¿…é¡»è¿”å› <code>true</code>ï¼ˆæˆåŠŸï¼‰æˆ– <code>false</code>ï¼ˆå¤±è´¥ï¼‰ã€‚</li><li>ä½ æ²¡è¿”å›ï¼Œé»˜è®¤æ˜¯ <code>undefined</code>ï¼Œæ‰€ä»¥æŠ¥é”™ã€‚</li><li><strong>åŠ ä¸Š <code>return true;</code> å³å¯è§£å†³ã€‚</strong></li></ul><p>å¦‚éœ€è¿›ä¸€æ­¥è§£é‡Šæˆ–æœ‰å…¶ä»–æŠ¥é”™ï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>Uncaught (in promise) TypeError: &#39;set&#39; on proxy: trap returned falsish for property &#39;jwt&#39;
</span></span><span class=line><span class=cl>    at Object.login (app.js:95:31)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=kr>const</span> <span class=nx>proxiedStore</span> <span class=o>=</span> <span class=k>new</span> <span class=nb>Proxy</span><span class=p>(</span><span class=nx>Store</span><span class=p>,</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>set</span><span class=o>:</span> <span class=p>(</span><span class=nx>target</span><span class=p>,</span> <span class=nx>prop</span><span class=p>,</span> <span class=nx>value</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>prop</span> <span class=o>==</span> <span class=s2>&#34;jwt&#34;</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>target</span><span class=p>[</span><span class=nx>prop</span><span class=p>]</span> <span class=o>=</span> <span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=nx>localStorage</span><span class=p>.</span><span class=nx>setItem</span><span class=p>(</span><span class=s2>&#34;jwt&#34;</span><span class=p>,</span> <span class=nx>value</span><span class=p>);</span>
</span></span><span class=line><span class=cl>      <span class=c1>// è¿™é‡Œç¼ºå°‘ return true
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=c1>// è¿™é‡Œä¹Ÿç¼ºå°‘ return true/false
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>});</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-javascript data-lang=javascript><span class=line><span class=cl><span class=kr>const</span> <span class=nx>proxiedStore</span> <span class=o>=</span> <span class=k>new</span> <span class=nb>Proxy</span><span class=p>(</span><span class=nx>Store</span><span class=p>,</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>set</span><span class=o>:</span> <span class=p>(</span><span class=nx>target</span><span class=p>,</span> <span class=nx>prop</span><span class=p>,</span> <span class=nx>value</span><span class=p>)</span> <span class=p>=&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=nx>prop</span> <span class=o>==</span> <span class=s2>&#34;jwt&#34;</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>target</span><span class=p>[</span><span class=nx>prop</span><span class=p>]</span> <span class=o>=</span> <span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>      <span class=nx>localStorage</span><span class=p>.</span><span class=nx>setItem</span><span class=p>(</span><span class=s2>&#34;jwt&#34;</span><span class=p>,</span> <span class=nx>value</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nx>target</span><span class=p>[</span><span class=nx>prop</span><span class=p>]</span> <span class=o>=</span> <span class=nx>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=kc>true</span><span class=p>;</span> <span class=c1>// å¿…é¡»è¿”å› true
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>});</span>
</span></span></code></pre></div><hr><p><strong>User</strong></p><p>è¿™äº›æŒ‡æ ‡æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>è¿™å¼ å›¾å±•ç¤ºçš„æ˜¯<strong>ç½‘é¡µæ€§èƒ½çš„æ ¸å¿ƒæŒ‡æ ‡ï¼ˆCore Web Vitalsï¼‰</strong>ï¼Œä¸»è¦ç”¨äºè¡¡é‡ç”¨æˆ·ä½“éªŒã€‚ä¸‹é¢æ˜¯æ¯ä¸ªæŒ‡æ ‡çš„è§£é‡Šï¼š</p><hr><h3 id=1-largest-contentful-paint-lcp>1. <strong>Largest Contentful Paint (LCP)</strong></h3><ul><li><strong>å«ä¹‰</strong>ï¼šæœ€å¤§å†…å®¹ç»˜åˆ¶æ—¶é—´ã€‚æŒ‡é¡µé¢ä¸­æœ€å¤§å¯è§å†…å®¹ï¼ˆå¦‚å¤§å›¾ç‰‡ã€ä¸»æ ‡é¢˜ç­‰ï¼‰æ¸²æŸ“å®Œæˆæ‰€éœ€çš„æ—¶é—´ã€‚</li><li><strong>ä½ çš„æ•°æ®</strong>ï¼š<strong>1.35ç§’</strong>ï¼ˆç»¿è‰²ï¼Œè¯´æ˜å¾ˆå¥½ï¼‰</li><li><strong>æ ‡å‡†</strong>ï¼šå°äº2.5ç§’ä¸ºå¥½ï¼Œ2.5-4ç§’éœ€æ”¹è¿›ï¼Œå¤§äº4ç§’å·®ã€‚</li><li><strong>LCP element</strong>ï¼šimgï¼Œè¯´æ˜æœ€å¤§å†…å®¹æ˜¯å›¾ç‰‡ã€‚</li></ul><hr><h3 id=2-cumulative-layout-shift-cls>2. <strong>Cumulative Layout Shift (CLS)</strong></h3><ul><li><strong>å«ä¹‰</strong>ï¼šç´¯è®¡å¸ƒå±€åç§»ã€‚è¡¡é‡é¡µé¢åœ¨åŠ è½½è¿‡ç¨‹ä¸­å†…å®¹çš„â€œè·³åŠ¨â€æˆ–â€œæŠ–åŠ¨â€ç¨‹åº¦ï¼ˆæ¯”å¦‚å›¾ç‰‡åŠ è½½åæŠŠæ–‡å­—æŒ¤ä¸‹å»ï¼‰ã€‚</li><li><strong>ä½ çš„æ•°æ®</strong>ï¼š<strong>0.19</strong>ï¼ˆæ©™è‰²ï¼Œè¯´æ˜éœ€è¦æ”¹è¿›ï¼‰</li><li><strong>æ ‡å‡†</strong>ï¼šå°äº0.1ä¸ºå¥½ï¼Œ0.1-0.25éœ€æ”¹è¿›ï¼Œå¤§äº0.25å·®ã€‚</li><li><strong>Worst cluster</strong>ï¼š2 shiftsï¼Œè¯´æ˜æœ‰ä¸¤æ¬¡æ˜æ˜¾çš„å¸ƒå±€è·³åŠ¨ã€‚</li></ul><hr><h3 id=3-interaction-to-next-paint-inp>3. <strong>Interaction to Next Paint (INP)</strong></h3><ul><li><strong>å«ä¹‰</strong>ï¼šäº¤äº’åˆ°ä¸‹æ¬¡ç»˜åˆ¶çš„æ—¶é—´ã€‚è¡¡é‡ç”¨æˆ·ä¸é¡µé¢äº¤äº’ï¼ˆå¦‚ç‚¹å‡»ã€è¾“å…¥ï¼‰åï¼Œé¡µé¢å“åº”å¹¶å®Œæˆè§†è§‰æ›´æ–°çš„é€Ÿåº¦ã€‚</li><li><strong>ä½ çš„æ•°æ®</strong>ï¼š<strong>64æ¯«ç§’</strong>ï¼ˆç»¿è‰²ï¼Œéå¸¸å¿«ï¼‰</li><li><strong>æ ‡å‡†</strong>ï¼šå°äº200msä¸ºå¥½ï¼Œ200-500mséœ€æ”¹è¿›ï¼Œå¤§äº500mså·®ã€‚</li></ul><hr><h2 id=æ€»ç»“-6>æ€»ç»“</h2><ul><li><strong>LCP 1.35s</strong>ï¼šå¾ˆå¥½ï¼Œé¡µé¢ä¸»å†…å®¹åŠ è½½å¾ˆå¿«ã€‚</li><li><strong>CLS 0.19</strong>ï¼šæœ‰ç‚¹é«˜ï¼Œé¡µé¢æœ‰å¯è§çš„è·³åŠ¨ï¼Œéœ€è¦ä¼˜åŒ–ï¼ˆæ¯”å¦‚å›¾ç‰‡/å¹¿å‘Š/å­—ä½“åŠ è½½æ—¶é¢„ç•™ç©ºé—´ï¼‰ã€‚</li><li><strong>INP 64ms</strong>ï¼šéå¸¸å¥½ï¼Œé¡µé¢å“åº”å¾ˆå¿«ã€‚</li></ul><hr><p><strong>ä¼˜åŒ–å»ºè®®ï¼š</strong></p><ul><li>é‡ç‚¹ä¼˜åŒ– CLSï¼Œå‡å°‘é¡µé¢åŠ è½½æ—¶çš„å¸ƒå±€å˜åŒ–ï¼ˆå¦‚ä¸ºå›¾ç‰‡ã€å¹¿å‘Šç­‰å…ƒç´ é¢„è®¾å®½é«˜ï¼Œé¿å…å¼‚æ­¥å†…å®¹æ’å…¥å¯¼è‡´è·³åŠ¨ï¼‰ã€‚</li></ul><p>å¦‚éœ€å…·ä½“ä¼˜åŒ–æ–¹æ³•æˆ–æ›´è¯¦ç»†è§£é‡Šï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼</p><hr><p><strong>User</strong></p><p>Plumnberæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>ä½ é—®çš„ <strong>Plumnber</strong>ï¼Œè¿™ä¸ªè¯å…¶å®æ˜¯<strong>æ‹¼å†™é”™è¯¯</strong>ï¼Œæ­£ç¡®çš„å•è¯åº”è¯¥æ˜¯ <strong>Plumber</strong>ã€‚</p><hr><h2 id=plumber-çš„æ„æ€>Plumber çš„æ„æ€</h2><h3 id=1-è‹±æ–‡é‡Šä¹‰>1. <strong>è‹±æ–‡é‡Šä¹‰</strong></h3><ul><li><strong>Plumber</strong>ï¼ˆåè¯ï¼‰ï¼šæ°´ç®¡å·¥ï¼Œç®¡é“å·¥</li><li>æŒ‡è´Ÿè´£å®‰è£…ã€ä¿®ç†æ°´ç®¡ã€ä¸‹æ°´é“ã€æš–æ°”ç®¡é“ç­‰çš„å·¥äººã€‚</li></ul><h3 id=2-å¸¸è§ç”¨æ³•>2. <strong>å¸¸è§ç”¨æ³•</strong></h3><ul><li><strong>He called a plumber to fix the leaking pipe.</strong><br>ä»–å«äº†ä¸ªæ°´ç®¡å·¥æ¥ä¿®æ¼æ°´çš„ç®¡å­ã€‚</li></ul><h3 id=3-itç¼–ç¨‹é¢†åŸŸ>3. <strong>IT/ç¼–ç¨‹é¢†åŸŸ</strong></h3><ul><li>åœ¨ç¼–ç¨‹é¢†åŸŸï¼Œ<strong>Plumber</strong> ä¹Ÿæ˜¯ä¸€äº›å·¥å…·/åº“çš„åå­—ï¼Œæ¯”å¦‚ï¼š<ul><li><strong>R è¯­è¨€çš„ Plumber</strong>ï¼šä¸€ä¸ªç”¨æ¥æŠŠ R ä»£ç å¿«é€Ÿå˜æˆ Web API çš„åŒ…ï¼ˆ<a class=link href=https://www.rplumber.io/ target=_blank rel=noopener>Plumber for R</a>ï¼‰ã€‚</li><li>å…¶ä»–ç¼–ç¨‹è¯­è¨€æˆ–æ¡†æ¶ä¸­ä¹Ÿå¯èƒ½æœ‰å« plumber çš„åº“ï¼Œé€šå¸¸å’Œâ€œç®¡é“â€ã€â€œæµå¤„ç†â€ç›¸å…³ã€‚</li></ul></li></ul><hr><h2 id=æ€»ç»“-7>æ€»ç»“</h2><ul><li><strong>Plumnber</strong> æ²¡æœ‰å®é™…å«ä¹‰ï¼Œåº”è¯¥æ˜¯ <strong>Plumber</strong> çš„æ‹¼å†™é”™è¯¯ã€‚</li><li><strong>Plumber</strong> ä¸»è¦æŒ‡â€œæ°´ç®¡å·¥â€ï¼Œåœ¨ IT é¢†åŸŸä¹Ÿå¸¸ç”¨ä½œå·¥å…·/åº“åã€‚</li></ul><p>å¦‚æœ‰å…·ä½“ä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚ä½ åœ¨å“ªçœ‹åˆ°çš„ Plumnberï¼‰ï¼Œå¯ä»¥è¡¥å……è¯´æ˜ï¼Œæˆ‘å¯ä»¥å¸®ä½ æ›´ç²¾ç¡®åœ°è§£é‡Šï¼</p><hr><p><strong>User</strong></p><p>PS C:\Users\tannal\tannalwork\projects\parley> cp .\slides-typst-meetup-24-lilaq\ ..
cp : å…·æœ‰æŒ‡å®šåç§° C:\Users\tannal\tannalwork\projects\slides-typst-meetup-24-lilaq çš„é¡¹å·²å­˜åœ¨ã€‚
æ‰€åœ¨ä½ç½® è¡Œ:1 å­—ç¬¦: 1</p><ul><li>cp .\slides-typst-meetup-24-lilaq\ ..</li><li><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>  + CategoryInfo          : ResourceExists: (C:\Users\tannal...meetup-24-lilaq:String) [Copy-Item], IOException
</span></span><span class=line><span class=cl>  + FullyQualifiedErrorId : DirectoryExist,Microsoft.PowerShell.Commands.CopyItemCommand
</span></span></code></pre></div></li></ul><p>PS C:\Users\tannal\tannalwork\projects\parley></p><p>poershellå‘½ä»¤</p><hr><p><strong>Cursor</strong></p><p>ä½ é‡åˆ°çš„ PowerShell æŠ¥é”™ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cp : å…·æœ‰æŒ‡å®šåç§° C:\Users\tannal\tannalwork\projects\slides-typst-meetup-24-lilaq çš„é¡¹å·²å­˜åœ¨ã€‚
</span></span></code></pre></div><p>æ„æ€æ˜¯<strong>ç›®æ ‡ç›®å½•å·²ç»å­˜åœ¨</strong>ï¼Œæ‰€ä»¥ <code>cp</code>ï¼ˆ<code>Copy-Item</code>ï¼‰å‘½ä»¤æ— æ³•è¦†ç›–å·²å­˜åœ¨çš„åŒåç›®å½•ã€‚</p><hr><h2 id=è§£å†³æ–¹æ³•-1>è§£å†³æ–¹æ³•</h2><h3 id=1-åŠ ä¸Š>1. <strong>åŠ ä¸Š <code>-Recurse -Force</code> å¹¶æŒ‡å®šæ–°ç›®å½•å</strong></h3><p>å¦‚æœä½ æƒ³æŠŠç›®å½•å¤åˆ¶ä¸ºä¸€ä¸ªæ–°åå­—ï¼ˆæ¯”å¦‚ <code>slides-typst-meetup-24-lilaq-copy</code>ï¼‰ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=nb>cp </span><span class=p>.\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span> <span class=p>..\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq-copy</span> <span class=n>-Recurse</span>
</span></span></code></pre></div><h3 id=2-è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•æ…ç”¨>2. <strong>è¦†ç›–å·²å­˜åœ¨çš„ç›®å½•ï¼ˆæ…ç”¨ï¼ï¼‰</strong></h3><p>PowerShell çš„ <code>cp</code>/<code>Copy-Item</code> é»˜è®¤<strong>ä¸ä¼šè¦†ç›–å·²å­˜åœ¨çš„ç›®å½•</strong>ã€‚å¦‚æœä½ ç¡®å®è¦è¦†ç›–ï¼Œå¯ä»¥å…ˆåˆ é™¤ç›®æ ‡ç›®å½•ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=nb>Remove-Item</span> <span class=p>..\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span> <span class=n>-Recurse</span> <span class=n>-Force</span>
</span></span><span class=line><span class=cl><span class=nb>cp </span><span class=p>.\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span> <span class=p>..\</span> <span class=n>-Recurse</span>
</span></span></code></pre></div><blockquote><p>âš ï¸ æ³¨æ„ï¼š<code>Remove-Item</code> ä¼šåˆ é™¤ç›®æ ‡ç›®å½•åŠå…¶æ‰€æœ‰å†…å®¹ï¼Œè¯·è°¨æ…æ“ä½œï¼</p></blockquote><h3 id=3-åªå¤åˆ¶ç›®å½•å†…å®¹åˆ°å·²å­˜åœ¨ç›®å½•>3. <strong>åªå¤åˆ¶ç›®å½•å†…å®¹åˆ°å·²å­˜åœ¨ç›®å½•</strong></h3><p>å¦‚æœä½ åªæ˜¯æƒ³æŠŠå†…å®¹åˆå¹¶åˆ°ä¸Šä¸€çº§ç›®å½•ä¸‹çš„åŒåç›®å½•ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=nb>cp </span><span class=p>.\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span><span class=p>\*</span> <span class=p>..\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span><span class=p>\</span> <span class=n>-Recurse</span>
</span></span></code></pre></div><p>è¿™æ ·ä¼šæŠŠå†…å®¹å¤åˆ¶è¿›å»ï¼Œè€Œä¸æ˜¯æ•´ä¸ªç›®å½•ã€‚</p><hr><h2 id=æ€»ç»“-8>æ€»ç»“</h2><ul><li><strong>æŠ¥é”™åŸå› </strong>ï¼šç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼ŒPowerShell ä¸å…è®¸ç›´æ¥è¦†ç›–ã€‚</li><li><strong>è§£å†³åŠæ³•</strong>ï¼šæ¢æ–°åå­—ã€å…ˆåˆ é™¤å†å¤åˆ¶ï¼Œæˆ–åªå¤åˆ¶å†…å®¹ã€‚</li></ul><p>å¦‚éœ€æ›´å…·ä½“çš„å‘½ä»¤æˆ–æœ‰å…¶ä»–éœ€æ±‚ï¼Œæ¬¢è¿è¡¥å……è¯´æ˜ï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>cp : å…·æœ‰æŒ‡å®šåç§° C:\Users\tannal\tannalwork\projects\slides-typst-meetup-24-lilaq çš„é¡¹å·²å­˜åœ¨ã€‚
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=nb>cp </span><span class=p>.\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span> <span class=p>..\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq-copy</span> <span class=n>-Recurse</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=nb>Remove-Item</span> <span class=p>..\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span> <span class=n>-Recurse</span> <span class=n>-Force</span>
</span></span><span class=line><span class=cl><span class=nb>cp </span><span class=p>.\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span> <span class=p>..\</span> <span class=n>-Recurse</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=nb>cp </span><span class=p>.\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span><span class=p>\*</span> <span class=p>..\</span><span class=nb>slides-typst</span><span class=n>-meetup</span><span class=p>-</span><span class=mf>24</span><span class=n>-lilaq</span><span class=p>\</span> <span class=n>-Recurse</span>
</span></span></code></pre></div><hr><p><strong>User</strong></p><p>PS C:\Users\tannal\tannalwork\projects\slides-typst-meetup-24-lilaq> git submodule update &ndash;init
fatal: Unable to find current revision in submodule path &rsquo;lilaq&rsquo;
PS C:\Users\tannal\tannalwork\projects\slides-typst-meetup-24-lilaq></p><p>çœ‹çœ‹è¿™ä¸ªï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>ä½ é‡åˆ°çš„è¿™ä¸ªé”™è¯¯ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>fatal: Unable to find current revision in submodule path &#39;lilaq&#39;
</span></span></code></pre></div><p><strong>æ„æ€æ˜¯ï¼š</strong><br>Git åœ¨å°è¯•åˆå§‹åŒ–æˆ–æ›´æ–°å­æ¨¡å— <code>lilaq</code> æ—¶ï¼Œæ‰¾ä¸åˆ°å®ƒåº”è¯¥æ£€å‡ºçš„é‚£ä¸€ä¸ª commitï¼ˆrevisionï¼‰ã€‚è¿™é€šå¸¸æœ‰ä»¥ä¸‹å‡ ç§å¸¸è§åŸå› ï¼š</p><hr><h2 id=å¸¸è§åŸå› ä¸è§£å†³åŠæ³•>å¸¸è§åŸå› ä¸è§£å†³åŠæ³•</h2><h3 id=1-ä¸»ä»“åº“çš„>1. <strong>ä¸»ä»“åº“çš„ <code>.gitmodules</code> æˆ– <code>lilaq</code> ç›®å½•æŒ‡å‘äº†ä¸€ä¸ªä¸å­˜åœ¨çš„ commit</strong></h3><ul><li>å¯èƒ½æ˜¯ä¸»ä»“åº“çš„ submodule æŒ‡å‘äº† <code>lilaq</code> ä»“åº“ä¸­ä¸€ä¸ªå·²ç»è¢«åˆ é™¤æˆ–è¿˜æ²¡ push ä¸Šå»çš„ commitã€‚</li><li><strong>è§£å†³åŠæ³•ï¼š</strong><ul><li>è¿›å…¥ <code>lilaq</code> ç›®å½•ï¼Œæ‰§è¡Œ <code>git fetch</code>ï¼Œç¡®ä¿æ‰€æœ‰è¿œç¨‹åˆ†æ”¯å’Œ commit éƒ½æ‹‰ä¸‹æ¥äº†ã€‚</li><li>å›åˆ°ä¸»ä»“åº“ç›®å½•ï¼Œé‡æ–°æ‰§è¡Œï¼š<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>git submodule update --init --recursive
</span></span></code></pre></div></li></ul></li></ul><h3 id=2-ä½ æœ¬åœ°çš„-submodule-ç›®å½•ä¸å¹²å‡€æˆ–æŸå>2. <strong>ä½ æœ¬åœ°çš„ submodule ç›®å½•ä¸å¹²å‡€æˆ–æŸå</strong></h3><ul><li>å¯èƒ½ä¹‹å‰çš„æ“ä½œä¸­ï¼Œ<code>lilaq</code> ç›®å½•å†…å®¹ä¸å®Œæ•´æˆ–æœ‰å†²çªã€‚</li><li><strong>è§£å†³åŠæ³•ï¼š</strong><ul><li>å…ˆå¤‡ä»½ä½ éœ€è¦çš„å†…å®¹ï¼Œç„¶ååˆ é™¤å­æ¨¡å—ç›®å½•ï¼š<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>rm -rf lilaq
</span></span></code></pre></div></li><li>å†é‡æ–°åˆå§‹åŒ– submoduleï¼š<div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>git submodule update --init --recursive
</span></span></code></pre></div></li></ul></li></ul><h3 id=3-è¿œç¨‹-submodule-ä»“åº“æœ‰é—®é¢˜>3. <strong>è¿œç¨‹ submodule ä»“åº“æœ‰é—®é¢˜</strong></h3><ul><li>æ¯”å¦‚ submodule çš„è¿œç¨‹ä»“åº“è¢«åˆ é™¤ã€æƒé™é—®é¢˜ã€ç½‘ç»œé—®é¢˜ç­‰ã€‚</li><li><strong>è§£å†³åŠæ³•ï¼š</strong><ul><li>æ£€æŸ¥ <code>.gitmodules</code> æ–‡ä»¶ä¸­çš„ URL æ˜¯å¦æ­£ç¡®ï¼Œå¯ä»¥æ‰‹åŠ¨è®¿é—®ä¸€ä¸‹çœ‹çœ‹èƒ½ä¸èƒ½ cloneã€‚</li><li>å¦‚æœæœ‰æƒé™é—®é¢˜ï¼Œæ£€æŸ¥ä½ çš„ SSH key æˆ– HTTPS è®¤è¯ã€‚</li></ul></li></ul><h3 id=4-ä¸»ä»“åº“çš„-submodule-æŒ‡å‘äº†ä¸€ä¸ªæœª-push-çš„-commit>4. <strong>ä¸»ä»“åº“çš„ submodule æŒ‡å‘äº†ä¸€ä¸ªæœª push çš„ commit</strong></h3><ul><li>æ¯”å¦‚ä¸»ä»“åº“çš„ç»´æŠ¤è€…åœ¨æœ¬åœ°æ›´æ–°äº† submoduleï¼Œä½†æ²¡æœ‰æŠŠ submodule çš„æ–° commit push åˆ°è¿œç¨‹ã€‚</li><li><strong>è§£å†³åŠæ³•ï¼š</strong><ul><li>è”ç³»ä¸»ä»“åº“çš„ç»´æŠ¤è€…ï¼Œè®©ä»–ä»¬ push å­æ¨¡å—ä»“åº“çš„æœ€æ–° commitã€‚</li></ul></li></ul><hr><h2 id=æ£€æŸ¥å‘½ä»¤>æ£€æŸ¥å‘½ä»¤</h2><p>ä½ å¯ä»¥ç”¨ä»¥ä¸‹å‘½ä»¤æ’æŸ¥ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl><span class=c1># æŸ¥çœ‹ .gitmodules æ–‡ä»¶</span>
</span></span><span class=line><span class=cl>cat .gitmodules
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># æŸ¥çœ‹ submodule çš„ commit id</span>
</span></span><span class=line><span class=cl>cat .git/modules/lilaq/HEAD
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># è¿›å…¥å­æ¨¡å—ç›®å½•ï¼Œæ£€æŸ¥ log</span>
</span></span><span class=line><span class=cl><span class=nb>cd</span> lilaq
</span></span><span class=line><span class=cl>git log --oneline
</span></span></code></pre></div><hr><h2 id=æ€»ç»“-9>æ€»ç»“</h2><ul><li>è¿™ä¸ªé”™è¯¯<strong>å¤§æ¦‚ç‡æ˜¯ä¸»ä»“åº“æŒ‡å‘äº† submodule ä»“åº“ä¸­ä¸€ä¸ªæœ¬åœ°æœ‰ä½†è¿œç¨‹æ²¡æœ‰çš„ commit</strong>ã€‚</li><li>æœ€å¸¸è§çš„è§£å†³åŠæ³•æ˜¯ï¼š<strong>è®© submodule ä»“åº“çš„ç»´æŠ¤è€… push æœ€æ–° commit åˆ°è¿œç¨‹</strong>ï¼Œæˆ–è€…ä½ è‡ªå·± <code>git fetch</code> æ‹‰å…¨æ‰€æœ‰ commitã€‚</li><li>å¦‚æœä½ åªæ˜¯æƒ³è·³è¿‡ submoduleï¼Œå¯ä»¥ä¸´æ—¶åˆ é™¤ <code>.gitmodules</code> ç›¸å…³é…ç½®ï¼Œä½†è¿™ä¸æ˜¯æ ¹æœ¬è§£å†³åŠæ³•ã€‚</li></ul><hr><p>å¦‚éœ€è¿›ä¸€æ­¥æ’æŸ¥ï¼Œå¯ä»¥æŠŠ <code>.gitmodules</code> æ–‡ä»¶å†…å®¹å’Œ <code>git status</code>ã€<code>git log</code> çš„è¾“å‡ºè´´å‡ºæ¥ï¼Œæˆ‘å¯ä»¥å¸®ä½ æ›´å…·ä½“åˆ†æï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>fatal: Unable to find current revision in submodule path &#39;lilaq&#39;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>    git submodule update --init --recursive
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>    rm -rf lilaq
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>    git submodule update --init --recursive
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl># æŸ¥çœ‹ .gitmodules æ–‡ä»¶
</span></span><span class=line><span class=cl>cat .gitmodules
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># æŸ¥çœ‹ submodule çš„ commit id
</span></span><span class=line><span class=cl>cat .git/modules/lilaq/HEAD
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl># è¿›å…¥å­æ¨¡å—ç›®å½•ï¼Œæ£€æŸ¥ log
</span></span><span class=line><span class=cl>cd lilaq
</span></span><span class=line><span class=cl>git log --oneline
</span></span></code></pre></div><hr><p><strong>User</strong></p><p>go get åŠ ä¸åŠ -u æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>ä½ çš„é—®é¢˜æ˜¯ï¼š<strong><code>go get</code> åŠ  <code>-u</code> å’Œä¸åŠ  <code>-u</code> æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</strong></p><hr><h2 id=1-ä¸åŠ --u-çš„æƒ…å†µ>1. ä¸åŠ  <code>-u</code> çš„æƒ…å†µ</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>go get example.com/some/module
</span></span></code></pre></div><ul><li><strong>ä½œç”¨</strong>ï¼šå®‰è£…ï¼ˆæˆ–å‡çº§ï¼‰æŒ‡å®šçš„ module åˆ°å½“å‰é¡¹ç›®çš„ <code>go.mod</code>ï¼Œ<strong>åªä¼šè·å–ä½ æŒ‡å®šçš„åŒ…å’Œå®ƒå½“å‰ä¾èµ–çš„ç‰ˆæœ¬</strong>ã€‚</li><li><strong>ä¾èµ–å‡çº§</strong>ï¼š<strong>ä¸ä¼š</strong>è‡ªåŠ¨å‡çº§å·²æœ‰ä¾èµ–çš„ç‰ˆæœ¬ï¼ˆåªä¼šä¸‹è½½ç¼ºå¤±çš„ä¾èµ–ï¼‰ã€‚</li></ul><hr><h2 id=2-åŠ --u-çš„æƒ…å†µ>2. åŠ  <code>-u</code> çš„æƒ…å†µ</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>go get -u example.com/some/module
</span></span></code></pre></div><ul><li><strong>ä½œç”¨</strong>ï¼šå®‰è£…ï¼ˆæˆ–å‡çº§ï¼‰æŒ‡å®šçš„ moduleï¼Œå¹¶ä¸”<strong>é€’å½’å‡çº§è¯¥åŒ…åŠå…¶æ‰€æœ‰ä¾èµ–åˆ°æœ€æ–°çš„æ¬¡è¦ç‰ˆæœ¬æˆ–è¡¥ä¸ç‰ˆæœ¬</strong>ï¼ˆä¸è·¨å¤§ç‰ˆæœ¬ï¼‰ã€‚</li><li><strong>ä¾èµ–å‡çº§</strong>ï¼š<strong>ä¼š</strong>è‡ªåŠ¨å‡çº§ä¾èµ–æ ‘ä¸­æ‰€æœ‰å¯ä»¥å‡çº§çš„åŒ…ï¼ˆä¸æ”¹å˜ä¸»ç‰ˆæœ¬å·ï¼‰ã€‚</li></ul><hr><h2 id=3-åŠ --upatch-çš„æƒ…å†µgo-117>3. åŠ  <code>-u=patch</code> çš„æƒ…å†µï¼ˆGo 1.17+ï¼‰</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-sh data-lang=sh><span class=line><span class=cl>go get -u<span class=o>=</span>patch example.com/some/module
</span></span></code></pre></div><ul><li><strong>ä½œç”¨</strong>ï¼šåªå‡çº§åˆ°æœ€æ–°çš„è¡¥ä¸ç‰ˆæœ¬ï¼ˆä¸å‡çº§æ¬¡è¦ç‰ˆæœ¬ï¼‰ã€‚</li></ul><hr><h2 id=4-æ€»ç»“å¯¹æ¯”>4. æ€»ç»“å¯¹æ¯”</h2><div class=table-wrapper><table><thead><tr><th>å‘½ä»¤</th><th>ä¾èµ–å‡çº§è¡Œä¸º</th></tr></thead><tbody><tr><td>go get xxx</td><td>åªè·å–æŒ‡å®šåŒ…åŠå…¶ä¾èµ–ï¼Œä¸å‡çº§å·²æœ‰ä¾èµ–</td></tr><tr><td>go get -u xxx</td><td>å‡çº§æŒ‡å®šåŒ…åŠå…¶æ‰€æœ‰ä¾èµ–åˆ°æœ€æ–°æ¬¡/è¡¥ä¸ç‰ˆ</td></tr><tr><td>go get -u=patch xxx</td><td>åªå‡çº§åˆ°æœ€æ–°è¡¥ä¸ç‰ˆæœ¬</td></tr></tbody></table></div><hr><h2 id=5-æ³¨æ„äº‹é¡¹-1>5. æ³¨æ„äº‹é¡¹</h2><ul><li><strong>Go 1.17+</strong> æ¨èç”¨ <code>go install</code> å®‰è£…å¯æ‰§è¡Œç¨‹åºï¼Œç”¨ <code>go get</code> ç®¡ç†ä¾èµ–ã€‚</li><li><strong>Go 1.18+</strong>ï¼Œ<code>go get</code> ä¸å†ç”¨äºå®‰è£…å¯æ‰§è¡Œæ–‡ä»¶ï¼Œåªç”¨äºä¾èµ–ç®¡ç†ã€‚</li></ul><hr><h2 id=6-ä¾‹å­>6. ä¾‹å­</h2><p>å‡è®¾ä½ çš„ <code>go.mod</code> é‡Œå·²ç»æœ‰ <code>github.com/gin-gonic/gin v1.7.0</code>ï¼Œä½ æ‰§è¡Œï¼š</p><ul><li><code>go get github.com/gin-gonic/gin</code><br>åªä¼šç¡®ä¿ <code>v1.7.0</code> å­˜åœ¨ï¼Œä¸ä¼šå‡çº§ã€‚</li><li><code>go get -u github.com/gin-gonic/gin</code><br>ä¼šå‡çº§åˆ° <code>v1.x</code> çš„æœ€æ–°ç‰ˆæœ¬ï¼ˆæ¯”å¦‚ <code>v1.9.1</code>ï¼‰ï¼Œå¹¶ä¸”é€’å½’å‡çº§ä¾èµ–ã€‚</li></ul><hr><p>å¦‚éœ€æ›´è¯¦ç»†çš„ç‰ˆæœ¬ç®¡ç†è¯´æ˜æˆ–é‡åˆ°å…·ä½“ä¾èµ–å†²çªï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>go get example.com/some/module
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>go get -u example.com/some/module
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>go get -u=patch example.com/some/module
</span></span></code></pre></div><hr><p><strong>User</strong></p><p>authorization
Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6IjI5MzQ0ODIxMzNAcXEuY29tIiwiZXhwIjoxNzUwNDg5NTcyLCJpZCI6MCwibmFtZSI6IiJ9.3P5LDZX7amz74G6sVGaMhd7OpEEC6aufRCUDSG2Av1w</p><p>è¿™ä¸ªbeareræ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ</p><hr><p><strong>Cursor</strong></p><p>ä½ é—®çš„ <code>Bearer</code> æ˜¯ä»€ä¹ˆï¼Œä¸‹é¢è¯¦ç»†è§£é‡Šï¼š</p><hr><h2 id=1-bearer-çš„å«ä¹‰>1. <code>Bearer</code> çš„å«ä¹‰</h2><ul><li><code>Bearer</code> æ˜¯ä¸€ç§<strong>HTTP è®¤è¯ç±»å‹</strong>ï¼Œå¸¸ç”¨äº API çš„èº«ä»½éªŒè¯ã€‚</li><li>å®ƒè¡¨ç¤º<strong>æŒæœ‰è€…ä»¤ç‰Œ</strong>ï¼ˆBearer Tokenï¼‰ï¼Œå³åªè¦ä½ â€œæŒæœ‰â€è¿™ä¸ª tokenï¼Œå°±è¢«è®¤ä¸ºæœ‰æƒé™è®¿é—®èµ„æºã€‚</li></ul><hr><h2 id=2-å…¸å‹ç”¨æ³•>2. å…¸å‹ç”¨æ³•</h2><p>åœ¨ HTTP è¯·æ±‚å¤´ä¸­ï¼Œé€šå¸¸è¿™æ ·å†™ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Authorization: Bearer &lt;token&gt;
</span></span></code></pre></div><p>æ¯”å¦‚ä½ ç»™çš„ä¾‹å­ï¼š</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6IjI5MzQ0ODIxMzNAcXEuY29tIiwiZXhwIjoxNzUwNDg5NTcyLCJpZCI6MCwibmFtZSI6IiJ9.3P5LDZX7amz74G6sVGaMhd7OpEEC6aufRCUDSG2Av1w
</span></span></code></pre></div><hr><h2 id=3-è¿™ä¸ª-token-æ˜¯ä»€ä¹ˆ>3. è¿™ä¸ª token æ˜¯ä»€ä¹ˆï¼Ÿ</h2><ul><li>ä½ è¿™ä¸ª token æ˜¯ä¸€ä¸ª<strong>JWTï¼ˆJSON Web Tokenï¼‰</strong>ï¼Œæ˜¯ä¸€ç§å¸¸è§çš„ Bearer Token æ ¼å¼ã€‚</li><li>JWT é‡Œé€šå¸¸åŒ…å«ç”¨æˆ·ä¿¡æ¯ã€è¿‡æœŸæ—¶é—´ç­‰ï¼ŒæœåŠ¡ç«¯å¯ä»¥ç”¨å®ƒæ¥æ ¡éªŒç”¨æˆ·èº«ä»½ã€‚</li></ul><hr><h2 id=4-bearer-token-çš„ç‰¹ç‚¹>4. Bearer Token çš„ç‰¹ç‚¹</h2><ul><li><strong>æ— çŠ¶æ€</strong>ï¼šæœåŠ¡ç«¯åªéœ€æ ¡éªŒ tokenï¼Œæ— éœ€ä¿å­˜ä¼šè¯ã€‚</li><li><strong>å®‰å…¨æ€§</strong>ï¼šåªè¦æ‹¿åˆ° token çš„äººéƒ½èƒ½è®¿é—®èµ„æºï¼Œæ‰€ä»¥è¦é€šè¿‡ HTTPS ä¼ è¾“ï¼Œé˜²æ­¢è¢«çªƒå–ã€‚</li><li><strong>å¸¸è§äº</strong>ï¼šOAuth 2.0ã€RESTful APIã€å‰åç«¯åˆ†ç¦»é¡¹ç›®ç­‰ã€‚</li></ul><hr><h2 id=5-æ€»ç»“>5. æ€»ç»“</h2><ul><li><code>Bearer</code> è¡¨ç¤ºâ€œæŒæœ‰è€…ä»¤ç‰Œâ€è®¤è¯æ–¹å¼ã€‚</li><li>åé¢è·Ÿçš„å­—ç¬¦ä¸²æ˜¯ tokenï¼ˆä½ è¿™é‡Œæ˜¯ JWTï¼‰ï¼Œç”¨æ¥è¯æ˜ä½ çš„èº«ä»½ã€‚</li><li>åªè¦æœ‰è¿™ä¸ª tokenï¼Œå°±èƒ½è®¿é—®éœ€è¦è®¤è¯çš„æ¥å£ã€‚</li></ul><hr><p>å¦‚éœ€äº†è§£ JWT çš„ç»“æ„ã€å¦‚ä½•ç”Ÿæˆ/æ ¡éªŒï¼Œæˆ–å®‰å…¨æ³¨æ„äº‹é¡¹ï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>Authorization: Bearer &lt;token&gt;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6IjI5MzQ0ODIxMzNAcXEuY29tIiwiZXhwIjoxNzUwNDg5NTcyLCJpZCI6MCwibmFtZSI6IiJ9.3P5LDZX7amz74G6sVGaMhd7OpEEC6aufRCUDSG2Av1w
</span></span></code></pre></div><hr><p><strong>User</strong></p><p>Enabling Language Models to Fill in the Blanks
Chris Donahue
Stanford University
Mina Lee
Stanford University
{cdonahue,minalee,pliang}@cs.stanford.edu
Percy Liang
Stanford University
Abstract
We present a simple approach for text infilling, the task of predicting missing spans of text
at any position in a document. While infilling could enable rich functionality especially
for writing assistance tools, more attention has
been devoted to language modelingâ€”a special
case of infilling where text is predicted at the
end of a document. In this paper, we aim to extend the capabilities of language models (LMs)
to the more general task of infilling. To this
end, we train (or fine-tune) off-the-shelf LMs
on sequences containing the concatenation of
artificially-masked text and the text which was
masked. We show that this approach, which
we call infilling by language modeling, can enable LMs to infill entire sentences effectively
on three different domains: short stories, scientific abstracts, and lyrics. Furthermore, we
show that humans have difficulty identifying
sentences infilled by our approach as machinegenerated in the domain of short stories.
1 Introduction
Text infilling is the task of predicting missing spans
of text which are consistent with the preceding and
subsequent text.1 Systems capable of infilling have
the potential to enable rich applications such as
assisting humans in editing or revising text (Shih
et al., 2019), connecting fragmented ideas (AI21,
2019), and restoring ancient documents (Assael
et al., 2019). Rather than targeting a particular
application, our goal here is to provide a general,
flexible, and simple infilling framework which can
convincingly infill in a variety of domains.
A special case of infilling is language modeling:
predicting text given preceding but not subsequent
text.2 Language models are (1) capable of generat1Text infilling is a generalization of the cloze task (Taylor,
1953)â€”cloze historically refers to infilling individual words.
2
In this paper, language modeling always refers to ordinary
LMs, i.e., â€œunidirectional,â€ â€œautoregressive,â€ or â€œleft-to-right.â€
She ate leftover pasta for lunch.
She ate [blank] for [blank].
leftover pasta [answer] lunch [answer]
Data
Input
Target
Our Infilling Framework
She ate [blank] for [blank].
She ate leftover pasta for lunch.
Infilling Task
Input
Output
Train
Language
Model
Infilling
Input
[sep]
Target
Data
Input
[sep]
Target
Output
Figure 1: We consider the task of infilling, which takes
incomplete text as input and outputs completed text. To
tackle this task, our framework constructs training examples by masking random spans to generate pairs of
inputs (text with blanks) and targets (answers for each
blank). We then train unidirectional language models on the concatenation of each pair. Once trained,
a model takes text input with blanks, predicts the answers, and then combines them to produce the output.
ing remarkably coherent text (Zellers et al., 2019;
See et al., 2019), (2) efficient at generating text,
and (3) conceptually simple, but cannot infill effectively as they can only leverage context in a
single direction (usually the past). On the other
hand, strategies such as BERT (Devlin et al., 2019)
and SpanBERT (Joshi et al., 2019) are able to infill
using both preceding and subsequent text. However, their use of bidirectional attention limits their
infilling capabilities to fixed-length spans. This is
problematic asâ€”for many applicationsâ€”we may
not know the length of a missing span a priori.
Zhu et al. (2019) propose a method capable of infilling variable-length spans, but it uses a specialized architecture and hence cannot easily leverage
large-scale pre-trained models.
In this work, we present infilling by language
modeling (ILM), a simple framework which enarXiv:2005.05339v2 [cs.CL] 10 Sep 2020
ables LMs to infill variable-length spans while preserving their aforementioned benefits: generation
quality, efficient sampling, and conceptual simplicity. Our framework involves a straightforward formulation of the infilling task which, as we demonstrate, can be learned effectively by existing LM
architectures. As shown in Figure 1, our approach
concatenates artificially-masked text with the text
which was masked, and adopts a standard LM training (or fine-tuning) procedure on such examples.
Once trained, infilling can be performed for a document with blanks by using the LM to generate text
and then replacing the blanks with this text.
In addition to its conceptual simplicity, our experiments show that ILM enables off-the-shelf LMs
to infill effectively. Furthermore, we find that infilling performance improves when starting from a
large-scale pre-trained LM (as opposed to training
from scratch), suggesting an additional benefit of
using our model-agnostic framework compared to
approaches which require specialized architectures.
We provide an interactive web demo of models
trained under our framework. This demo can infill
multiple variable-length spans with different granularities (e.g. words, n-grams, and sentences) on the
domains of short stories, scientific abstracts, and
song lyrics: <a class=link href=https://chrisdonahue.com/ilm target=_blank rel=noopener>https://chrisdonahue.com/ilm</a>.
All code, data, and trained models are available
at <a class=link href=https://github.com/chrisdonahue/ilm target=_blank rel=noopener>https://github.com/chrisdonahue/ilm</a>
and also on the CodaLab platform at https:
//worksheets.codalab.org/worksheets/
0x9987b5d9cce74cf4b2a5f84b54ee447b.
2 Problem Statement
The task of infilling is to take incomplete text xËœ,
containing one or more missing spans, and return
completed text x. Let [blank] be a placeholder for a
contiguous sequence (span) of one or more missing
tokens. Then, incomplete text xËœ is a sequence of
tokens some of which are [blank]. In order to map
xËœ to x, an infilling strategy must specify both how
many and which tokens to generate for each [blank].
Note that there may be many reasonable x for a
given xËœ. Hence, we are interested in learning a
distribution p(x | xËœ).
3 Infilling by Language Modeling
In this section, we describe our ILM framework.
We first outline a simple reparametrization of the
infilling task. Then, we define a procedure for automatically generating suitable training examples
which can be fed to an off-the-shelf LM.
3.1 Formulation
Fedus et al. (2018) explore an infilling framework
where LMs are trained on concatenations of xËœ and
x, i.e., they use LMs to directly predict x given xËœ.
While their approach is effective at infilling individual words, it is somewhat redundant as the model
must â€œpredictâ€ the unmasked text in xËœ. Additionally, a model is not guaranteed to exactly reproduce
the unmasked text.
Instead, we make the trivial observation that it
suffices to predict only the missing spans y which
will replace the [blank] tokens in xËœ. We can then
construct x by simply replacing [blank] tokens in
xËœ with predicted spans y in a deterministic fashion.
In order to handle multiple variable-length spans,
we pose y as the concatenation of all missing spans
separated by special [answer] tokens (one [answer]
per [blank]) (Figure 1). We can thus cast infilling
as learning p(y | xËœ) without loss of generality.
3.2 Training
Given a corpus consisting of complete text examples, our framework first manufactures infilling
examples and then trains an LM on these examples. To produce an infilling example for a given
x, we first sample an xËœ from a stochastic function
Mask(x) which randomly replaces some number
of spans in x with [blank] tokens. Then, we concatenate together the spans which were replacedâ€”
separated by [answer] tokensâ€”to form a training
target y. Finally, we construct the complete infilling example by concatenating xËœ, [sep], and y (see
Figure 2 for a complete example).
We train (or fine-tune) LMs on these infilling
examples using standard LM training methodology,
yielding models of the form pÎ¸(y | xËœ). Specifically,
we train GPT-2 (Radford et al., 2019) off the shelf,
but any LM can potentially be used.
This framework has several advantages. First,
it incurs almost no computational overhead compared to language modeling. Specifically, if there
are k missing spans in xËœ, the concatenation of xËœ
and y contains only 2k+ 1 more tokens than x (one
[blank] and one [answer] per missing span plus one
[sep]). As k is usually small (averaging around 2
per example in our experiments), sequence lengths
remain similar to those encountered for the same
x during language modeling. In contrast, using
LMs to directly predict x from xËœ as in Fedus et al.
(2018) effectively doubles the sequence length of x.
This is particularly problematic when considering
models like GPT-2 whose memory usage grows
quadratically with sequence length. Second, our
framework requires minimal change (three additional tokens) to an existing LMâ€™s vocabulary. Finally, because the entirety of xËœ is in the â€œpastâ€ when
predicting y, the ILM framework combines the ability to attend to incorporate context on both sides of
a blank with the simplicity of decoding from LMs.
4 Experimental Setup
We design our experiments to determine if training an off-the-shelf LM architecture with our
ILM framework can produce effective infilling
models for a variety of datasets. Specifically,
we train on three datasets of different sizes and
semantics (details in Appendix A): short STORIES (Mostafazadeh et al., 2016), CS paper ABSTRACTS, and song LYRICS.
4.1 Mask Function
A benefit of the ILM framework is that it can
be trained to infill spans corrupted by arbitrary
mask functions. Here, we explore a mask function which simultaneously trains models to infill
different granularities of text; specifically, words,
n-grams, sentences, paragraphs, and documents.
By using a unique special token per granularity
(e.g. [blank word]), this mask function offers users
coarse but intuitive control over the length of the
spans to be infilled.
We configure our mask function to mask each
token in a given document with around 15% probability, echoing the configuration of Devlin et al.
(2019). However, instead of masking individual
tokens uniformly at random, we perform a preorder traversal of the granularity hierarchy tree,
randomly masking entire subtrees with 3% probability. For the datasets we consider, this results in a
marginal token mask rate of about 15% (details in
Appendix B).
While we train to infill several different granularities, we primarily evaluate and discuss the ability
of our models to infill sentences for brevity. Quantitative results of our models on other granularities
can be found in Appendix D, and granularity functionality can also be explored in our web demo.
4.2 Task and Model Configurations
For all experiments, we train the same architecture
(GPT-2 â€œsmallâ€) using the same hyperparameters
She ate leftover pasta for lunch.
She ate [blank] for [blank].
She ate leftover pasta for lunch. [end]
.lunch for leftover pasta ate She [end]
She ate [blank] for [blank]. She ate
leftover pasta for lunch. [end]
She ate [blank] for [blank]. [sep]
leftover pasta [answer] lunch [answer]
Data
Masked
LM
LM-Rev
LM-All
ILM
Training Examples for Different Strategies
Figure 2: Training examples for three baseline infilling
strategies and ILM on a given artificially-masked sentence. For each strategy, we train the same architecture
(GPT-2) on such examples. At both training and test
time, examples are fed from left to right; anything to
the left of a green target is available to the model as
context when predicting the target. Precisely, LM only
considers past context, and LM-Rev only considers future. LM-All considers all available context but uses
long sequence lengths. Our proposed ILM considers
all context while using fewer tokens.
(Appendix C) while varying the infilling strategy
and dataset. In addition to our proposed ILM strategy for infilling, we consider three baseline strategies: (1) language modeling (LM; â€œinfillingâ€ based
only on past context), (2) reverse language modeling (LM-Rev; â€œinfillingâ€ based only on future
context), and (3) language modeling based on all
available context (LM-All). LM-All simply concatenates x and xËœ together as in Fedus et al. (2018).
LM-All represents arguably the simplest way one
could conceive of infilling with LMs, but results in
long sequence lengths. Training examples for all
strategies are depicted in Figure 2.
For each strategy, we also vary whether training
is initialized from the pre-trained GPT-2 model or
from scratch. Despite discrepancies between the
pre-training and our fine-tuning for most infilling
strategies, all of the infilling experiments initialized
from the pre-trained checkpoint performed better
than their from-scratch counterparts. This indicates
that ILM can effectively leverage large-scale language modeling pre-training to improve infilling
performance. Henceforth, we will only discuss the
models initialized from the pre-trained checkpoint,
though we report quantitative performance for all
models in Appendix D.
For the models trained on STORIES and ABSTRACTS, we trained models to convergence using
early stopping based on the validation set perplexity
(PPL) of each model computed only on the masked
tokens. These models took about a day to reach
STO ABS LYR Length
LM 18.3 27.9 27.7 1.00
LM-Rev 27.1 46.5 34.3 1.00
LM-All 15.6 22.3 21.4 1.81
ILM 15.6 22.4 22.6 1.01
Table 1: Quantitative evaluation results. We report test
set perplexity (PPL) on the sentence infilling task for
different model configurations on all three datasets, as
well as average length of all test set examples in tokens relative to that of the original sequence (lower is
better for all columns). Our proposed ILM framework
achieves better PPL than both LM and LM-Rev, implying that it is able to take advantage of both past and
future context. ILM achieves similar PPL to LM-All
with shorter sequence lengths (hence less memory).
their early stopping criteria on a single GPU. For
the larger LYRICS dataset, we trained models for 2
epochs (about two days on a single GPU).
5 Quantitative Evaluation
We evaluate the quantitative performance of our
models on the sentence infilling task by measuring
PPL on test data.3
In this setting, a sentence is selected at random and masked out, and we measure
the likelihood assigned by a model to the masked
sentence in the context of the rest of the document.
Regardless of differences in the ordering and number of tokens that each strategy uses to represent
a test example, PPL is always computed only for
the span of tokens comprising the original sentence
(e.g. green tokens in Figure 2).
Table 1 shows that across all datasets, ILM outperforms models which see only past or future context (LM and LM-Rev respectively), implying that
our proposed framework is able to take advantage
of bidirectional context despite using unidirectional
models. Additionally, while one might expect LMAll to outperform ILM because its training examples more closely â€œresembleâ€ those of standard
LMs, ILM achieves similar performance to LMAll. This indicates that GPT-2 is able to effectively
learn the â€œsyntaxâ€ of ILM examples and achieve
reasonable infilling performance with shorter sequences (and hence with much less memory usage).
We also observe that models trained via ILM perform similarly on the special case of language mod3Overlap-based metrics such as BLEU score (Papineni
et al., 2002) are not appropriate for evaluating infilling as
there are many realistic infills that have no word-level overlap
with the original, e.g., â€œa sandwichâ€ instead of â€œleftover pasta.â€
eling compared to the models which were trained
only on language modeling (Appendix D.1). This
suggests that ILM does not just repurpose LMs
to infill, but rather extends their capabilities while
maintaining their original functionality.
6 Human Evaluation
In addition to our quantitative evaluation, we seek
to evaluate the qualitative performance of ILM. To
this end, we sample a story from the STORIES test
set and randomly replace one of its five humanwritten sentences with a model output. Then,
we task human annotators on Amazon Mechanical Turk with identifying which of the sentences
in a story was machine-generated (details in Appendix E).
We compare our ILM model to three baseline
infilling strategies: an LM (context beyond the replaced sentence was discarded), the best model
(self-attention; SA) from Zhu et al. (2019), and
the pre-trained BERT (base) model (Devlin et al.,
2019). All approaches except for BERT were first
fine-tuned on the STORIES dataset. To infill using
BERT, we replace the tokens representing the original sentence with mask tokens, and then generate
text by replacing mask tokens one at a time (conditioning on previously-generated tokens). While
vocabulary differences make it is less useful to compare PPL for the SA and BERT baselines to our
GPT-2-based strategies, we can still meaningfully
compare them in this human evaluation setting.
For each approach we compute a score, which
we define as the percentage of examples where the
annotator did not correctly identify the machinegenerated sentence. Therefore, a higher score implies a better (more natural, human-like) model.
We collect 100 responses for each model and report the scores in Table 2, with qualitative examples
in Figure 3 and Appendix E.
Of the four strategies, ILM achieves the highest
score, implying that sentences infilled by ILM are
harder for humans to recognize as fake than those
produced by other strategies. Somewhat surprisingly, we observed that despite only observing past
context the LM model performed better than BERT
and SA. BERT may have performed poorly due to
the intrinsic difficulty of finding convincing infills
with a precise length in tokens. SA may have performed poorly because, unlike LM and ILM, it was
not initialized from a large-scaled pre-trained LM.
BERT SA LM ILM
Score (%) 20 29 41 45
Table 2: Human evaluation results. We use BERT (Devlin et al., 2019), the best model from Zhu et al. (2019)
(SA), and our LM and ILM models to replace random
sentences in five-sentence stories from the STORIES
test set. Then, we task humans with identifying which
sentence of the five was generated by a machine. We
report the score of each model: the percentage of infilled stories where the human failed to identify the
machine-generated sentence. Our ILM model achieves
a higher score than all of the other models. Note that
the max score is effectively 80%, as a perfect model
would cause annotators to randomly choose one of the
five sentences.
BERT
SA
LM
ILM
Human
favoritea &ldquo;, Mary brightly said.
She wasn&rsquo;t sure she had to go to the store.
She went to check the tv.
Patty knew her friends wanted pizza.
She also had the place looking spotless.
Example Story with Masked Sentence
Patty was excited about having her friends over.
She had been working hard preparing the food.
[blank]
All of her friends arrived
and were seated at the table.
Patty had a great time with her friends.
Figure 3: Example of a short story in our STORIES
dataset with its third sentence masked, and sentences infilled by different models. The sentences generated by
BERT and SA models are off-topic, the sentence generated by LM model is irrelevant to the future context,
while the ones generated by ILM and Human successfully account for both previous and future context.
7 Related Work
Methodology. A number of systems have the
capability to infill but have practical drawbacks.
Many systems are unable to automatically determine span length, and thus, can only infill fixedlength spans (Fedus et al., 2018; Devlin et al., 2019;
Yang et al., 2019; Joshi et al., 2019; Gu et al., 2019;
Liu et al., 2019). Methods such as BERT present
additional challenges during inference (Wang and
Cho, 2019). Rudinger et al. (2015) frame narrative
cloze as a generation task and employ language
models, but they only consider one infill of a fixed
length. Zhu et al. (2019); Shen et al. (2020) infill multiple variable-length sequences, but these
approaches require the masked context to be iteratively updated and reprocessed to fill in blanks one
a time. In contrast, our approach appends infilled
text to the context and does not require reprocessing the entire input sequence for each blank. AI21
(2019) train an LM which can fill in the middle of
a paragraph given the first and last sentencesâ€”our
work generalizes to such capabilities.
Task. The cloze task (Taylor, 1953) evaluates
language proficiency by asking systems to fill
in randomly-deleted words by examining context.
Cloze has been extended in the forms of discourse (Deyes, 1984) and narrative cloze (Chambers and Jurafsky, 2008), which remove phrases
and narrative events respectively. Recently, cloze
has been used not only for evaluation, but also to
improve text generation quality (Fedus et al., 2018)
and transfer learning (Devlin et al., 2019) (under
the name â€œmasked language modelingâ€). Text infilling can be thought of as generalizing the cloze task
from single words to spans of unknown length. Raffel et al. (2019) explore infilling as a pre-training
objective to improve downstream performance on
inference tasks; our work focuses on generation.
Story generation. Recent work seeks to generate stories given a title and storyline (Yao et al.,
2019), entities (Clark et al., 2018), premise (Fan
et al., 2018), or surrounding context and rare words
(Ippolito et al., 2019). Our work differs in that
we aim to build systems capable of making predictions based only on text context, rather than aspects
specific to stories (e.g. storyline).
8 Conclusion
We presented a simple strategy for the task of
infilling which leverages language models. Our
approach is capable of infilling sentences which
humans have difficulty recognizing as machinegenerated. Furthermore, we demonstrated that our
infilling framework is effective when starting from
large-scale pre-trained LMs, which may be useful
in limited data settings. In future work, we plan to
incorporate these features into co-creation systems
which assist humans in the writing process. We
hope that our work encourages more investigation
of infilling, which may be a key missing element
of current writing assistance tools.
Acknowledgments
This work was funded by DARPA CwC under ARO
prime contract no. W911NF-15-1-0462. We thank
all reviewers for their helpful comments.
References
AI21. 2019. HAIM: A modest step towards controllable text generation. AI21 Labs Blog.
Yannis Assael, Thea Sommerschield, and Jonathan
Prag. 2019. Restoring ancient text using deep
learning: a case study on greek epigraphy.
arXiv:1910.06262.
N. Chambers and D. Jurafsky. 2008. Unsupervised
learning of narrative event chains. In Human Language Technology and Association for Computational Linguistics (HLT/ACL).
Elizabeth Clark, Yangfeng Ji, and Noah A Smith. 2018.
Neural text generation in stories using entity representations as context. In Association for Computational Linguistics: Human Language Technologies.
J. Devlin, M. Chang, K. Lee, and K. Toutanova. 2019.
BERT: Pre-training of deep bidirectional transformers for language understanding. In Association
for Computational Linguistics (ACL), pages 4171â€“
4186.
T. Deyes. 1984. Towards an authentic â€˜discourse clozeâ€™.
Applied Linguistics, 5(2):128â€“137.
A. Fan, M. Lewis, and Y. Dauphin. 2018. Hierarchical neural story generation. arXiv preprint
arXiv:1805.04833.
W. Fedus, I. Goodfellow, and A. M. Dai. 2018.
Maskgan: Better text generation via filling in the.
In International Conference on Learning Representations (ICLR).
J. Gu, Q. Liu, and K. Cho. 2019. Insertion-based decoding with automatically inferred generation order.
arXiv preprint arXiv:1902.01370.
D. Ippolito, D. Grangier, C. Callison-Burch, and
D. Eck. 2019. Unsupervised hierarchical story infilling. In NAACL Workshop on Narrative Understanding, pages 37â€“43.
M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer,
and O. Levy. 2019. SpanBERT: Improving pretraining by representing and predicting spans. arXiv
preprint arXiv:1907.10529.
D. Liu, J. Fu, P. Liu, and J. Lv. 2019. TIGS: An inference algorithm for text infilling with gradient search.
arXiv preprint arXiv:1905.10752.
N. Mostafazadeh, N. Chambers, X. He, D. Parikh,
D. Batra, L. Vanderwende, P. Kohli, and J. Allen.
2016. A corpus and cloze evaluation for deeper
understanding of commonsense stories. In North
American Association for Computational Linguistics
(NAACL).
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and
I. Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8).
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,
M. Matena, Y. Zhou, W. Li, and P. J. Liu. 2019.
Exploring the limits of transfer learning with a
unified text-to-text transformer. arXiv preprint
arXiv:1910.10683.
R. Rudinger, P. Rastogi, F. Ferraro, and B. V. Durme.
2015. Script induction as language modeling. In
Empirical Methods in Natural Language Processing
(EMNLP).
Abigail See, Aneesh Pappu, Rohun Saxena, Akhila
Yerukola, and Christopher D Manning. 2019. Do
massively pretrained language models make better
storytellers? arXiv:1909.10705.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909.
Tianxiao Shen, Victor Quach, Regina Barzilay, and
Tommi Jaakkola. 2020. Blank language models.
arXiv:2002.03079.
Y. Shih, W. Chang, and Y. Yang. 2019. XL-Editor:
Post-editing sentences with xlnet. arXiv preprint
arXiv:1910.10479.
W. L. Taylor. 1953. â€œCloze procedureâ€: A new
tool for measuring readability. Journalism Bulletin,
30(4):415â€“433.
A. Wang and K. Cho. 2019. BERT has a mouth, and
it must speak: BERT as a Markov random field language model. arXiv preprint arXiv:1902.04094.
T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue,
A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, and J. Brew. 2019. HuggingFaceâ€™s transformers:
State-of-the-art natural language processing. arXiv
preprint arXiv:1910.03771.
Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding.
arXiv preprint arXiv:1906.08237.
L. Yao, N. Peng, R. Weischedel, K. Knight, D. Zhao,
and R. Yan. 2019. Plan-and-write: Towards better
automatic storytelling. In Association for the Advancement of Artificial Intelligence (AAAI).
Rowan Zellers, Ari Holtzman, Hannah Rashkin,
Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. 2019. Defending against neural fake
news. In NeurIPS.
W. Zhu, Z. Hu, and E. Xing. 2019. Text infilling. arXiv
preprint arXiv:1901.00158.
A Datasets</p><ul><li>STORIES (100K examples, 5M words)
Short stories from the ROCStories dataset
(Mostafazadeh et al., 2016). Each story contains
a title and five sentences.</li><li>ABSTRACTS (200K examples, 30M words)
Abstracts from CS papers on arXiv</li><li>LYRICS (2M examples, 60M words)
Song lyrics from lyrics.com
We experimented on multiple datasets to demonstrate that our framework was not custom tailored
to a single domain. On the STORIES and ABSTRACTS datasets, we include metadata (story title,
paper subject matter, etc.), as the first â€œparagraphâ€
of the document. By providing these paragraphs
(Appendix B), our infilling model implicitly learns
to summarize (e.g. infill a title given a story), and
do conditional generation (e.g. infill a story given a
title). On the LYRICS dataset, infilling models may
be especially helpful to humans; external aid in the
form of rhyming dictionaries is already commonly
employed in this domain.
To ensure that all experiments were trained on
the same data, we removed infilling examples
which would have exceeded our training sequence
length of 256 tokens for the model with the longest
sequence length (LM-All). This removed no examples from STORIES, a small fraction of examples
from LYRICS, and a substantial number of examples from ABSTRACTS.
B Masking function
We design a mask function which takes the entire
document and selectively masks several span granularities: words, n-grams, sentences, paragraphs,
and entire documents. Accordingly, models trained
via ILM on this masking function offer users the
ability to specify the granularity of text to infill
at a particular location. This allows users to have
coarse but intuitive control over infilling length, so
that multiple paragraphs are not generated when
the user was expecting a single word.
Our masking function first constructs a tree
of the training example (using the natural hierarchy of documents, paragraphs, sentences, and
words). Then, using a pre-order tree traversal, each subtree is masked with 3% probability (or ignored if any of its ancestors are already
masked). If the entire document (root node of
the tree) is masked, then the infilling modelâ€™s job
is equivalent to that of a language model. If a
word (leaf) is selected to be masked, 50% of the
time we mask that individual word, otherwise we
mask an n-gram of random length between 1 and
min(8, # words left in the sentence) words (inclusive). Note that a word may comprise multiple
tokens, as GPT-2 uses sub-word tokenization (Sennrich et al., 2015). We chose the value of 3% as, for
the datasets we considered, it resulted in a marginal
token mask rate of around 15%, echoing the configuration of Devlin et al. (2019).
We add special tokens for each granularity to
our modelâ€™s vocabulary (e.g. [blank word]), so
that the user may specify which granularity they
would like the infilling model to produce. This
functionality can be explored in our demo: https:
//chrisdonahue.com/ilm.
While we focus on this specific mask function in
this paper, we structured the ILM codebase to allow
users to train infilling models for completely different use cases. Users need only define a new mask
function which takes complete documents and outputs lists of character-level spans representing the
desired spans to be masked.
C Hyperparameters
We use early stopping based on the PPL of
the model on infilling the masked token for the
validation set. We train all models using the
default fine-tuning parameters specified in the
transformers library (Wolf et al., 2019), except that we use a batch size of 24 and a sequence
length of 256.
Note that the most straightforward way of training an LM on ILM examples (Section 3.2) is to
maximize the likelihood of the entire concatenated
example: xËœ, [sep], and y. This trains the model to
predict tokens in xËœ even though such behavior is
not necessary at inference time as xËœ will always be
fully-specified. Nevertheless, we found that this additional supervision improved performance when
evaluating model PPL of y. Conveniently, this is
also the default behavior when adapting existing
LM training code for use with ILM.
D Evaluation on language modeling and
infilling other granularities
Our quantitative evaluation (Section 5) examined
the sentence infilling performance of GPT-2 initialized from the large-scale pre-trained checkpoint
STO ABS LYR
LM (scratch) 33.4 52.1 25.1
LM-Rev (scratch) 32.9 53.9 24.7
LM-All (scratch) 30.4 44.6 26.2
ILM (scratch) 30.8 45.3 30.6
LM 17.6 25.7 20.8
LM-Rev 25.1 36.7 23.7
LM-All 17.8 25.2 21.5
ILM 18.1 23.9 23.0
Table 3: Document infilling PPL (or language modeling) of ILM and baselines initialized either from
scratch or from the pre-trained checkpoint across three
datasets. Note that PPL of ILM is similar to LM, implying that our infilling strategy can reasonably maintain
the ability to perform language modeling while extending the ability to infill.
STO ABS LYR
LM (scratch) 34.0 52.8 28.9
LM-Rev (scratch) 34.9 59.3 30.4
LM-All (scratch) 27.0 46.2 24.3
ILM (scratch) 25.5 46.0 27.5
LM 17.5 25.5 23.9
LM-Rev 26.5 39.0 29.2
LM-All 15.1 24.4 19.3
ILM 14.9 23.5 20.2
Table 4: Mixture infilling PPL of all models (a mixture
of all granularities).
after fine-tuning on different datasets and infilling
strategies. Here, we report PPL for GPT-2 both
initialized from scratch and from the pre-trained
checkpoint for several other configurations: language modeling, a mixture of granularities, specific
granularities, and language modeling.
D.1 Language modeling
In Table 3, we report PPL for â€œdocument infilling,â€
which is equivalent to language modeling (because
xËœ is always [blank document]). Because of how
we structured our mask function (Appendix B), 3%
of infilling examples consist of the entire document
masked out, which results in the ability of our ILM
framework to perform standard infilling. We see
that performance of ILM is similar to that of LM on
this task, even though ILM sees far fewer examples
of language modeling compared to LM.
STO ABS LYR
LM (scratch) 35.6 51.5 25.1
LM-Rev (scratch) 34.8 65.1 24.7
LM-All (scratch) 33.4 45.0 26.2
ILM (scratch) 34.3 45.3 30.6
LM 18.3 24.2 20.8
LM-Rev 26.5 42.8 23.7
LM-All 20.4 23.4 21.5
ILM 20.7 22.5 23.0
Table 5: Paragraph infilling PPL of all models.
STO ABS LYR
LM (scratch) 36.0 65.4 33.5
LM-Rev (scratch) 35.1 92.2 35.8
LM-All (scratch) 27.1 53.8 27.1
ILM (scratch) 26.7 51.0 31.0
LM 18.3 27.9 27.7
LM-Rev 27.1 46.5 34.3
LM-All 15.6 22.3 21.4
ILM 15.6 22.4 22.6
Table 6: Sentence infilling PPL of all models.
D.2 Mixture of granularities
In Table 4, we report results for a mixture of granularities. Specifically, we run the same mask function we use for training (Appendix B) on our test
data and evaluate PPL on the masked spans. This
reflects general infilling ability across a wide variety of granularities (and hence lengths). Unlike
our other quantitative evaluations, there may be
multiple variable-length spans missing from each
example in this evaluation. Results are similar to
that of sentence infilling. Namely, that ILM outperforms LM and LM-Rev and is similar to LM-All
despite using much less memory.
D.3 Individual granularities
In Tables 5 to 8 we report PPL values for infilling
performance on paragraphs, sentences, n-grams,
and words, respectively, across the three datasets.
For each granularity, we create one infilling example per document from the test set with exactly
one masked span (randomly chosen from all spans
of that granularity for that document). Then, we
compute PPL only on the tokens which comprise
the masked span, i.e., PPL is computed for all models on exactly the same set of tokens. Across all
granularities, we observe that ILM outperforms
STO ABS LYR
LM (scratch) 36.1 62.5 34.1
LM-Rev (scratch) 36.4 89.1 36.3
LM-All (scratch) 26.4 60.1 24.3
ILM (scratch) 23.1 49.5 26.3
LM 19.2 25.5 28.2
LM-Rev 26.6 45.0 34.8
LM-All 14.5 20.5 18.6
ILM 13.8 21.5 18.8
Table 7: N-gram infilling PPL of all models.
STO ABS LYR
LM (scratch) 32.3 57.2 34.8
LM-Rev (scratch) 31.6 100.0 36.7
LM-All (scratch) 12.6 51.8 12.5
ILM (scratch) 9.2 37.9 12.2
LM 17.1 23.0 28.7
LM-Rev 24.1 45.0 35.1
LM-All 7.5 15.8 9.5
ILM 5.4 14.2 8.5
Table 8: Word infilling PPL of all models.
LM and LM-Rev and either outperforms or is comparable with LM-All while using less memory.
E Details on human evaluation
For human evaluation, we sampled 100 stories from
the test set of the STORIES dataset. From each story,
we masked out one sentence at a time, thereby resulting in 500 stories with masked sentences. Then
we used these stories as context and tasked each
model with infilling the masked sentence.
We compared 8 models in total. In addition to
the four models reported in Section 6 (BERT, SA,
LM, and ILM), we included the models which are
initialized from scratch (as opposed to initialized
from the large-scale pre-trained checkpoint) for
exhaustive comparison. Furthermore, to filter out
spam, we used a control model which always generates â€œThis sentence was generated by a computer.â€
Lastly, we included the original sentence from the
dataset as a reference model (Human) to sanity
check the max score is around 80%.
Each annotator was shown 8 stories, one from
each model, and was asked to identify one of the
five sentences generated by machine (see Figure 4
for an example). Among the 100 collected responses, we filtered out 5 responses whose annotation for the control model was wrong. The quantitative and qualitative results can be found in Table 9
and Figure 5, respectively. All model outputs and
responses of human evaluation can be found at
<a class=link href=https://github.com/chrisdonahue/ilm target=_blank rel=noopener>https://github.com/chrisdonahue/ilm</a>.
Score (%)
Control 0
BERT 20
SA 29
LM (scratch) 40
LM 41
ILM (scratch) 39
ILM 45
Human 78
Table 9: Human evaluation results. Identify one of the five sentences generated by machine.
â—‹ Patty was excited about having her friends over.
â—‹ She had been working hard preparing the food.
â—‹ Patty knew her friends wanted pizza.
â—‹ All of her friends arrived and were seated at the table.
â—‹ Patty had a great time with her friends.
Figure 4: Example of a task and instruction for human
evaluation on Amazon Mechanical Turk.
Example Story with Masked Sentence
Lily always loved to read.
She wondered sometimes,
what it would be like to write a book?
[blank]
Lily did well in the course, and during it,
wrote a short book.
BERT
SA
LM
ILM
Human
I held her hand and helped her sit.
Of her, but she didn&rsquo;t know her.
She practiced reading a lot every week.
Finally, in middle school, her teacher
introduced her to writing that.
She decided to take a course on fiction writing.
BERT
SA
LM
ILM
Human
Or rather, what the next job would be now.
I was going out I was going to the beach.
I put on about thirty sugar cubes.
The issues are getting so many people crazy.
I could never catch up and each week
got worse.
Example Story with Masked Sentence
My old job kept asking too much of me.
Every Wednesday there was a ton of new work.
[blank]
Eventually I got too far behind and had to quit.
I will look for a new job.
BERT
SA
LM
ILM
Human
Today was the first concert that she had to
see every where.
She was going to go to the play.
When she went on stage she smoothly
walked right past the audience.
When she got on stage the band was amazing.
As soon as she got on the audience applauded.
Example Story with Masked Sentence
Yesterday was Kelly&rsquo;s first concert.
She was nervous to get on stage.
[blank]
Kelly was then happy.
She couldn&rsquo;t wait to do it again.
Figure 5: Examples of sentence-level infills by different models.</li></ul><p>é€‰é¢˜èƒŒæ™¯ï¼Œç ”ç©¶èƒŒæ™¯ ï¼ˆåˆ†ææœ¬é€‰é¢˜èŒƒç•´å†…å°šæœªå¾—åˆ°è¾ƒå¥½è§£å†³çš„å­¦æœ¯æˆ–å®è·µéš¾é¢˜ï¼Œé˜è¿°é€‰é¢˜çš„ç¼˜èµ·ä¸ä¾æ®ï¼‰</p><hr><p><strong>Cursor</strong></p><p>å¥½çš„ï¼Œä»¥ä¸‹æ˜¯æ ¹æ®ä½ ç»™å‡ºçš„è®ºæ–‡å†…å®¹ï¼Œæ•´ç†å‡ºçš„**â€œé€‰é¢˜èƒŒæ™¯/ç ”ç©¶èƒŒæ™¯â€**åˆ†æï¼Œé€‚åˆç”¨äºå¼€é¢˜æŠ¥å‘Šæˆ–è®ºæ–‡å‰è¨€éƒ¨åˆ†ï¼š</p><hr><h2 id=é€‰é¢˜èƒŒæ™¯--ç ”ç©¶èƒŒæ™¯>é€‰é¢˜èƒŒæ™¯ / ç ”ç©¶èƒŒæ™¯</h2><h3 id=1-æ–‡æœ¬è¡¥å…¨text-infillingçš„é‡è¦æ€§ä¸åº”ç”¨å‰æ™¯>1. <strong>æ–‡æœ¬è¡¥å…¨ï¼ˆText Infillingï¼‰çš„é‡è¦æ€§ä¸åº”ç”¨å‰æ™¯</strong></h3><p>æ–‡æœ¬è¡¥å…¨ï¼ˆtext infillingï¼‰æ˜¯æŒ‡åœ¨æ–‡æœ¬ä¸­ä»»æ„ä½ç½®é¢„æµ‹ç¼ºå¤±ç‰‡æ®µï¼Œä½¿å¾—è¡¥å…¨åçš„æ–‡æœ¬åœ¨è¯­ä¹‰å’Œè¯­æ³•ä¸Šä¸ä¸Šä¸‹æ–‡ä¸€è‡´ã€‚è¿™ä¸€ä»»åŠ¡åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰å¹¿æ³›çš„ä»·å€¼ï¼Œä¾‹å¦‚ï¼š</p><ul><li><strong>å†™ä½œè¾…åŠ©</strong>ï¼šå¸®åŠ©ç”¨æˆ·ç¼–è¾‘ã€æ¶¦è‰²ã€æ‰©å±•æˆ–ä¿®å¤æ–‡æœ¬ï¼Œæé«˜å†™ä½œæ•ˆç‡å’Œè´¨é‡ã€‚</li><li><strong>ç¢ç‰‡åŒ–ä¿¡æ¯æ•´åˆ</strong>ï¼šå°†é›¶æ•£çš„æƒ³æ³•ã€ç‰‡æ®µæˆ–ç¬”è®°è‡ªåŠ¨ä¸²è”æˆè¿è´¯çš„æ–‡æœ¬ã€‚</li><li><strong>å¤æ–‡/æ®‹ç¼ºæ–‡çŒ®ä¿®å¤</strong>ï¼šç”¨äºä¿®å¤å†å²æ–‡çŒ®ã€å¤ç±ã€ç¢‘åˆ»ç­‰å› æŸæ¯è€Œç¼ºå¤±çš„å†…å®¹ã€‚</li><li><strong>å¯¹è¯ç³»ç»Ÿä¸æ™ºèƒ½é—®ç­”</strong>ï¼šæå‡ç³»ç»Ÿå¯¹ä¸å®Œæ•´è¾“å…¥çš„ç†è§£å’Œå“åº”èƒ½åŠ›ã€‚</li></ul><h3 id=2-ç°æœ‰æ–¹æ³•çš„å±€é™æ€§>2. <strong>ç°æœ‰æ–¹æ³•çš„å±€é™æ€§</strong></h3><p>å°½ç®¡æ–‡æœ¬è¡¥å…¨ä»»åŠ¡å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†ç›®å‰ä¸»æµçš„è‡ªç„¶è¯­è¨€ç”Ÿæˆæ¨¡å‹åœ¨è¯¥ä»»åŠ¡ä¸Šä»å­˜åœ¨æ˜æ˜¾ä¸è¶³ï¼Œä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š</p><ul><li><strong>å•å‘æ€§é™åˆ¶</strong>ï¼šä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPTç³»åˆ—ï¼‰ä¸»è¦ç”¨äºâ€œè¯­è¨€å»ºæ¨¡â€ä»»åŠ¡ï¼Œå³åªèƒ½åŸºäºå·²æœ‰æ–‡æœ¬çš„â€œè¿‡å»â€å†…å®¹è¿›è¡Œé¢„æµ‹ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨â€œæœªæ¥â€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ä½¿å¾—å®ƒä»¬åœ¨è¡¥å…¨æ–‡æœ¬ä¸­é—´ç¼ºå¤±å†…å®¹æ—¶è¡¨ç°ä¸ä½³ã€‚</li><li><strong>å›ºå®šé•¿åº¦é™åˆ¶</strong>ï¼šå¦‚BERTã€SpanBERTç­‰åŸºäºæ©ç ï¼ˆmaskï¼‰çš„åŒå‘æ¨¡å‹ï¼Œè™½ç„¶èƒ½å¤Ÿåˆ©ç”¨å‰åæ–‡ä¿¡æ¯ï¼Œä½†å…¶è¡¥å…¨èƒ½åŠ›é€šå¸¸å±€é™äº<strong>å›ºå®šé•¿åº¦</strong>çš„æ©ç ï¼Œéš¾ä»¥çµæ´»åº”å¯¹å®é™…åº”ç”¨ä¸­<strong>å˜é•¿ç¼ºå¤±ç‰‡æ®µ</strong>çš„è¡¥å…¨éœ€æ±‚ã€‚</li><li><strong>æ¶æ„å¤æ‚æ€§ä¸è¿ç§»æ€§å·®</strong>ï¼šéƒ¨åˆ†ç ”ç©¶ï¼ˆå¦‚Zhu et al., 2019ï¼‰æå‡ºäº†å¯å˜é•¿åº¦è¡¥å…¨æ–¹æ³•ï¼Œä½†å¾€å¾€ä¾èµ–äºä¸“é—¨è®¾è®¡çš„æ¨¡å‹ç»“æ„ï¼Œéš¾ä»¥ç›´æ¥åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œè¿ç§»å’Œæ‰©å±•æ€§æœ‰é™ã€‚</li><li><strong>æ¨ç†æ•ˆç‡ä¸å†…å­˜æ¶ˆè€—</strong>ï¼šæŸäº›æ–¹æ³•ï¼ˆå¦‚LM-Allï¼‰è™½ç„¶èƒ½åˆ©ç”¨å…¨éƒ¨ä¸Šä¸‹æ–‡ï¼Œä½†ä¼šå¯¼è‡´è¾“å…¥åºåˆ—é•¿åº¦å¤§å¹…å¢åŠ ï¼Œå¸¦æ¥æ›´é«˜çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ï¼Œå½±å“å®é™…åº”ç”¨ã€‚</li></ul><h3 id=3-å­¦æœ¯ä¸å®è·µéš¾é¢˜>3. <strong>å­¦æœ¯ä¸å®è·µéš¾é¢˜</strong></h3><ul><li><strong>å¦‚ä½•è®©ç°æœ‰å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-2ï¼‰å…·å¤‡é«˜æ•ˆã€çµæ´»çš„æ–‡æœ¬è¡¥å…¨èƒ½åŠ›ï¼Œæ—¢èƒ½åˆ©ç”¨åŒå‘ä¸Šä¸‹æ–‡ï¼Œåˆèƒ½æ”¯æŒå˜é•¿ç‰‡æ®µè¡¥å…¨ï¼Ÿ</strong></li><li><strong>å¦‚ä½•åœ¨ä¸æ˜¾è‘—å¢åŠ æ¨¡å‹å¤æ‚åº¦å’Œæ¨ç†æˆæœ¬çš„å‰æä¸‹ï¼Œæå‡è¡¥å…¨æ–‡æœ¬çš„è‡ªç„¶åº¦å’Œå¤šæ ·æ€§ï¼Ÿ</strong></li><li><strong>å¦‚ä½•è®©è¡¥å…¨ç»“æœåœ¨å¤šç§æ–‡æœ¬ç±»å‹ï¼ˆå¦‚æ•…äº‹ã€æ‘˜è¦ã€æ­Œè¯ç­‰ï¼‰ä¸­éƒ½å…·å¤‡è‰¯å¥½çš„é€šç”¨æ€§å’Œå¯æ§æ€§ï¼Ÿ</strong></li></ul><h3 id=4-é€‰é¢˜ç¼˜èµ·ä¸ä¾æ®>4. <strong>é€‰é¢˜ç¼˜èµ·ä¸ä¾æ®</strong></h3><p>éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œå¦‚ä½•è¿›ä¸€æ­¥æ‹“å±•å…¶ç”Ÿæˆèƒ½åŠ›ã€æå‡å…¶åœ¨å®é™…å†™ä½œå’Œç¼–è¾‘åœºæ™¯ä¸­çš„å®ç”¨æ€§ï¼Œæˆä¸ºå½“å‰å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå…³æ³¨çš„çƒ­ç‚¹ã€‚æ–‡æœ¬è¡¥å…¨ä½œä¸ºè¯­è¨€ç”Ÿæˆçš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå°šæœªå¾—åˆ°è¶³å¤Ÿé‡è§†å’Œç³»ç»Ÿæ€§ç ”ç©¶ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå±€é™äºå•å‘é¢„æµ‹ï¼Œè¦ä¹ˆéš¾ä»¥å¤„ç†å˜é•¿ç¼ºå¤±ï¼Œè¦ä¹ˆå¯¹æ¨¡å‹æ¶æ„æœ‰è¾ƒé«˜è¦æ±‚ï¼Œéš¾ä»¥ç›´æ¥è¿ç§»å’Œè½åœ°ã€‚</p><p>æœ¬ç ”ç©¶æ­£æ˜¯åŸºäºä¸Šè¿°å­¦æœ¯å’Œå®è·µéš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§<strong>åŸºäºè¯­è¨€å»ºæ¨¡çš„é€šç”¨æ–‡æœ¬è¡¥å…¨æ¡†æ¶</strong>ï¼ˆInfilling by Language Modeling, ILMï¼‰ï¼Œæ—¨åœ¨å……åˆ†åˆ©ç”¨ç°æœ‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œé€šè¿‡ç®€å•çš„è®­ç»ƒæ•°æ®é‡æ„å’Œæœ€å°çš„æ¶æ„æ”¹åŠ¨ï¼Œå®ç°é«˜æ•ˆã€çµæ´»ã€é€šç”¨çš„æ–‡æœ¬è¡¥å…¨ã€‚è¯¥æ–¹æ³•ä¸ä»…æå‡äº†è¡¥å…¨è´¨é‡ï¼Œè¿˜å…¼é¡¾äº†æ¨ç†æ•ˆç‡å’Œå®é™…åº”ç”¨çš„å¯æ‰©å±•æ€§ï¼Œä¸ºæ™ºèƒ½å†™ä½œã€æ–‡æœ¬ä¿®å¤ç­‰é¢†åŸŸæä¾›äº†æ–°çš„è§£å†³æ€è·¯å’ŒæŠ€æœ¯åŸºç¡€ã€‚</p><hr><p>å¦‚éœ€æ›´ç²¾ç‚¼æˆ–æ›´è¯¦ç»†çš„ç‰ˆæœ¬ï¼Œæˆ–éœ€è¦ç»“åˆå…·ä½“åº”ç”¨åœºæ™¯ï¼ˆå¦‚å†™ä½œè¾…åŠ©ã€å¤æ–‡ä¿®å¤ç­‰ï¼‰è¿›ä¸€æ­¥å±•å¼€ï¼Œè¯·å‘ŠçŸ¥ï¼</p><hr><p><strong>User</strong></p><p>æ–‡çŒ®ç»¼è¿° ï¼ˆç³»ç»Ÿæ¢³ç†æœ¬é€‰é¢˜ç›¸å…³çš„å…·æœ‰ä»£è¡¨æ€§çš„æ–‡çŒ®ï¼Œåˆ†æç›¸å…³ç ”ç©¶çš„å‘å±•è„‰ç»œä¸è¿›å±•ï¼Œè¯„è¿°å·²æœ‰ç ”ç©¶å­˜åœ¨çš„é—®é¢˜ä¸ä¸è¶³ï¼‰</p><hr><p><strong>Cursor</strong></p><p>å¥½çš„ï¼Œä»¥ä¸‹æ˜¯é’ˆå¯¹â€œæ–‡æœ¬è¡¥å…¨/æ–‡æœ¬infillingâ€ä»»åŠ¡çš„<strong>æ–‡çŒ®ç»¼è¿°</strong>ï¼Œæ¶µç›–ä»£è¡¨æ€§æ–‡çŒ®ã€å‘å±•è„‰ç»œã€è¿›å±•ä¸ä¸è¶³ï¼Œé€‚åˆç”¨äºå¼€é¢˜æŠ¥å‘Šæˆ–è®ºæ–‡ç»¼è¿°éƒ¨åˆ†ï¼š</p><hr><h2 id=æ–‡çŒ®ç»¼è¿°>æ–‡çŒ®ç»¼è¿°</h2><h3 id=1-ä»»åŠ¡èµ·æºä¸æ—©æœŸç ”ç©¶>1. ä»»åŠ¡èµ·æºä¸æ—©æœŸç ”ç©¶</h3><p><strong>æ–‡æœ¬è¡¥å…¨ï¼ˆText Infillingï¼‰<strong>ä»»åŠ¡æœ€æ—©å¯è¿½æº¯åˆ°</strong>Clozeæµ‹è¯•</strong>ï¼ˆTaylor, 1953ï¼‰ï¼Œå³è®©ç³»ç»Ÿæ ¹æ®ä¸Šä¸‹æ–‡å¡«è¡¥è¢«éšæœºåˆ é™¤çš„å•è¯ã€‚Clozeä»»åŠ¡æœ€åˆç”¨äºè¯­è¨€èƒ½åŠ›è¯„æµ‹ï¼Œåè¢«å¹¿æ³›ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚</p><ul><li><strong>Taylor, W. L. (1953). â€œCloze procedureâ€: A new tool for measuring readability.</strong><br>æå‡ºClozeæµ‹è¯•ï¼Œå¼€å¯äº†åŸºäºä¸Šä¸‹æ–‡çš„æ–‡æœ¬è¡¥å…¨ç ”ç©¶ã€‚</li></ul><p>éšç€NLPæŠ€æœ¯çš„å‘å±•ï¼ŒClozeä»»åŠ¡è¢«æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯ï¼Œå¦‚<strong>å™äº‹Cloze</strong>ï¼ˆChambers & Jurafsky, 2008ï¼‰ï¼Œç”¨äºæ¨æ–­æ•…äº‹ä¸­çš„ç¼ºå¤±äº‹ä»¶ã€‚</p><hr><h3 id=2-åŸºäºè¯­è¨€æ¨¡å‹çš„è¡¥å…¨æ–¹æ³•>2. åŸºäºè¯­è¨€æ¨¡å‹çš„è¡¥å…¨æ–¹æ³•</h3><h4 id=21-å•å‘è‡ªå›å½’è¯­è¨€æ¨¡å‹>2.1 å•å‘è‡ªå›å½’è¯­è¨€æ¨¡å‹</h4><ul><li><strong>GPTç³»åˆ—ï¼ˆRadford et al., 2019ï¼‰</strong><br>é‡‡ç”¨è‡ªå›å½’ï¼ˆå·¦åˆ°å³ï¼‰æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆè¿è´¯æ–‡æœ¬ï¼Œä½†åªèƒ½åˆ©ç”¨â€œè¿‡å»â€ä¸Šä¸‹æ–‡ï¼Œéš¾ä»¥è¡¥å…¨ä¸­é—´ç¼ºå¤±å†…å®¹ã€‚</li></ul><h4 id=22-åŒå‘æ©ç è¯­è¨€æ¨¡å‹>2.2 åŒå‘æ©ç è¯­è¨€æ¨¡å‹</h4><ul><li><p><strong>BERTï¼ˆDevlin et al., 2019ï¼‰</strong><br>é€šè¿‡æ©ç ï¼ˆmaskï¼‰æœºåˆ¶ï¼Œåˆ©ç”¨å‰åæ–‡ä¿¡æ¯é¢„æµ‹è¢«é®ç›–çš„å•è¯ã€‚</p><ul><li><strong>ä¼˜ç‚¹</strong>ï¼šèƒ½åˆ©ç”¨åŒå‘ä¸Šä¸‹æ–‡ï¼Œè¡¥å…¨æ•ˆæœå¥½ã€‚</li><li><strong>ç¼ºç‚¹</strong>ï¼šåªèƒ½è¡¥å…¨<strong>å›ºå®šé•¿åº¦</strong>çš„æ©ç ï¼Œéš¾ä»¥å¤„ç†å˜é•¿ç¼ºå¤±ç‰‡æ®µï¼Œä¸”æ¨ç†æ—¶éœ€é€ä¸ªtokenç”Ÿæˆï¼Œæ•ˆç‡è¾ƒä½ã€‚</li></ul></li><li><p><strong>SpanBERTï¼ˆJoshi et al., 2019ï¼‰</strong><br>æ‰©å±•BERTï¼Œæ”¯æŒspançº§åˆ«çš„æ©ç è¡¥å…¨ï¼Œä½†ä»ç„¶å±€é™äºå›ºå®šé•¿åº¦spanã€‚</p></li></ul><h4 id=23-å˜é•¿è¡¥å…¨ä¸æ’å…¥å¼ç”Ÿæˆ>2.3 å˜é•¿è¡¥å…¨ä¸æ’å…¥å¼ç”Ÿæˆ</h4><ul><li><p><strong>Text Infillingï¼ˆZhu et al., 2019ï¼‰</strong><br>æå‡ºå¯å˜é•¿åº¦spançš„è¡¥å…¨æ–¹æ³•ï¼Œé‡‡ç”¨ç‰¹æ®Šçš„æ’å…¥å¼ç”Ÿæˆæ¶æ„ã€‚</p><ul><li><strong>ä¼˜ç‚¹</strong>ï¼šæ”¯æŒå˜é•¿è¡¥å…¨ã€‚</li><li><strong>ç¼ºç‚¹</strong>ï¼šéœ€è¦ä¸“é—¨è®¾è®¡çš„æ¨¡å‹ç»“æ„ï¼Œéš¾ä»¥ç›´æ¥åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ã€‚</li></ul></li><li><p><strong>Insertion-based Decodingï¼ˆGu et al., 2019ï¼‰</strong><br>é€šè¿‡è‡ªåŠ¨æ¨æ–­ç”Ÿæˆé¡ºåºï¼Œå®ç°æ’å…¥å¼ç”Ÿæˆï¼Œæå‡äº†çµæ´»æ€§ï¼Œä½†è®­ç»ƒå’Œæ¨ç†å¤æ‚åº¦è¾ƒé«˜ã€‚</p></li></ul><hr><h3 id=3-ç”Ÿæˆå¼è¡¥å…¨ä¸è¿ç§»å­¦ä¹ >3. ç”Ÿæˆå¼è¡¥å…¨ä¸è¿ç§»å­¦ä¹ </h3><ul><li><p><strong>MaskGANï¼ˆFedus et al., 2018ï¼‰</strong><br>åˆ©ç”¨GANæ€æƒ³è¿›è¡Œæ–‡æœ¬è¡¥å…¨ï¼Œèƒ½ç”Ÿæˆå¤šæ ·åŒ–çš„è¡¥å…¨å†…å®¹ï¼Œä½†è®­ç»ƒä¸ç¨³å®šï¼Œéš¾ä»¥å¤§è§„æ¨¡åº”ç”¨ã€‚</p></li><li><p><strong>T5ï¼ˆRaffel et al., 2019ï¼‰</strong><br>ç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒspan infillingä½œä¸ºé¢„è®­ç»ƒç›®æ ‡ï¼Œæå‡äº†ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ï¼Œä½†ä¸»è¦å…³æ³¨è¿ç§»å­¦ä¹ è€Œéç”Ÿæˆè´¨é‡ã€‚</p></li></ul><hr><h3 id=4-è¿‘æœŸä»£è¡¨æ€§å·¥ä½œ>4. è¿‘æœŸä»£è¡¨æ€§å·¥ä½œ</h3><ul><li><strong>ILMï¼ˆInfilling by Language Modeling, Donahue et al., 2020ï¼‰</strong><br>æœ¬æ–‡æå‡ºçš„ILMæ–¹æ³•ï¼Œé€šè¿‡é‡æ„è®­ç»ƒæ ·æœ¬ï¼Œå°†å¸¦æœ‰ç©ºç™½çš„æ–‡æœ¬ä¸ç­”æ¡ˆæ‹¼æ¥ï¼Œåˆ©ç”¨æ ‡å‡†è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-2ï¼‰è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚<ul><li><strong>ä¼˜ç‚¹</strong>ï¼šæ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œèƒ½åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒå˜é•¿è¡¥å…¨ï¼Œæ¨ç†é«˜æ•ˆã€‚</li><li><strong>è¿›å±•</strong>ï¼šåœ¨æ•…äº‹ã€æ‘˜è¦ã€æ­Œè¯ç­‰å¤šé¢†åŸŸå–å¾—äº†ä¼˜å¼‚çš„è¡¥å…¨æ•ˆæœï¼Œä¸”äººç±»éš¾ä»¥åŒºåˆ†å…¶è¡¥å…¨å¥ä¸çœŸå®å¥ã€‚</li></ul></li></ul><hr><h3 id=5-ç°æœ‰ç ”ç©¶çš„ä¸è¶³ä¸æŒ‘æˆ˜>5. ç°æœ‰ç ”ç©¶çš„ä¸è¶³ä¸æŒ‘æˆ˜</h3><ul><li><strong>çµæ´»æ€§ä¸è¶³</strong>ï¼šBERTç­‰æ©ç æ¨¡å‹åªèƒ½è¡¥å…¨å›ºå®šé•¿åº¦ï¼Œæ’å…¥å¼ç”Ÿæˆæ–¹æ³•ç»“æ„å¤æ‚ï¼Œéš¾ä»¥ç›´æ¥è¿ç§»ã€‚</li><li><strong>æ¨ç†æ•ˆç‡é—®é¢˜</strong>ï¼šéƒ¨åˆ†æ–¹æ³•æ¨ç†æ—¶éœ€å¤šæ¬¡è¿­ä»£ï¼Œæ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥åº”ç”¨äºå®é™…åœºæ™¯ã€‚</li><li><strong>æ³›åŒ–èƒ½åŠ›æœ‰é™</strong>ï¼šéƒ¨åˆ†æ–¹æ³•ä»…åœ¨ç‰¹å®šé¢†åŸŸæˆ–ç‰¹å®šç±»å‹æ–‡æœ¬ä¸Šæœ‰æ•ˆï¼Œç¼ºä¹é€šç”¨æ€§ã€‚</li><li><strong>å¯¹å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„åˆ©ç”¨ä¸å……åˆ†</strong>ï¼šè®¸å¤šæ–¹æ³•æ— æ³•ç›´æ¥åˆ©ç”¨å¦‚GPT-2ã€GPT-3ç­‰å¤§æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ã€‚</li><li><strong>è¡¥å…¨å†…å®¹çš„å¤šæ ·æ€§ä¸è‡ªç„¶åº¦</strong>ï¼šå¦‚ä½•ç”Ÿæˆæ—¢å¤šæ ·åˆè‡ªç„¶ã€ä¸ä¸Šä¸‹æ–‡é«˜åº¦ä¸€è‡´çš„è¡¥å…¨å†…å®¹ï¼Œä»æ˜¯éš¾ç‚¹ã€‚</li></ul><hr><h2 id=ç»¼è¿°æ€»ç»“>ç»¼è¿°æ€»ç»“</h2><p>æ€»ä½“æ¥çœ‹ï¼Œæ–‡æœ¬è¡¥å…¨ä»»åŠ¡ç»å†äº†ä»Clozeæµ‹è¯•åˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªå›å½’ã€æ©ç ã€æ’å…¥å¼ç­‰å¤šç§æ–¹æ³•çš„å‘å±•ã€‚è¿‘å¹´æ¥ï¼Œå¦‚ä½•<strong>é«˜æ•ˆåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒå˜é•¿ã€çµæ´»ã€è‡ªç„¶çš„æ–‡æœ¬è¡¥å…¨</strong>ï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹ã€‚ILMç­‰æ–°æ–¹æ³•ä¸ºè§£å†³ä¸Šè¿°éš¾é¢˜æä¾›äº†æ–°çš„æ€è·¯ï¼Œä½†åœ¨è¡¥å…¨å¤šæ ·æ€§ã€æ¨ç†æ•ˆç‡ã€é¢†åŸŸæ³›åŒ–ç­‰æ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚</p><hr><p>å¦‚éœ€æ›´è¯¦ç»†çš„æ–‡çŒ®å¼•ç”¨æ ¼å¼ã€æŸä¸€æ–¹å‘çš„æ·±å…¥åˆ†æï¼Œæˆ–ç»“åˆä¸­æ–‡æ–‡çŒ®è¡¥å……ï¼Œæ¬¢è¿ç»§ç»­æé—®ï¼</p><hr><p><strong>User</strong></p><p>ä¸€å¤§æ®µæ–‡å­—ï¼Œä¸è¦åˆ†ç‚¹</p><hr><p><strong>Cursor</strong></p><p>æ–‡æœ¬è¡¥å…¨ï¼ˆText Infillingï¼‰ä»»åŠ¡æœ€æ—©èµ·æºäºClozeæµ‹è¯•ï¼ˆTaylor, 1953ï¼‰ï¼Œå³è®©ç³»ç»Ÿæ ¹æ®ä¸Šä¸‹æ–‡å¡«è¡¥è¢«éšæœºåˆ é™¤çš„å•è¯ï¼Œè¿™ä¸€ä»»åŠ¡æœ€åˆç”¨äºè¯­è¨€èƒ½åŠ›è¯„æµ‹ï¼Œåæ¥è¢«å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ã€‚éšç€NLPæŠ€æœ¯çš„å‘å±•ï¼ŒClozeä»»åŠ¡è¢«æ‰©å±•åˆ°æ›´å¤æ‚çš„åœºæ™¯ï¼Œå¦‚å™äº‹Clozeï¼ˆChambers & Jurafsky, 2008ï¼‰ï¼Œç”¨äºæ¨æ–­æ•…äº‹ä¸­çš„ç¼ºå¤±äº‹ä»¶ã€‚è¿‘å¹´æ¥ï¼Œéšç€æ·±åº¦å­¦ä¹ å’Œå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å…´èµ·ï¼Œæ–‡æœ¬è¡¥å…¨ä»»åŠ¡çš„ç ”ç©¶å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æ—©æœŸçš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPTç³»åˆ—ï¼ŒRadford et al., 2019ï¼‰é‡‡ç”¨å•å‘ç”Ÿæˆæ–¹å¼ï¼Œèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆè¿è´¯æ–‡æœ¬ï¼Œä½†åªèƒ½åˆ©ç”¨â€œè¿‡å»â€ä¸Šä¸‹æ–‡ï¼Œéš¾ä»¥è¡¥å…¨ä¸­é—´ç¼ºå¤±å†…å®¹ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼ŒBERTï¼ˆDevlin et al., 2019ï¼‰ç­‰åŒå‘æ©ç è¯­è¨€æ¨¡å‹é€šè¿‡maskæœºåˆ¶ï¼Œåˆ©ç”¨å‰åæ–‡ä¿¡æ¯é¢„æµ‹è¢«é®ç›–çš„å•è¯ï¼Œæå‡äº†è¡¥å…¨èƒ½åŠ›ï¼Œä½†å…¶è¡¥å…¨èƒ½åŠ›é€šå¸¸å±€é™äºå›ºå®šé•¿åº¦çš„æ©ç ï¼Œéš¾ä»¥çµæ´»åº”å¯¹å®é™…åº”ç”¨ä¸­å˜é•¿ç¼ºå¤±ç‰‡æ®µçš„è¡¥å…¨éœ€æ±‚ã€‚SpanBERTï¼ˆJoshi et al., 2019ï¼‰è¿›ä¸€æ­¥æ‰©å±•äº†BERTï¼Œæ”¯æŒspançº§åˆ«çš„æ©ç è¡¥å…¨ï¼Œä½†ä»ç„¶å±€é™äºå›ºå®šé•¿åº¦spanã€‚ä¸ºäº†è§£å†³å˜é•¿è¡¥å…¨é—®é¢˜ï¼ŒZhuç­‰ï¼ˆ2019ï¼‰æå‡ºäº†å¯å˜é•¿åº¦spançš„è¡¥å…¨æ–¹æ³•ï¼Œé‡‡ç”¨ç‰¹æ®Šçš„æ’å…¥å¼ç”Ÿæˆæ¶æ„ï¼Œè™½ç„¶æ”¯æŒå˜é•¿è¡¥å…¨ï¼Œä½†éœ€è¦ä¸“é—¨è®¾è®¡çš„æ¨¡å‹ç»“æ„ï¼Œéš¾ä»¥ç›´æ¥åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ã€‚Insertion-based Decodingï¼ˆGu et al., 2019ï¼‰é€šè¿‡è‡ªåŠ¨æ¨æ–­ç”Ÿæˆé¡ºåºï¼Œå®ç°æ’å…¥å¼ç”Ÿæˆï¼Œæå‡äº†çµæ´»æ€§ï¼Œä½†è®­ç»ƒå’Œæ¨ç†å¤æ‚åº¦è¾ƒé«˜ã€‚MaskGANï¼ˆFedus et al., 2018ï¼‰åˆ©ç”¨GANæ€æƒ³è¿›è¡Œæ–‡æœ¬è¡¥å…¨ï¼Œèƒ½ç”Ÿæˆå¤šæ ·åŒ–çš„è¡¥å…¨å†…å®¹ï¼Œä½†è®­ç»ƒä¸ç¨³å®šï¼Œéš¾ä»¥å¤§è§„æ¨¡åº”ç”¨ã€‚T5ï¼ˆRaffel et al., 2019ï¼‰æå‡ºäº†ç»Ÿä¸€çš„æ–‡æœ¬åˆ°æ–‡æœ¬é¢„è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒspan infillingä½œä¸ºé¢„è®­ç»ƒç›®æ ‡ï¼Œæå‡äº†ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ï¼Œä½†ä¸»è¦å…³æ³¨è¿ç§»å­¦ä¹ è€Œéç”Ÿæˆè´¨é‡ã€‚è¿‘æœŸï¼ŒDonahueç­‰ï¼ˆ2020ï¼‰æå‡ºçš„ILMï¼ˆInfilling by Language Modelingï¼‰æ–¹æ³•ï¼Œé€šè¿‡é‡æ„è®­ç»ƒæ ·æœ¬ï¼Œå°†å¸¦æœ‰ç©ºç™½çš„æ–‡æœ¬ä¸ç­”æ¡ˆæ‹¼æ¥ï¼Œåˆ©ç”¨æ ‡å‡†è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-2ï¼‰è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼Œæ— éœ€ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œèƒ½å……åˆ†åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒå˜é•¿è¡¥å…¨ï¼Œæ¨ç†é«˜æ•ˆï¼Œå¹¶åœ¨æ•…äº‹ã€æ‘˜è¦ã€æ­Œè¯ç­‰å¤šé¢†åŸŸå–å¾—äº†ä¼˜å¼‚çš„è¡¥å…¨æ•ˆæœï¼Œä¸”äººç±»éš¾ä»¥åŒºåˆ†å…¶è¡¥å…¨å¥ä¸çœŸå®å¥ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç°æœ‰ç ”ç©¶ä»å­˜åœ¨ä¸€äº›ä¸è¶³ï¼Œå¦‚BERTç­‰æ©ç æ¨¡å‹åªèƒ½è¡¥å…¨å›ºå®šé•¿åº¦ï¼Œæ’å…¥å¼ç”Ÿæˆæ–¹æ³•ç»“æ„å¤æ‚ï¼Œéš¾ä»¥ç›´æ¥è¿ç§»ï¼Œéƒ¨åˆ†æ–¹æ³•æ¨ç†æ—¶éœ€å¤šæ¬¡è¿­ä»£ï¼Œæ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥åº”ç”¨äºå®é™…åœºæ™¯ï¼Œéƒ¨åˆ†æ–¹æ³•ä»…åœ¨ç‰¹å®šé¢†åŸŸæˆ–ç‰¹å®šç±»å‹æ–‡æœ¬ä¸Šæœ‰æ•ˆï¼Œç¼ºä¹é€šç”¨æ€§ï¼Œè®¸å¤šæ–¹æ³•æ— æ³•ç›´æ¥åˆ©ç”¨å¦‚GPT-2ã€GPT-3ç­‰å¤§æ¨¡å‹çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥åŠå¦‚ä½•ç”Ÿæˆæ—¢å¤šæ ·åˆè‡ªç„¶ã€ä¸ä¸Šä¸‹æ–‡é«˜åº¦ä¸€è‡´çš„è¡¥å…¨å†…å®¹ä»æ˜¯éš¾ç‚¹ã€‚æ€»ä½“æ¥çœ‹ï¼Œæ–‡æœ¬è¡¥å…¨ä»»åŠ¡ç»å†äº†ä»Clozeæµ‹è¯•åˆ°åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªå›å½’ã€æ©ç ã€æ’å…¥å¼ç­‰å¤šç§æ–¹æ³•çš„å‘å±•ï¼Œè¿‘å¹´æ¥ï¼Œå¦‚ä½•é«˜æ•ˆåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒå˜é•¿ã€çµæ´»ã€è‡ªç„¶çš„æ–‡æœ¬è¡¥å…¨ï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹ï¼ŒILMç­‰æ–°æ–¹æ³•ä¸ºè§£å†³ä¸Šè¿°éš¾é¢˜æä¾›äº†æ–°çš„æ€è·¯ï¼Œä½†åœ¨è¡¥å…¨å¤šæ ·æ€§ã€æ¨ç†æ•ˆç‡ã€é¢†åŸŸæ³›åŒ–ç­‰æ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚</p></section><footer class=article-footer><section class=article-tags><a href=/zh-cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA/>è®¡ç®—æœº</a>
<a href=/zh-cn/tags/%E9%80%BB%E8%BE%91/>é€»è¾‘</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>æœ€åæ›´æ–°äº Jun 18, 2025 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>ç›¸å…³æ–‡ç« </h2><div class=related-content><div class="flex article-list--tile"><article><a href=/zh-cn/p/2025%E5%B9%B46%E6%9C%8817%E6%97%A5/><div class=article-details><h2 class=article-title>2025å¹´6æœˆ17æ—¥</h2></div></a></article><article><a href=/zh-cn/p/2025%E5%B9%B46%E6%9C%8816%E6%97%A5/><div class=article-details><h2 class=article-title>2025å¹´6æœˆ16æ—¥</h2></div></a></article><article><a href=/zh-cn/p/2025%E5%B9%B46%E6%9C%8815%E6%97%A5/><div class=article-details><h2 class=article-title>2025å¹´6æœˆ15æ—¥</h2></div></a></article><article><a href=/zh-cn/p/2025%E5%B9%B46%E6%9C%8814%E6%97%A5/><div class=article-details><h2 class=article-title>2025å¹´6æœˆ14æ—¥</h2></div></a></article><article><a href=/zh-cn/p/%E8%B6%85%E8%B6%8A%E7%B3%BB%E7%BB%9F/><div class=article-details><h2 class=article-title>è¶…è¶Šç³»ç»Ÿ</h2></div></a></article></div></div></aside><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js></script><script src=https://cdn.jsdelivr.net/npm/blueimp-md5@2.18.0/js/md5.min.js></script><script>const gitalk=new Gitalk({clientID:"Ov23liqc7SD1bN7OvCI6",clientSecret:"7b0ed5cd67a344de02c2efded9da2dfb8ac14783",repo:"tannal.github.io",owner:"tannal",admin:["tannal"],distractionFreeMode:!1,id:md5(location.pathname),proxy:null});(function(){const e=["localhost","127.0.0.1"],t=window.location.hostname;if(e.includes(t)){document.getElementById("gitalk-container").innerHTML="Gitalk comments not available by default when the website is previewed locally.";return}gitalk.render("gitalk-container")})()</script><footer class=site-footer><section class=copyright>&copy;
2024 -
2025 è°­ç›Ÿ</section><section class=powerby>ä½¿ç”¨ <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> æ„å»º<br>ä¸»é¢˜ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.32.0>Stack</a></b> ç”± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> è®¾è®¡</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>