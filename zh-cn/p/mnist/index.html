<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content='多机多卡训练 在本教程中，我们将使用 PyTorch 和 Accelerate 库来训练一个简单的神经网络模型，该模型用于对 FashionMNIST 数据集中的服装图像进行分类。我们将使用多机多卡训练模型，以便在多个 GPU 上并行训练模型。\nimport os import torch import torch.distributed as dist from torch import nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor from accelerate import Accelerator def setup(rank, world_size): os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39; os.environ[&#39;MASTER_PORT&#39;] = &#39;12355&#39; dist.init_process_group("nccl", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def train(rank, world_size): setup(rank, world_size) accelerator = Accelerator() # 下载训练数据 training_data = datasets.FashionMNIST( root="data", train=True, download=True, transform=ToTensor(), ) # 下载测试数据 test_data = datasets.FashionMNIST( root="data", train=False, download=True, transform=ToTensor(), ) batch_size = 32 train_dataloader = DataLoader(training_data, batch_size=batch_size) test_dataloader = DataLoader(test_data, batch_size=batch_size) device = accelerator.device # 定义模型 class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28 * 28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork().to(device) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) model, train_dataloader, test_dataloader, optimizer = accelerator.prepare( model, train_dataloader, test_dataloader, optimizer ) def train_epoch(dataloader, model, loss_fn, optimizer): size = len(dataloader.dataset) model.train() for batch, (X, y) in enumerate(dataloader): pred = model(X) loss = loss_fn(pred, y) accelerator.backward(loss) optimizer.step() optimizer.zero_grad() if batch % 100 == 0: loss, current = loss.item(), (batch + 1) * len(X) print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]") def test(dataloader, model, loss_fn): size = len(dataloader.dataset) num_batches = len(dataloader) model.eval() test_loss, correct = 0, 0 with torch.no_grad(): for X, y in dataloader: pred = model(X) test_loss += loss_fn(pred, y).item() correct += (pred.argmax(1) == y).type(torch.float).sum().item() test_loss /= num_batches correct /= size print(f"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n") epochs = 5 for t in range(epochs): print(f"Epoch {t+1}\\n-------------------------------") train_epoch(train_dataloader, model, loss_fn, optimizer) test(test_dataloader, model, loss_fn) print("Done!") cleanup() if __name__ == "__main__": world_size = torch.cuda.device_count() torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True) 在上面的代码中，我们首先导入必要的库，然后定义了一些辅助函数，如 setup 和 cleanup。接下来，我们定义了 train 函数，该函数用于训练模型。在 train 函数中，我们首先调用 setup 函数来初始化进程组，然后创建一个 Accelerator 对象，该对象用于加速训练过程。接着，我们下载了 FashionMNIST 数据集，并创建了训练和测试数据加载器。然后，我们定义了一个简单的神经网络模型，并将其移动到 GPU 上。接着，我们定义了损失函数和优化器，并调用 accelerator.prepare 函数来准备模型、数据加载器和优化器。最后，我们定义了 train_epoch 和 test 函数，用于训练和测试模型。最后，我们使用 torch.multiprocessing.spawn 函数来启动多个进程，以便在多个 GPU 上并行训练模型。\n'><title>Mnist</title><link rel=canonical href=https://tannal.github.io/zh-cn/p/mnist/><link rel=stylesheet href=/scss/style.min.946cca6c6259ef94ac55abfae7c7bf3291ea3ed5eea17ef77500b257217c6710.css><meta property='og:title' content="Mnist"><meta property='og:description' content='多机多卡训练 在本教程中，我们将使用 PyTorch 和 Accelerate 库来训练一个简单的神经网络模型，该模型用于对 FashionMNIST 数据集中的服装图像进行分类。我们将使用多机多卡训练模型，以便在多个 GPU 上并行训练模型。\nimport os import torch import torch.distributed as dist from torch import nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor from accelerate import Accelerator def setup(rank, world_size): os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39; os.environ[&#39;MASTER_PORT&#39;] = &#39;12355&#39; dist.init_process_group("nccl", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def train(rank, world_size): setup(rank, world_size) accelerator = Accelerator() # 下载训练数据 training_data = datasets.FashionMNIST( root="data", train=True, download=True, transform=ToTensor(), ) # 下载测试数据 test_data = datasets.FashionMNIST( root="data", train=False, download=True, transform=ToTensor(), ) batch_size = 32 train_dataloader = DataLoader(training_data, batch_size=batch_size) test_dataloader = DataLoader(test_data, batch_size=batch_size) device = accelerator.device # 定义模型 class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28 * 28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork().to(device) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) model, train_dataloader, test_dataloader, optimizer = accelerator.prepare( model, train_dataloader, test_dataloader, optimizer ) def train_epoch(dataloader, model, loss_fn, optimizer): size = len(dataloader.dataset) model.train() for batch, (X, y) in enumerate(dataloader): pred = model(X) loss = loss_fn(pred, y) accelerator.backward(loss) optimizer.step() optimizer.zero_grad() if batch % 100 == 0: loss, current = loss.item(), (batch + 1) * len(X) print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]") def test(dataloader, model, loss_fn): size = len(dataloader.dataset) num_batches = len(dataloader) model.eval() test_loss, correct = 0, 0 with torch.no_grad(): for X, y in dataloader: pred = model(X) test_loss += loss_fn(pred, y).item() correct += (pred.argmax(1) == y).type(torch.float).sum().item() test_loss /= num_batches correct /= size print(f"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n") epochs = 5 for t in range(epochs): print(f"Epoch {t+1}\\n-------------------------------") train_epoch(train_dataloader, model, loss_fn, optimizer) test(test_dataloader, model, loss_fn) print("Done!") cleanup() if __name__ == "__main__": world_size = torch.cuda.device_count() torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True) 在上面的代码中，我们首先导入必要的库，然后定义了一些辅助函数，如 setup 和 cleanup。接下来，我们定义了 train 函数，该函数用于训练模型。在 train 函数中，我们首先调用 setup 函数来初始化进程组，然后创建一个 Accelerator 对象，该对象用于加速训练过程。接着，我们下载了 FashionMNIST 数据集，并创建了训练和测试数据加载器。然后，我们定义了一个简单的神经网络模型，并将其移动到 GPU 上。接着，我们定义了损失函数和优化器，并调用 accelerator.prepare 函数来准备模型、数据加载器和优化器。最后，我们定义了 train_epoch 和 test 函数，用于训练和测试模型。最后，我们使用 torch.multiprocessing.spawn 函数来启动多个进程，以便在多个 GPU 上并行训练模型。\n'><meta property='og:url' content='https://tannal.github.io/zh-cn/p/mnist/'><meta property='og:site_name' content='谭盟'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='神经网络'><meta property='article:published_time' content='2024-10-10T10:51:56+08:00'><meta property='article:modified_time' content='2024-10-10T10:51:56+08:00'><meta name=twitter:title content="Mnist"><meta name=twitter:description content='多机多卡训练 在本教程中，我们将使用 PyTorch 和 Accelerate 库来训练一个简单的神经网络模型，该模型用于对 FashionMNIST 数据集中的服装图像进行分类。我们将使用多机多卡训练模型，以便在多个 GPU 上并行训练模型。\nimport os import torch import torch.distributed as dist from torch import nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor from accelerate import Accelerator def setup(rank, world_size): os.environ[&#39;MASTER_ADDR&#39;] = &#39;localhost&#39; os.environ[&#39;MASTER_PORT&#39;] = &#39;12355&#39; dist.init_process_group("nccl", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def train(rank, world_size): setup(rank, world_size) accelerator = Accelerator() # 下载训练数据 training_data = datasets.FashionMNIST( root="data", train=True, download=True, transform=ToTensor(), ) # 下载测试数据 test_data = datasets.FashionMNIST( root="data", train=False, download=True, transform=ToTensor(), ) batch_size = 32 train_dataloader = DataLoader(training_data, batch_size=batch_size) test_dataloader = DataLoader(test_data, batch_size=batch_size) device = accelerator.device # 定义模型 class NeuralNetwork(nn.Module): def __init__(self): super().__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28 * 28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits model = NeuralNetwork().to(device) loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) model, train_dataloader, test_dataloader, optimizer = accelerator.prepare( model, train_dataloader, test_dataloader, optimizer ) def train_epoch(dataloader, model, loss_fn, optimizer): size = len(dataloader.dataset) model.train() for batch, (X, y) in enumerate(dataloader): pred = model(X) loss = loss_fn(pred, y) accelerator.backward(loss) optimizer.step() optimizer.zero_grad() if batch % 100 == 0: loss, current = loss.item(), (batch + 1) * len(X) print(f"loss: {loss:>7f} [{current:>5d}/{size:>5d}]") def test(dataloader, model, loss_fn): size = len(dataloader.dataset) num_batches = len(dataloader) model.eval() test_loss, correct = 0, 0 with torch.no_grad(): for X, y in dataloader: pred = model(X) test_loss += loss_fn(pred, y).item() correct += (pred.argmax(1) == y).type(torch.float).sum().item() test_loss /= num_batches correct /= size print(f"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n") epochs = 5 for t in range(epochs): print(f"Epoch {t+1}\\n-------------------------------") train_epoch(train_dataloader, model, loss_fn, optimizer) test(test_dataloader, model, loss_fn) print("Done!") cleanup() if __name__ == "__main__": world_size = torch.cuda.device_count() torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True) 在上面的代码中，我们首先导入必要的库，然后定义了一些辅助函数，如 setup 和 cleanup。接下来，我们定义了 train 函数，该函数用于训练模型。在 train 函数中，我们首先调用 setup 函数来初始化进程组，然后创建一个 Accelerator 对象，该对象用于加速训练过程。接着，我们下载了 FashionMNIST 数据集，并创建了训练和测试数据加载器。然后，我们定义了一个简单的神经网络模型，并将其移动到 GPU 上。接着，我们定义了损失函数和优化器，并调用 accelerator.prepare 函数来准备模型、数据加载器和优化器。最后，我们定义了 train_epoch 和 test 函数，用于训练和测试模型。最后，我们使用 torch.multiprocessing.spawn 函数来启动多个进程，以便在多个 GPU 上并行训练模型。\n'><link rel="shortcut icon" href=/favicon.ico></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/zh-cn/><img src=/img/avatar_hu_23460dfa2f6975f3.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🤟</span></figure><div class=site-meta><h1 class=site-name><a href=/zh-cn>谭盟</a></h1><h2 class=site-description>stay hungry, stay foolish</h2></div></header><ol class=menu-social><li><a href=https://space.bilibili.com/3546751254399476 target=_blank title="B 站「起个名字叫犟驴」" rel=me><!doctype html><svg t="1712105268862" class="icon" viewBox="0 0 1024 1024" p-id="5725" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M1019.54782609 345.3106087c-3.20556522-142.1133913-127.15408696-169.36069565-127.15408696-169.36069566s-96.70121739-.53426087-222.25252174-1.60278261l91.3586087-88.15304347s14.42504348-18.16486957-10.15095652-38.46678261c-24.576-20.30191304-26.17878261-11.21947826-34.72695653-5.87686957-7.47965217 5.3426087-117.00313043 112.72904348-136.23652174 131.96243479-49.68626087.0-101.50956522-.53426087-151.73008695-.53426087h17.63060869S315.392 43.98747826 306.84382609 38.1106087s-9.61669565-14.42504348-34.72695652 5.87686956c-24.576 20.30191304-10.15095652 38.46678261-10.15095653 38.46678261l93.49565218 90.82434783c-101.50956522.0-189.12834783.53426087-229.73217392 2.13704347C-5.69878261 213.34817391 4.45217391 345.3106087 4.45217391 345.3106087s1.60278261 283.15826087.0 426.34017391c14.42504348 143.18191304 124.48278261 166.15513043 124.48278261 166.15513043s43.8093913 1.06852174 76.39930435 1.06852174c3.20556522 9.08243478 5.87686957 53.96034783 56.0973913 53.96034783 49.68626087.0 56.0973913-53.96034783 56.09739131-53.96034783s365.96869565-1.60278261 396.42156522-1.60278261c1.60278261 15.49356522 9.08243478 56.63165217 59.30295652 56.09739131 49.68626087-1.06852174 53.42608696-59.30295652 53.42608695-59.30295652s17.09634783-1.60278261 67.85113044.0c118.60591304-21.90469565 125.55130435-160.81252174 125.55130435-160.81252174s-2.13704348-285.82956522-.53426087-427.94295652zM917.504 798.36382609c0 22.43895652-17.6306087 40.60382609-39.53530435 40.60382608H156.71652174c-21.90469565.0-39.53530435-18.16486957-39.53530435-40.60382608V320.20034783c0-22.43895652 17.6306087-40.60382609 39.53530435-40.60382609h721.25217391c21.90469565.0 39.53530435 18.16486957 39.53530435 40.60382609v478.16347826z" fill="#8a8a8a" p-id="5726"/><path d="M409.088 418.816l-203.264 38.912 17.408 76.288 201.216-38.912zm109.568 202.24c-49.664 106.496-94.208 26.112-94.208 26.112l-33.28 21.504s65.536 89.6 128 21.504c73.728 68.096 130.048-22.016 130.048-22.016l-30.208-19.456c0-.512-52.736 75.776-100.352-27.648zM619.008 495.104l201.728 38.912 16.896-76.288-202.752-38.912z" fill="#8a8a8a" p-id="5727"/></svg></a></li><li><a href=https://github.com/tannal target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com/megotannal target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li><li><a href=https://www.zhihu.com/people/roseduan target=_blank title=Zhihu rel=me><!doctype html><svg t="1704259577746" class="icon" viewBox="0 0 1024 1024" p-id="5040" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M570.581333 806.272h61.952L652.928 876.117333 764.074667 806.272h130.986666V230.186667h-324.48V806.272zM636.501333 292.693333h192.64V743.68h-73.898666l-73.813334 46.378667L668.032 743.808l-31.530667-.128V292.736zM515.754667 493.738667H377.429333a2999.466667 2999.466667.0 005.802667-194.56h135.338667S523.776 239.445334 495.872 240.128H261.76c9.216-34.730667 20.821333-70.613333 34.688-107.690667.0.0-63.701333.0-85.333333 57.130667C202.112 213.12 176.128 303.786667 129.877333 396.416c15.573333-1.706667 67.114667-3.114667 97.450667-58.794667 5.589333-15.616 6.656-17.621333 13.568-38.485333h76.373333c0 27.776-3.157333 177.109333-4.437333 194.474667h-138.24c-31.104.0-41.173333 62.549333-41.173333 62.549333h173.482666C295.253333 688.256 232.789333 799.573333 119.466667 887.466667c54.186667 15.488 108.202667-2.432 134.912-26.197334.0.0 60.8-55.338667 94.122666-183.381333L491.264 849.834667s20.906667-71.168-3.285333-105.856c-20.053333-23.637333-74.24-87.552-97.322667-110.72l-38.698667 30.72c11.52-36.992 18.474667-72.96 20.821334-107.690667h163.072s-.213333-62.549333-20.053334-62.549333z" p-id="5041" fill="#8a8a8a"/></svg></a></li><li><a href=https://mp.weixin.qq.com/s/hs36eiU7Zr-UdJ0YBtZgKQ target=_blank title="微信公众号 roseduan写字的地方" rel=me><!doctype html><svg t="1704259948565" class="icon" viewBox="0 0 1194 1024" p-id="5272" xmlns:xlink="http://www.w3.org/1999/xlink" width="233.203125" height="200"><path d="M728.064 535.296a35.498667 35.498667.0 11-70.912.0 35.498667 35.498667.0 0170.912.0m246.016.0a35.498667 35.498667.0 11-70.997333.0 35.498667 35.498667.0 0170.997333.0" fill="#8a8a8a" p-id="5273"/><path d="M902.144 930.133333l-6.656 1.450667a594.176 594.176.0 01-18.602667 3.669333c-26.453333 4.778667-44.629333 6.826667-64 6.144C645.034666 935.850666 514.218666 853.504 461.824 722.688a446.464 446.464.0 01-6.826667-19.114667l-1.962666-6.058666a199.68 199.68.0 01-3.84-13.653334 338.858667 338.858667.0 01-9.216-75.690666c0-171.008 157.013333-305.92 354.304-314.709334l8.874666-.682666c7.850667-.597333 12.970667-.853333 18.773334-.853334h14.762666l9.301334.170667c188.16 14.677333 340.394667 156.672 340.394666 323.925333.0 78.336-38.826667 155.733333-109.653333 222.293334a303.530667 303.530667.0 01-7.082667 6.4l9.984 71.082666a49.152 49.152.0 01-69.12 58.453334l-98.816-46.421334a582.485333 582.485333.0 01-9.472 2.304zm220.16-314.026666c0-131.754667-124.586667-247.978667-279.125333-260.096H821.930667c-3.754667.0-7.68.085333-13.994667.597333l-10.666667.768C631.466667 364.8 503.978667 474.453333 503.978667 608.256c0 18.773333 2.730667 40.874667 7.509333 60.842667.597333 2.474667 1.450667 5.632 2.56 9.216l1.706667 4.949333c2.133333 6.570667 4.266667 12.629333 5.632 15.957333 42.325333 105.728 149.930667 173.397333 293.717333 178.176 13.738667.512 28.16-1.109333 50.346667-5.12a531.626667 531.626667.0 0016.64-3.242666l5.632-1.28c4.693333-1.109333 8.448-1.962667 19.370666-4.778667a32 32 0 0121.418667 2.133333L1013.76 905.216l-9.984-69.290667a32 32 0 0113.909333-31.232c1.194667-.853333 8.618667-6.912 15.189334-13.056 58.709333-55.210667 89.429333-116.394667 89.429333-175.616zM12.970667 378.197333C12.970667 171.52 206.677333 5.376 442.709333 5.376c208.384.0 394.24 130.304 431.872 306.090667 1.28 5.973333 1.28 11.264.512 16.896a32 32 0 01-63.658666-5.973334C779.178667 179.2 621.482667 69.376 442.709333 69.376c-202.666666.0-365.738666 139.776-365.738666 308.906667.0 89.941333 46.165333 171.52 136.021333 237.568a32 32 0 0111.434667 35.84l-23.296 69.973333 104.96-48.896a32 32 0 0122.442666-1.706667l5.461334 1.706667c54.357333 11.093333 76.288 14.506667 108.714666 14.506667a200.789333 200.789333.0 0032.256-4.010667c-.512.170667-1.536.512-2.730666 1.536a33.109333 33.109333.0 0139.253333 1.109333A32 32 0 01516.864 730.88c-11.776 14.848-47.36 20.309333-74.24 20.309333-37.12.0-62.037333-3.584-119.893333-15.530666L198.570667 793.6a49.493333 49.493333.0 01-68.949334-59.733333l26.88-80.64c-93.525333-75.52-143.530666-170.24-143.530666-275.029334z" fill="#8a8a8a" p-id="5274"/><path d="M344.576 254.378667a44.373333 44.373333.0 11-88.746667.085333 44.373333 44.373333.0 0188.746667.0M636.586667 254.378667A44.373333 44.373333.0 11547.84 254.464a44.373333 44.373333.0 0188.746667.0" fill="#8a8a8a" p-id="5275"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/zh-cn/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/zh-cn/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>归档</span></a></li><li><a href=/zh-cn/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>搜索</span></a></li><li><a href=/zh-cn/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M8 7a4 4 0 108 0A4 4 0 008 7"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://tannal.github.io/>English</option><option value=https://tannal.github.io/zh-cn/ selected>Chinese</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/zh-cn/categories/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>大语言模型</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/zh-cn/p/mnist/>Mnist</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Oct 10, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 2 分钟</time></div></footer></div></header><section class=article-content><h1 id=多机多卡训练>多机多卡训练</h1><p>在本教程中，我们将使用 PyTorch 和 Accelerate 库来训练一个简单的神经网络模型，该模型用于对 FashionMNIST 数据集中的服装图像进行分类。我们将使用多机多卡训练模型，以便在多个 GPU 上并行训练模型。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>
</span></span><span class=line><span class=cl>import os
</span></span><span class=line><span class=cl>import torch
</span></span><span class=line><span class=cl>import torch.distributed as dist
</span></span><span class=line><span class=cl>from torch import nn
</span></span><span class=line><span class=cl>from torch.utils.data import DataLoader
</span></span><span class=line><span class=cl>from torchvision import datasets
</span></span><span class=line><span class=cl>from torchvision.transforms import ToTensor
</span></span><span class=line><span class=cl>from accelerate import Accelerator
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>def setup<span class=o>(</span>rank, world_size<span class=o>)</span>:
</span></span><span class=line><span class=cl>    os.environ<span class=o>[</span><span class=s1>&#39;MASTER_ADDR&#39;</span><span class=o>]</span> <span class=o>=</span> <span class=s1>&#39;localhost&#39;</span>
</span></span><span class=line><span class=cl>    os.environ<span class=o>[</span><span class=s1>&#39;MASTER_PORT&#39;</span><span class=o>]</span> <span class=o>=</span> <span class=s1>&#39;12355&#39;</span>
</span></span><span class=line><span class=cl>    dist.init_process_group<span class=o>(</span><span class=s2>&#34;nccl&#34;</span>, <span class=nv>rank</span><span class=o>=</span>rank, <span class=nv>world_size</span><span class=o>=</span>world_size<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>def cleanup<span class=o>()</span>:
</span></span><span class=line><span class=cl>    dist.destroy_process_group<span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>def train<span class=o>(</span>rank, world_size<span class=o>)</span>:
</span></span><span class=line><span class=cl>    setup<span class=o>(</span>rank, world_size<span class=o>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=nv>accelerator</span> <span class=o>=</span> Accelerator<span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 下载训练数据</span>
</span></span><span class=line><span class=cl>    <span class=nv>training_data</span> <span class=o>=</span> datasets.FashionMNIST<span class=o>(</span>
</span></span><span class=line><span class=cl>        <span class=nv>root</span><span class=o>=</span><span class=s2>&#34;data&#34;</span>,
</span></span><span class=line><span class=cl>        <span class=nv>train</span><span class=o>=</span>True,
</span></span><span class=line><span class=cl>        <span class=nv>download</span><span class=o>=</span>True,
</span></span><span class=line><span class=cl>        <span class=nv>transform</span><span class=o>=</span>ToTensor<span class=o>()</span>,
</span></span><span class=line><span class=cl>    <span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 下载测试数据</span>
</span></span><span class=line><span class=cl>    <span class=nv>test_data</span> <span class=o>=</span> datasets.FashionMNIST<span class=o>(</span>
</span></span><span class=line><span class=cl>        <span class=nv>root</span><span class=o>=</span><span class=s2>&#34;data&#34;</span>,
</span></span><span class=line><span class=cl>        <span class=nv>train</span><span class=o>=</span>False,
</span></span><span class=line><span class=cl>        <span class=nv>download</span><span class=o>=</span>True,
</span></span><span class=line><span class=cl>        <span class=nv>transform</span><span class=o>=</span>ToTensor<span class=o>()</span>,
</span></span><span class=line><span class=cl>    <span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nv>batch_size</span> <span class=o>=</span> <span class=m>32</span>
</span></span><span class=line><span class=cl>    <span class=nv>train_dataloader</span> <span class=o>=</span> DataLoader<span class=o>(</span>training_data, <span class=nv>batch_size</span><span class=o>=</span>batch_size<span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=nv>test_dataloader</span> <span class=o>=</span> DataLoader<span class=o>(</span>test_data, <span class=nv>batch_size</span><span class=o>=</span>batch_size<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nv>device</span> <span class=o>=</span> accelerator.device
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 定义模型</span>
</span></span><span class=line><span class=cl>    class NeuralNetwork<span class=o>(</span>nn.Module<span class=o>)</span>:
</span></span><span class=line><span class=cl>        def __init__<span class=o>(</span>self<span class=o>)</span>:
</span></span><span class=line><span class=cl>            super<span class=o>()</span>.__init__<span class=o>()</span>
</span></span><span class=line><span class=cl>            self.flatten <span class=o>=</span> nn.Flatten<span class=o>()</span>
</span></span><span class=line><span class=cl>            self.linear_relu_stack <span class=o>=</span> nn.Sequential<span class=o>(</span>
</span></span><span class=line><span class=cl>                nn.Linear<span class=o>(</span><span class=m>28</span> * 28, 512<span class=o>)</span>,
</span></span><span class=line><span class=cl>                nn.ReLU<span class=o>()</span>,
</span></span><span class=line><span class=cl>                nn.Linear<span class=o>(</span>512, 512<span class=o>)</span>,
</span></span><span class=line><span class=cl>                nn.ReLU<span class=o>()</span>,
</span></span><span class=line><span class=cl>                nn.Linear<span class=o>(</span>512, 10<span class=o>)</span>,
</span></span><span class=line><span class=cl>            <span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        def forward<span class=o>(</span>self, x<span class=o>)</span>:
</span></span><span class=line><span class=cl>            <span class=nv>x</span> <span class=o>=</span> self.flatten<span class=o>(</span>x<span class=o>)</span>
</span></span><span class=line><span class=cl>            <span class=nv>logits</span> <span class=o>=</span> self.linear_relu_stack<span class=o>(</span>x<span class=o>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> logits
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nv>model</span> <span class=o>=</span> NeuralNetwork<span class=o>()</span>.to<span class=o>(</span>device<span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=nv>loss_fn</span> <span class=o>=</span> nn.CrossEntropyLoss<span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=nv>optimizer</span> <span class=o>=</span> torch.optim.SGD<span class=o>(</span>model.parameters<span class=o>()</span>, <span class=nv>lr</span><span class=o>=</span>1e-2<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    model, train_dataloader, test_dataloader, <span class=nv>optimizer</span> <span class=o>=</span> accelerator.prepare<span class=o>(</span>
</span></span><span class=line><span class=cl>        model, train_dataloader, test_dataloader, optimizer
</span></span><span class=line><span class=cl>    <span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    def train_epoch<span class=o>(</span>dataloader, model, loss_fn, optimizer<span class=o>)</span>:
</span></span><span class=line><span class=cl>        <span class=nv>size</span> <span class=o>=</span> len<span class=o>(</span>dataloader.dataset<span class=o>)</span>
</span></span><span class=line><span class=cl>        model.train<span class=o>()</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> batch, <span class=o>(</span>X, y<span class=o>)</span> in enumerate<span class=o>(</span>dataloader<span class=o>)</span>:
</span></span><span class=line><span class=cl>            <span class=nv>pred</span> <span class=o>=</span> model<span class=o>(</span>X<span class=o>)</span>
</span></span><span class=line><span class=cl>            <span class=nv>loss</span> <span class=o>=</span> loss_fn<span class=o>(</span>pred, y<span class=o>)</span>
</span></span><span class=line><span class=cl>            accelerator.backward<span class=o>(</span>loss<span class=o>)</span>
</span></span><span class=line><span class=cl>            optimizer.step<span class=o>()</span>
</span></span><span class=line><span class=cl>            optimizer.zero_grad<span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> batch % <span class=nv>100</span> <span class=o>==</span> 0:
</span></span><span class=line><span class=cl>                loss, <span class=nv>current</span> <span class=o>=</span> loss.item<span class=o>()</span>, <span class=o>(</span>batch + 1<span class=o>)</span> * len<span class=o>(</span>X<span class=o>)</span>
</span></span><span class=line><span class=cl>                print<span class=o>(</span>f<span class=s2>&#34;loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    def test<span class=o>(</span>dataloader, model, loss_fn<span class=o>)</span>:
</span></span><span class=line><span class=cl>        <span class=nv>size</span> <span class=o>=</span> len<span class=o>(</span>dataloader.dataset<span class=o>)</span>
</span></span><span class=line><span class=cl>        <span class=nv>num_batches</span> <span class=o>=</span> len<span class=o>(</span>dataloader<span class=o>)</span>
</span></span><span class=line><span class=cl>        model.eval<span class=o>()</span>
</span></span><span class=line><span class=cl>        test_loss, <span class=nv>correct</span> <span class=o>=</span> 0, <span class=m>0</span>
</span></span><span class=line><span class=cl>        with torch.no_grad<span class=o>()</span>:
</span></span><span class=line><span class=cl>            <span class=k>for</span> X, y in dataloader:
</span></span><span class=line><span class=cl>                <span class=nv>pred</span> <span class=o>=</span> model<span class=o>(</span>X<span class=o>)</span>
</span></span><span class=line><span class=cl>                <span class=nv>test_loss</span> <span class=o>+=</span> loss_fn<span class=o>(</span>pred, y<span class=o>)</span>.item<span class=o>()</span>
</span></span><span class=line><span class=cl>                <span class=nv>correct</span> <span class=o>+=</span> <span class=o>(</span>pred.argmax<span class=o>(</span>1<span class=o>)</span> <span class=o>==</span> y<span class=o>)</span>.type<span class=o>(</span>torch.float<span class=o>)</span>.sum<span class=o>()</span>.item<span class=o>()</span>
</span></span><span class=line><span class=cl>        test_loss /<span class=o>=</span> num_batches
</span></span><span class=line><span class=cl>        correct /<span class=o>=</span> size
</span></span><span class=line><span class=cl>        print<span class=o>(</span>f<span class=s2>&#34;Test Error: \n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \n&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nv>epochs</span> <span class=o>=</span> <span class=m>5</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> t in range<span class=o>(</span>epochs<span class=o>)</span>:
</span></span><span class=line><span class=cl>        print<span class=o>(</span>f<span class=s2>&#34;Epoch {t+1}\n-------------------------------&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>        train_epoch<span class=o>(</span>train_dataloader, model, loss_fn, optimizer<span class=o>)</span>
</span></span><span class=line><span class=cl>        test<span class=o>(</span>test_dataloader, model, loss_fn<span class=o>)</span>
</span></span><span class=line><span class=cl>    print<span class=o>(</span><span class=s2>&#34;Done!&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    cleanup<span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=nv>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span>:
</span></span><span class=line><span class=cl>    <span class=nv>world_size</span> <span class=o>=</span> torch.cuda.device_count<span class=o>()</span>
</span></span><span class=line><span class=cl>    torch.multiprocessing.spawn<span class=o>(</span>train, <span class=nv>args</span><span class=o>=(</span>world_size,<span class=o>)</span>, <span class=nv>nprocs</span><span class=o>=</span>world_size, <span class=nv>join</span><span class=o>=</span>True<span class=o>)</span>
</span></span></code></pre></div><p>在上面的代码中，我们首先导入必要的库，然后定义了一些辅助函数，如 <code>setup</code> 和 <code>cleanup</code>。接下来，我们定义了 <code>train</code> 函数，该函数用于训练模型。在 <code>train</code> 函数中，我们首先调用 <code>setup</code> 函数来初始化进程组，然后创建一个 <code>Accelerator</code> 对象，该对象用于加速训练过程。接着，我们下载了 FashionMNIST 数据集，并创建了训练和测试数据加载器。然后，我们定义了一个简单的神经网络模型，并将其移动到 GPU 上。接着，我们定义了损失函数和优化器，并调用 <code>accelerator.prepare</code> 函数来准备模型、数据加载器和优化器。最后，我们定义了 <code>train_epoch</code> 和 <code>test</code> 函数，用于训练和测试模型。最后，我们使用 <code>torch.multiprocessing.spawn</code> 函数来启动多个进程，以便在多个 GPU 上并行训练模型。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>
</span></span><span class=line><span class=cl>conda install pytorch torchvision torchaudio pytorch-cuda<span class=o>=</span>12.1 -c pytorch -c nvidia
</span></span><span class=line><span class=cl>pip install accelerate
</span></span></code></pre></div></section><footer class=article-footer><section class=article-tags><a href=/zh-cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/zh-cn/p/llm.c-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/><div class=article-details><h2 class=article-title>LLM.C 人工智能训练和推理的最佳实践</h2></div></a></article><article><a href=/zh-cn/p/lanntai-ai-for-human/><div class=article-details><h2 class=article-title>lanntai AI for human</h2></div></a></article></div></div></aside><div id=gitalk-container></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css><script src=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js></script><script src=https://cdn.jsdelivr.net/npm/blueimp-md5@2.18.0/js/md5.min.js></script><script>const gitalk=new Gitalk({clientID:"Ov23liqc7SD1bN7OvCI6",clientSecret:"7b0ed5cd67a344de02c2efded9da2dfb8ac14783",repo:"tannal.github.io",owner:"tannal",admin:["tannal"],distractionFreeMode:!1,id:md5(location.pathname),proxy:null});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("gitalk-container").innerHTML="Gitalk comments not available by default when the website is previewed locally.";return}gitalk.render("gitalk-container")})()</script><footer class=site-footer><section class=copyright>&copy;
2024 -
2025 谭盟</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>