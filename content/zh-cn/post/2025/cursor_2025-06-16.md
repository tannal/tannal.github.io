---
title: 2025年6月16日
date: 2025-06-16 00:00:00+0000
categories:
    - 神经网络
tags:
    - 计算机
    - 逻辑
---


# How to perform inference after download
_Exported on 6/17/2025 at 06:40:00 GMT+8 from Cursor (1.0.0)_

---

**User**

huggingface-cli download runwayml/stable-diffusion-v1-5 --local-dir ./models/sd-v1-5

下载完权重后如何推理？

---

**Cursor**

下载完模型权重后，你可以使用 `diffusers` 库进行推理。

首先，请确保你已经安装了 `diffusers`、`transformers` 和 `accelerate` 库。如果你还没有安装，可以使用以下命令：

```bash
pip install diffusers transformers accelerate torch torchvision
```

然后，你可以使用以下 Python 代码来加载模型并进行推理：

```python
from diffusers import StableDiffusionPipeline
import torch

# 指定你下载模型权重的本地目录
model_path = "./models/sd-v1-5"

# 检查是否有可用的CUDA设备 (GPU)
if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
    print("CUDA (GPU) is not available. Running on CPU, which will be much slower.")

# 从本地目录加载 Stable Diffusion 管线
# 注意：如果你的模型下载不完整或者路径不对，这里可能会报错
# 如果你遇到了内存不足的问题，可以尝试使用 torch.float16 来加载模型 (requires GPU)
try:
    if device == "cuda":
        pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)
    else:
        pipe = StableDiffusionPipeline.from_pretrained(model_path)
    pipe.to(device)
    print(f"Model loaded successfully on {device}.")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Please ensure the model path is correct and the weights are fully downloaded.")
    exit()

# 定义你的文本提示
prompt = "a photo of an astronaut riding a horse on mars"

print(f"Generating image for prompt: '{prompt}'...")

# 进行推理，生成图像
# num_inference_steps 可以调整，步数越多通常图像质量越好，但推理时间也越长
# guidance_scale 越大，图像越符合提示，但可能牺牲多样性
try:
    image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]

    # 保存生成的图像
    image.save("astronaut_on_mars.png")
    print("Image saved as 'astronaut_on_mars.png'")
except Exception as e:
    print(f"Error during image generation: {e}")
    print("This might be due to insufficient memory or other runtime issues.")

```

**代码解释：**

1.  **`model_path = "./models/sd-v1-5"`**: 这里需要设置为你之前使用 `huggingface-cli` 下载模型时指定的 `--local-dir` 路径。
2.  **`pipe = StableDiffusionPipeline.from_pretrained(model_path, ...)`**: 这行代码会从你指定的本地路径加载 Stable Diffusion 模型。
    *   `torch_dtype=torch.float16`：如果你有 GPU 并且显存有限，可以使用半精度浮点数来加载模型，这会减少内存占用，但需要 GPU 支持。
3.  **`pipe.to(device)`**: 将模型加载到你的设备上（GPU 或 CPU）。GPU 会快很多。
4.  **`prompt = "..."`**: 定义你想要生成图像的文本描述。
5.  **`image = pipe(prompt, ...).images[0]`**: 执行推理过程。
    *   `num_inference_steps`: 扩散过程的步数。默认是 50，步数越多通常细节越好，但推理时间越长。
    *   `guidance_scale`: 引导比例。这是一个超参数，控制文本提示对生成图像的影响程度。较高的值会使图像更忠实于提示，但可能会牺牲多样性。
6.  **`image.save("astronaut_on_mars.png")`**: 将生成的图像保存为 PNG 文件。

确保你的 `models/sd-v1-5` 目录结构与 `huggingface-cli` 下载后的一致，包含 `tokenizer`、`text_encoder`、`unet` 等子目录和相关文件。

```shellscript
pip install diffusers transformers accelerate torch torchvision
```

```python
from diffusers import StableDiffusionPipeline
import torch

# 指定你下载模型权重的本地目录
model_path = "./models/sd-v1-5"

# 检查是否有可用的CUDA设备 (GPU)
if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
    print("CUDA (GPU) is not available. Running on CPU, which will be much slower.")

# 从本地目录加载 Stable Diffusion 管线
# 注意：如果你的模型下载不完整或者路径不对，这里可能会报错
# 如果你遇到了内存不足的问题，可以尝试使用 torch.float16 来加载模型 (requires GPU)
try:
    if device == "cuda":
        pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)
    else:
        pipe = StableDiffusionPipeline.from_pretrained(model_path)
    pipe.to(device)
    print(f"Model loaded successfully on {device}.")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Please ensure the model path is correct and the weights are fully downloaded.")
    exit()

# 定义你的文本提示
prompt = "a photo of an astronaut riding a horse on mars"

print(f"Generating image for prompt: '{prompt}'...")

# 进行推理，生成图像
# num_inference_steps 可以调整，步数越多通常图像质量越好，但推理时间也越长
# guidance_scale 越大，图像越符合提示，但可能牺牲多样性
try:
    image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]

    # 保存生成的图像
    image.save("astronaut_on_mars.png")
    print("Image saved as 'astronaut_on_mars.png'")
except Exception as e:
    print(f"Error during image generation: {e}")
    print("This might be due to insufficient memory or other runtime issues.")

```

---

**User**

diffusers 是什么？

---

**Cursor**

`diffusers` 是 Hugging Face 公司开发的一个流行的 Python 库，专门用于 **扩散模型（Diffusion Models）**。

简而言之，`diffusers` 库的目的是：

1.  **简化扩散模型的使用（推理）**：它提供了大量预训练的扩散模型（包括 Stable Diffusion, DALL-E 2, latent diffusion 等），并封装了易于使用的推理管线（`Pipeline`），让用户只需几行代码就能根据文本提示生成图像。
2.  **简化扩散模型的训练和微调**：它提供了一套模块化的组件（如 U-Net 模型、调度器、编码器等），以及用于训练扩散模型的工具和脚本，方便研究人员和开发者进行模型的训练和实验。

**主要特点和作用：**

*   **开箱即用（Out-of-the-box）的管线（Pipelines）**：这是 `diffusers` 最受欢迎的功能之一。它将生成图像所需的所有复杂步骤（如文本编码、U-Net 前向传播、调度器采样等）封装在一个简单的 API 调用中。例如，你只需要提供一个文本提示，管线就能返回生成的图像。
*   **丰富的模型库**：它集成了 Hugging Face Hub 上大量的预训练扩散模型，用户可以轻松加载和使用这些模型。
*   **模块化设计**：扩散模型由多个组件组成（如 U-Net 模型用于去噪，调度器用于噪声采样，文本编码器用于处理文本提示等）。`diffusers` 库将这些组件进行了模块化，方便用户自定义和组合。
*   **支持多种调度器（Schedulers）**：调度器是扩散模型中控制去噪过程的关键部分，不同的调度器会影响生成图像的速度和质量。`diffusers` 提供了多种调度器供选择。
*   **优化性能**：它包含了各种性能优化，如半精度浮点运算（`torch_dtype=torch.float16`）支持，以减少显存占用和加速推理。
*   **活跃的社区支持**：作为 Hugging Face 生态系统的一部分，`diffusers` 拥有庞大的社区，不断有新的模型和功能被添加进来。

**总结来说，`diffusers` 库的出现极大地降低了使用和开发扩散模型的门槛，使得 Stable Diffusion 这类强大的文本到图像生成模型能够被更广泛的用户和开发者所利用。**

---

**User**

Skip to content
Navigation Menu
Stability-AI
stablediffusion

Type / to search
Code
Issues
248
Pull requests
52
Actions
Projects
Security
Insights
Owner avatar
stablediffusion
Public
Stability-AI/stablediffusion
Go to file
t
Name		
hardmaru
hardmaru
Update modelcard.md
cf1d67a
 · 
2 years ago
assets
Add files via upload
2 years ago
checkpoints
add stable unclip
2 years ago
configs
merge unclip into main
2 years ago
doc
Fix diffusers code snippet
2 years ago
ldm
merge unclip into main
2 years ago
scripts
merge unclip into main
2 years ago
.gitignore
Add standard python gitignore with additions for IDEs and MacOS
3 years ago
LICENSE
release more models
3 years ago
LICENSE-MODEL
release more models
3 years ago
README.md
Update README.md
2 years ago
environment.yaml
release more models
3 years ago
modelcard.md
Update modelcard.md
2 years ago
requirements.txt
Bump gradio from 3.11 to 3.13.2
2 years ago
setup.py
release more models
3 years ago
Repository files navigation
README
MIT license
License
Stable Diffusion Version 2
t2i t2i t2i

This repository contains Stable Diffusion models trained from scratch and will be continuously updated with new checkpoints. The following list provides an overview of all currently available models. More coming soon.

News
March 24, 2023

Stable UnCLIP 2.1

New stable diffusion finetune (Stable unCLIP 2.1, Hugging Face) at 768x768 resolution, based on SD2.1-768. This model allows for image variations and mixing operations as described in Hierarchical Text-Conditional Image Generation with CLIP Latents, and, thanks to its modularity, can be combined with other models such as KARLO. Comes in two variants: Stable unCLIP-L and Stable unCLIP-H, which are conditioned on CLIP ViT-L and ViT-H image embeddings, respectively. Instructions are available here.

A public demo of SD-unCLIP is already available at clipdrop.co/stable-diffusion-reimagine

December 7, 2022

Version 2.1

New stable diffusion model (Stable Diffusion 2.1-v, Hugging Face) at 768x768 resolution and (Stable Diffusion 2.1-base, HuggingFace) at 512x512 resolution, both based on the same number of parameters and architecture as 2.0 and fine-tuned on 2.0, on a less restrictive NSFW filtering of the LAION-5B dataset. Per default, the attention operation of the model is evaluated at full precision when xformers is not installed. To enable fp16 (which can cause numerical instabilities with the vanilla attention module on the v2.1 model) , run your script with ATTN_PRECISION=fp16 python <thescript.py>
November 24, 2022

Version 2.0

New stable diffusion model (Stable Diffusion 2.0-v) at 768x768 resolution. Same number of parameters in the U-Net as 1.5, but uses OpenCLIP-ViT/H as the text encoder and is trained from scratch. SD 2.0-v is a so-called v-prediction model.

The above model is finetuned from SD 2.0-base, which was trained as a standard noise-prediction model on 512x512 images and is also made available.

Added a x4 upscaling latent text-guided diffusion model.

New depth-guided stable diffusion model, finetuned from SD 2.0-base. The model is conditioned on monocular depth estimates inferred via MiDaS and can be used for structure-preserving img2img and shape-conditional synthesis.

d2i

A text-guided inpainting model, finetuned from SD 2.0-base.

We follow the original repository and provide basic inference scripts to sample from the models.

The original Stable Diffusion model was created in a collaboration with CompVis and RunwayML and builds upon the work:

High-Resolution Image Synthesis with Latent Diffusion Models
Robin Rombach*, Andreas Blattmann*, Dominik Lorenz, Patrick Esser, Björn Ommer
CVPR '22 Oral | GitHub | arXiv | Project page

and many others.

Stable Diffusion is a latent text-to-image diffusion model.

Requirements
You can update an existing latent diffusion environment by running

conda install pytorch==1.12.1 torchvision==0.13.1 -c pytorch
pip install transformers==4.19.2 diffusers invisible-watermark
pip install -e .
xformers efficient attention
For more efficiency and speed on GPUs, we highly recommended installing the xformers library.

Tested on A100 with CUDA 11.4. Installation needs a somewhat recent version of nvcc and gcc/g++, obtain those, e.g., via

export CUDA_HOME=/usr/local/cuda-11.4
conda install -c nvidia/label/cuda-11.4.0 cuda-nvcc
conda install -c conda-forge gcc
conda install -c conda-forge gxx_linux-64==9.5.0
Then, run the following (compiling takes up to 30 min).

cd ..
git clone https://github.com/facebookresearch/xformers.git
cd xformers
git submodule update --init --recursive
pip install -r requirements.txt
pip install -e .
cd ../stablediffusion
Upon successful installation, the code will automatically default to memory efficient attention for the self- and cross-attention layers in the U-Net and autoencoder.

General Disclaimer
Stable Diffusion models are general text-to-image diffusion models and therefore mirror biases and (mis-)conceptions that are present in their training data. Although efforts were made to reduce the inclusion of explicit pornographic material, we do not recommend using the provided weights for services or products without additional safety mechanisms and considerations. The weights are research artifacts and should be treated as such. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding model card. The weights are available via the StabilityAI organization at Hugging Face under the CreativeML Open RAIL++-M License.

Stable Diffusion v2
Stable Diffusion v2 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 865M UNet and OpenCLIP ViT-H/14 text encoder for the diffusion model. The SD 2-v model produces 768x768 px outputs.

Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 DDIM sampling steps show the relative improvements of the checkpoints:

sd evaluation results

Text-to-Image
txt2img-stable2 txt2img-stable2

Stable Diffusion 2 is a latent diffusion model conditioned on the penultimate text embeddings of a CLIP ViT-H/14 text encoder. We provide a reference script for sampling.

Reference Sampling Script
This script incorporates an invisible watermarking of the outputs, to help viewers identify the images as machine-generated. We provide the configs for the SD2-v (768px) and SD2-base (512px) model.

First, download the weights for SD2.1-v and SD2.1-base.

To sample from the SD2.1-v model, run the following:

python scripts/txt2img.py --prompt "a professional photograph of an astronaut riding a horse" --ckpt <path/to/768model.ckpt/> --config configs/stable-diffusion/v2-inference-v.yaml --H 768 --W 768  
or try out the Web Demo: Hugging Face Spaces.

To sample from the base model, use

python scripts/txt2img.py --prompt "a professional photograph of an astronaut riding a horse" --ckpt <path/to/model.ckpt/> --config <path/to/config.yaml/>  
By default, this uses the DDIM sampler, and renders images of size 768x768 (which it was trained on) in 50 steps. Empirically, the v-models can be sampled with higher guidance scales.

Note: The inference config for all model versions is designed to be used with EMA-only checkpoints. For this reason use_ema=False is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights.

Enable Intel® Extension for PyTorch* optimizations in Text-to-Image script
If you're planning on running Text-to-Image on Intel® CPU, try to sample an image with TorchScript and Intel® Extension for PyTorch* optimizations. Intel® Extension for PyTorch* extends PyTorch by enabling up-to-date features optimizations for an extra performance boost on Intel® hardware. It can optimize memory layout of the operators to Channel Last memory format, which is generally beneficial for Intel CPUs, take advantage of the most advanced instruction set available on a machine, optimize operators and many more.

Prerequisites

Before running the script, make sure you have all needed libraries installed. (the optimization was checked on Ubuntu 20.04). Install jemalloc, numactl, Intel® OpenMP and Intel® Extension for PyTorch*.

apt-get install numactl libjemalloc-dev
pip install intel-openmp
pip install intel_extension_for_pytorch -f https://software.intel.com/ipex-whl-stable
To sample from the SD2.1-v model with TorchScript+IPEX optimizations, run the following. Remember to specify desired number of instances you want to run the program on (more).

MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000 python -m intel_extension_for_pytorch.cpu.launch --ninstance <number of an instance> --enable_jemalloc scripts/txt2img.py --prompt \"a corgi is playing guitar, oil on canvas\" --ckpt <path/to/768model.ckpt/> --config configs/stable-diffusion/intel/v2-inference-v-fp32.yaml  --H 768 --W 768 --precision full --device cpu --torchscript --ipex
To sample from the base model with IPEX optimizations, use

MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000 python -m intel_extension_for_pytorch.cpu.launch --ninstance <number of an instance> --enable_jemalloc scripts/txt2img.py --prompt \"a corgi is playing guitar, oil on canvas\" --ckpt <path/to/model.ckpt/> --config configs/stable-diffusion/intel/v2-inference-fp32.yaml  --n_samples 1 --n_iter 4 --precision full --device cpu --torchscript --ipex
If you're using a CPU that supports bfloat16, consider sample from the model with bfloat16 enabled for a performance boost, like so

# SD2.1-v
MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000 python -m intel_extension_for_pytorch.cpu.launch --ninstance <number of an instance> --enable_jemalloc scripts/txt2img.py --prompt \"a corgi is playing guitar, oil on canvas\" --ckpt <path/to/768model.ckpt/> --config configs/stable-diffusion/intel/v2-inference-v-bf16.yaml --H 768 --W 768 --precision full --device cpu --torchscript --ipex --bf16
# SD2.1-base
MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000 python -m intel_extension_for_pytorch.cpu.launch --ninstance <number of an instance> --enable_jemalloc scripts/txt2img.py --prompt \"a corgi is playing guitar, oil on canvas\" --ckpt <path/to/model.ckpt/> --config configs/stable-diffusion/intel/v2-inference-bf16.yaml --precision full --device cpu --torchscript --ipex --bf16
Image Modification with Stable Diffusion
depth2img-stable2

Depth-Conditional Stable Diffusion
To augment the well-established img2img functionality of Stable Diffusion, we provide a shape-preserving stable diffusion model.

Note that the original method for image modification introduces significant semantic changes w.r.t. the initial image. If that is not desired, download our depth-conditional stable diffusion model and the dpt_hybrid MiDaS model weights, place the latter in a folder midas_models and sample via

python scripts/gradio/depth2img.py configs/stable-diffusion/v2-midas-inference.yaml <path-to-ckpt>
or

streamlit run scripts/streamlit/depth2img.py configs/stable-diffusion/v2-midas-inference.yaml <path-to-ckpt>
This method can be used on the samples of the base model itself. For example, take this sample generated by an anonymous discord user. Using the gradio or streamlit script depth2img.py, the MiDaS model first infers a monocular depth estimate given this input, and the diffusion model is then conditioned on the (relative) depth output.

depth2image


This model is particularly useful for a photorealistic style; see the examples. For a maximum strength of 1.0, the model removes all pixel-based information and only relies on the text prompt and the inferred monocular depth estimate.

depth2img-stable3

Classic Img2Img
For running the "classic" img2img, use

python scripts/img2img.py --prompt "A fantasy landscape, trending on artstation" --init-img <path-to-img.jpg> --strength 0.8 --ckpt <path/to/model.ckpt>
and adapt the checkpoint and config paths accordingly.

Image Upscaling with Stable Diffusion
upscaling-x4 After downloading the weights, run

python scripts/gradio/superresolution.py configs/stable-diffusion/x4-upscaling.yaml <path-to-checkpoint>
or

streamlit run scripts/streamlit/superresolution.py -- configs/stable-diffusion/x4-upscaling.yaml <path-to-checkpoint>
for a Gradio or Streamlit demo of the text-guided x4 superresolution model.
This model can be used both on real inputs and on synthesized examples. For the latter, we recommend setting a higher noise_level, e.g. noise_level=100.

Image Inpainting with Stable Diffusion
inpainting-stable2

Download the SD 2.0-inpainting checkpoint and run

python scripts/gradio/inpainting.py configs/stable-diffusion/v2-inpainting-inference.yaml <path-to-checkpoint>
or

streamlit run scripts/streamlit/inpainting.py -- configs/stable-diffusion/v2-inpainting-inference.yaml <path-to-checkpoint>
for a Gradio or Streamlit demo of the inpainting model. This scripts adds invisible watermarking to the demo in the RunwayML repository, but both should work interchangeably with the checkpoints/configs.

Shout-Outs
Thanks to Hugging Face and in particular Apolinário for support with our model releases!
Stable Diffusion would not be possible without LAION and their efforts to create open, large-scale datasets.
The DeepFloyd team at Stability AI, for creating the subset of LAION-5B dataset used to train the model.
Stable Diffusion 2.0 uses OpenCLIP, trained by Romain Beaumont.
Our codebase for the diffusion models builds heavily on OpenAI's ADM codebase and https://github.com/lucidrains/denoising-diffusion-pytorch. Thanks for open-sourcing!
CompVis initial stable diffusion release
Patrick's implementation of the streamlit demo for inpainting.
img2img is an application of SDEdit by Chenlin Meng from the Stanford AI Lab.
Kat's implementation of the PLMS sampler, and more.
DPMSolver integration by Cheng Lu.
Facebook's xformers for efficient attention computation.
MiDaS for monocular depth estimation.
License
The code in this repository is released under the MIT License.

The weights are available via the StabilityAI organization at Hugging Face, and released under the CreativeML Open RAIL++-M License License.

BibTeX
@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
About
High-Resolution Image Synthesis with Latent Diffusion Models

Resources
 Readme
License
 MIT, Unknown licenses found
 Activity
 Custom properties
Stars
 41.1k stars
Watchers
 457 watching
Forks
 5.3k forks
Report repository
Releases
No releases published
Packages
No packages published
Contributors
17
@rromb
@hardmaru
@apolinario
@aalbersk
@dmarx
@miao-ju
@jamesthesnake
@Dango233
@lwneal
@kjerk
@TracelessLe
@UdonDa
@Enter-tainer
+ 3 contributors
Languages
Python
100.0%
Footer
© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Docs
Contact
Manage cookies
Do not share my personal information


这个仓库有训练代码么？

---

**Cursor**

是的，这个仓库包含了一些关于 Stable Diffusion 模型的训练信息，但它**不是一个开箱即用的、用于从头开始训练 Stable Diffusion 的完整训练代码库**。

从 `README.md` 中可以看到：

1.  **模型来源和更新：** "This repository contains Stable Diffusion models trained from scratch and will be continuously updated with new checkpoints." 这说明这个仓库是发布和管理 Stable Diffusion 模型的地方，并且模型确实是在这里训练的。
2.  **版本更新描述：** "Version 2.1" 和 "Version 2.0" 的新闻部分详细说明了模型是如何基于之前的版本“fine-tuned”（微调）的，以及使用了哪些文本编码器和训练数据集的过滤等信息。这证实了训练和微调的发生。
3.  **核心引用：** "The original Stable Diffusion model was created in a collaboration with CompVis and RunwayML and builds upon the work: High-Resolution Image Synthesis with Latent Diffusion Models Robin Rombach*, Andreas Blattmann*, Dominik Lorenz, Patrick Esser, Björn Ommer CVPR '22 Oral | GitHub | arXiv | Project page"。这里的 "GitHub" 链接 `CompVis/latent-diffusion` 指向的是**原始的 Latent Diffusion Models 项目**，Stable Diffusion 是在其基础上构建的。**通常，如果你想从头训练或深入了解训练过程，你会去 `CompVis/latent-diffusion` 这个仓库。**
4.  **提供的是推理脚本：** `README.md` 中明确提到："We follow the original repository and provide basic inference scripts to sample from the models." 并且列举了 `scripts/txt2img.py`、`scripts/img2img.py` 等用于文本到图像、图像到图像、超分辨率、inpainting 的推理脚本和 Gradio/Streamlit 演示。

**总结：**

`Stability-AI/stablediffusion` 这个仓库主要是一个**模型发布和推理演示**的仓库。它提供了Stable Diffusion模型本身以及用于这些模型进行推理的示例脚本。它包含了模型训练的背景信息和更新日志，但**并没有直接提供用于从头开始训练 Stable Diffusion 大模型的完整、易于运行的训练代码和脚本**。

如果你想进行 Stable Diffusion 的训练或微调，更常见且推荐的方式是使用：

*   **`diffusers` 库：** `diffusers` 提供了易于使用的训练和微调脚本，极大地简化了这一过程。这是目前进行 Stable Diffusion 微调最主流的方式。
*   **`CompVis/latent-diffusion`：** 这是 Stable Diffusion 的原始论文实现仓库，提供了更底层的训练代码，但相对来说更复杂，对硬件和环境要求也更高。

---

**User**

Novel architectures have recently improved generative image synthesis leading
to excellent visual quality in various tasks. Much of this success is due to the
scalability of these architectures and hence caused by a dramatic increase in model
complexity and in the computational resources invested in training these models.
Our work questions the underlying paradigm of compressing large training data into
ever growing parametric representations. We rather present an orthogonal, semiparametric approach. We complement comparably small diffusion or autoregressive
models with a separate image database and a retrieval strategy. During training
we retrieve a set of nearest neighbors from this external database for each training
instance and condition the generative model on these informative samples. While
the retrieval approach is providing the (local) content, the model is focusing on
learning the composition of scenes based on this content. As demonstrated by our
experiments, simply swapping the database for one with different contents transfers
a trained model post-hoc to a novel domain. The evaluation shows competitive
performance on tasks which the generative model has not been trained on, such as
class-conditional synthesis, zero-shot stylization or text-to-image synthesis without
requiring paired text-image data. With negligible memory and computational
overhead for the external database and retrieval we can significantly reduce the
parameter count of the generative model and still outperform the state-of-the-art.

这论文是stable diffusion论文么？

如果我要从零训练一个diffusion model，应该看哪些论文，有哪些开源项目pytorch实现了diffusion的数据处理，训练和推理过程

---

**Cursor**

好的，我们来逐一解答这两个问题。

### 第一个问题：这论文是 Stable Diffusion 论文吗？

根据您提供的摘要内容，**这篇论文不是 Stable Diffusion 的论文。**

**理由如下：**

1.  **核心思想不同：**
    *   您提供的摘要描述了一种**“半参数化（semi-parametric）”**的方法。它通过结合一个相对较小的扩散模型（或自回归模型）与一个**独立的图像数据库和检索策略**来进行图像生成。在训练过程中，模型会从外部数据库中检索最近邻样本，并以这些样本为条件进行生成。
    *   而 **Stable Diffusion 的核心创新在于“潜在扩散模型（Latent Diffusion Models）”**。它将扩散过程从高维像素空间转移到**低维的潜在空间**进行，并通过一个强大的**自编码器（AutoEncoder）**在像素空间和潜在空间之间进行转换。它是一个**完全参数化**的模型，通过在大规模文本-图像对数据集上进行端到端训练来学习生成过程，不依赖于运行时从外部数据库检索图片来提供内容。

2.  **关键词和贡献点的差异：**
    *   摘要中反复强调“semi-parametric approach”、“separate image database and a retrieval strategy”、“swapping the database for one with different contents transfers a trained model post-hoc to a novel domain”、“reduce the parameter count of the generative model”。这些都是典型的半参数化模型的特征。
    *   Stable Diffusion 的论文 (`High-Resolution Image Synthesis with Latent Diffusion Models`) 则强调“latent space”、“perceptual compression”、“cross-attention mechanism for conditioning”、“text-to-image synthesis from scratch”。

虽然两者都涉及生成图像和扩散模型，但它们解决问题的方式和核心技术路线是不同的。

### 第二个问题：如果我要从零训练一个 diffusion model，应该看哪些论文，有哪些开源项目 PyTorch 实现了 diffusion 的数据处理、训练和推理过程？

从零训练一个 Diffusion Model 是一项复杂的任务，需要理解其理论基础和实践细节。

#### 1. 必读核心论文：

1.  **Denoising Diffusion Probabilistic Models (DDPM)**
    *   **作者：** Jonathan Ho, Ajay Jain, Pieter Abbeel (2020)
    *   **重要性：** 这是现代扩散模型的开山之作，重新点燃了对扩散模型的研究热情。它提出了一个**实用且稳定的训练和采样算法**，奠定了后续所有扩散模型的基础。理解这个模型是理解扩散模型工作原理的关键。

2.  **Score-Based Generative Modeling through Stochastic Differential Equations**
    *   **作者：** Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Stefano Ermon (2020)
    *   **重要性：** 这篇论文从**分值匹配（Score Matching）**的角度统一了扩散模型和基于分数的生成模型。它引入了**随机微分方程（SDEs）**来描述扩散过程，并由此引出了更灵活、更高效的采样器（如 DDIM，虽然 DDIM 论文独立）。

3.  **High-Resolution Image Synthesis with Latent Diffusion Models (Stable Diffusion 论文)**
    *   **作者：** Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer (2022)
    *   **重要性：** 这篇论文是 **Stable Diffusion** 的理论基础。它解决了在像素空间直接进行扩散模型训练和推理计算量巨大的问题，提出在**低维潜在空间**进行扩散。这是将扩散模型扩展到高分辨率图像生成并使其广泛应用的关键。

4.  **Classifier-Free Diffusion Guidance**
    *   **作者：** Jonathan Ho, Prafulla Dhariwal, Alex Nichol (2022)
    *   **重要性：** 提出了**分类器自由引导（Classifier-Free Guidance）**的技术。这种技术在不使用外部分类器的情况下，通过结合条件和无条件的去噪网络输出，极大地提高了生成图像的质量和与文本提示的符合度。几乎所有现代的文本到图像扩散模型（包括 Stable Diffusion）都使用这种引导方式。

#### 2. PyTorch 开源项目和实现：

以下是几个在 PyTorch 中实现了扩散模型数据处理、训练和推理的优秀开源项目：

1.  **Hugging Face `diffusers` 库 (强烈推荐)**
    *   **GitHub:** `https://github.com/huggingface/diffusers`
    *   **特点：** 这是目前最活跃、最全面、最易用的扩散模型库。
        *   **数据处理：** 虽然它不直接提供原始数据集的下载和清洗，但其训练脚本（如在 `examples/text_to_image` 或 `examples/unconditional_image_generation` 中）会展示如何使用 `datasets` 库或 `ImageFolder` 加载数据，并应用 `torchvision.transforms` 进行预处理（缩放、归一化等）。
        *   **训练：** 提供了大量用于**微调（fine-tuning）和从头开始训练**各种扩散模型（包括 DDPM、Latent Diffusion、ControlNet、LoRA 等）的官方示例脚本。这些脚本是高度优化的，支持分布式训练 (`accelerate`) 和混合精度训练。对于从头训练，你可以修改这些脚本以适应你的数据集和模型架构。
        *   **推理：** 提供高级 `Pipeline` API，仅需几行代码即可加载预训练模型并生成图像。同时，也提供了底层的组件（如 U-Net、调度器、编码器等），方便你构建自定义的推理流程。
    *   **为何推荐：** 模块化设计、丰富的预训练模型、详细的文档和教程、活跃的社区。如果你要从零开始，它的训练示例是非常好的起点。

2.  **`CompVis/latent-diffusion`**
    *   **GitHub:** `https://github.com/CompVis/latent-diffusion`
    *   **特点：** 这是 **Stable Diffusion 论文的官方实现仓库**。
        *   **数据处理：** 包含其训练 Latent Diffusion 模型所需的数据加载和预处理逻辑。
        *   **训练：** 提供了原始的 Latent Diffusion 模型的训练代码。如果你想深入理解 Stable Diffusion 模型的原始训练细节，或者复现论文中的结果，这个仓库是必不可少的。
        *   **推理：** 提供了基本的推理脚本，但不如 `diffusers` 库那么用户友好和功能全面。
    *   **为何推荐：** 了解 Stable Diffusion 原始实现细节，但代码可能比 `diffusers` 更复杂，更偏向研究而非日常使用。

3.  **`hojonathanho/diffusion` (DDPM 官方实现)**
    *   **GitHub:** `https://github.com/hojonathanho/diffusion`
    *   **特点：** 这是 **DDPM 论文的官方 PyTorch 实现**。
        *   **数据处理/训练/推理：** 代码库相对简洁，专注于 DDPM 的核心算法。它主要用于无条件图像生成。
    *   **为何推荐：** 如果你想从最基础的 DDPM 开始，理解其核心训练循环和采样过程，这个仓库是极好的学习资源。它比 Latent Diffusion 或 Stable Diffusion 的代码更简单。

4.  **`lucidrains/denoising-diffusion-pytorch`**
    *   **GitHub:** `https://github.com/lucidrains/denoising-diffusion-pytorch`
    *   **特点：** 这是一个由社区维护的、非常**简洁易懂的 DDPM 实现**。
        *   **数据处理/训练/推理：** 代码结构清晰，非常适合初学者学习扩散模型的基本原理和实现。
    *   **为何推荐：** 作为 DDPM 的辅助学习材料，代码可读性高。

**数据处理和训练过程的通用要点：**

*   **数据集：** 对于文本到图像模型，你需要大规模的文本-图像对数据集，如 LAION-5B (或其子集)。对于无条件图像生成，可以使用 ImageNet, CelebA, CIFAR 等。
*   **图像预处理：**
    *   **裁剪/缩放：** 将图像统一到目标分辨率（例如 256x256, 512x512, 768x768）。
    *   **归一化：** 将像素值从 \([0, 255]\) 范围转换到 \([-1, 1]\) 范围，这是扩散模型常用的输入范围。
    *   **数据增强：** 随机水平翻转、随机裁剪等。
*   **文本预处理 (对于条件扩散模型)：**
    *   **分词（Tokenization）：** 使用像 CLIP (OpenCLIP) 这样的预训练文本编码器的分词器。
    *   **编码：** 将分词后的文本输入到文本编码器（如 CLIP 的文本 Transformer）中，获得文本嵌入 (text embeddings)。这些嵌入会作为条件信息输入到扩散模型的 U-Net 中。
*   **训练循环：**
    *   **噪声调度器：** 定义如何逐步添加噪声 (如线性、余弦调度)。
    *   **损失函数：** 通常是预测噪声和真实噪声之间的 L2 损失。
    *   **优化器：** AdamW 是常见选择。
    *   **学习率调度器：** 预热 (warmup) 和余弦退火 (cosine decay) 常见。
    *   **EMA (Exponential Moving Average)：** 在训练过程中维护模型参数的 EMA 副本，通常能生成更高质量的样本。
    *   **混合精度训练 (Mixed Precision)：** 使用 `torch.amp` (或 NVIDIA APEX) 来加速训练并减少显存占用。
    *   **分布式训练：** 对于大型数据集和模型，使用 `torch.distributed` 或 Hugging Face `accelerate` 进行多 GPU 训练。

从 `diffusers` 库的训练示例入手，逐步理解各个组件和训练流程，是目前最有效率的从零开始训练扩散模型的方式。

---

**User**

Skip to content
Navigation Menu
huggingface
diffusers

Type / to search
Code
Issues
498
Pull requests
199
Discussions
Actions
Projects
1
Security
Insights
Files
stable
.github
benchmarks
docker
docs
examples
scripts
src/diffusers
commands
experimental
hooks
loaders
models
pipelines
allegro
amused
animatediff
audioldm
audioldm2
aura_flow
blip_diffusion
chroma
cogvideo
cogview3
cogview4
consisid
consistency_models
controlnet
controlnet_hunyuandit
controlnet_sd3
controlnet_xs
cosmos
dance_diffusion
ddim
ddpm
deepfloyd_if
deprecated
dit
easyanimate
flux
hidream_image
hunyuan_video
hunyuandit
i2vgen_xl
kandinsky
kandinsky2_2
kandinsky3
kolors
latent_consistency_models
latent_diffusion
latte
ledits_pp
ltx
lumina
lumina2
marigold
mochi
musicldm
omnigen
pag
paint_by_example
pia
pixart_alpha
sana
semantic_stable_diffusion
shap_e
stable_audio
stable_cascade
stable_diffusion
README.md
__init__.py
clip_image_project_model.py
convert_from_ckpt.py
pipeline_flax_stable_diffusion.py
pipeline_flax_stable_diffusion_img2img.py
pipeline_flax_stable_diffusion_inpaint.py
pipeline_onnx_stable_diffusion.py
pipeline_onnx_stable_diffusion_img2img.py
pipeline_onnx_stable_diffusion_inpaint.py
pipeline_onnx_stable_diffusion_upscale.py
pipeline_output.py
pipeline_stable_diffusion.py
pipeline_stable_diffusion_depth2img.py
pipeline_stable_diffusion_image_variation.py
pipeline_stable_diffusion_img2img.py
pipeline_stable_diffusion_inpaint.py
pipeline_stable_diffusion_instruct_pix2pix.py
pipeline_stable_diffusion_latent_upscale.py
pipeline_stable_diffusion_upscale.py
pipeline_stable_unclip.py
pipeline_stable_unclip_img2img.py
safety_checker.py
safety_checker_flax.py
stable_unclip_image_normalizer.py
stable_diffusion_3
stable_diffusion_attend_and_excite
stable_diffusion_diffedit
stable_diffusion_gligen
stable_diffusion_k_diffusion
stable_diffusion_ldm3d
stable_diffusion_panorama
stable_diffusion_safe
stable_diffusion_sag
stable_diffusion_xl
__init__.py
pipeline_flax_stable_diffusion_xl.py
pipeline_output.py
pipeline_stable_diffusion_xl.py
pipeline_stable_diffusion_xl_img2img.py
pipeline_stable_diffusion_xl_inpaint.py
pipeline_stable_diffusion_xl_instruct_pix2pix.py
watermark.py
stable_video_diffusion
t2i_adapter
text_to_video_synthesis
unclip
unidiffuser
visualcloze
wan
wuerstchen
README.md
__init__.py
auto_pipeline.py
free_init_utils.py
free_noise_utils.py
onnx_utils.py
pipeline_flax_utils.py
pipeline_loading_utils.py
pipeline_utils.py
transformers_loading_utils.py
quantizers
schedulers
utils
__init__.py
callbacks.py
configuration_utils.py
dependency_versions_check.py
dependency_versions_table.py
image_processor.py
optimization.py
py.typed
training_utils.py
video_processor.py
tests
utils
.gitignore
CITATION.cff
CODE_OF_CONDUCT.md
CONTRIBUTING.md
LICENSE
MANIFEST.in
Makefile
PHILOSOPHY.md
README.md
_typos.toml
pyproject.toml
setup.py
diffusers/src/diffusers/pipelines
/stable_diffusion/
qgallouedecgithub-actions[bot]
qgallouedec
and
github-actions[bot]
Use HF Papers (#11567)
c8bb1ff
 · 
last month
Name	Last commit message	Last commit date
..
README.md
Use HF Papers (#11567)
last month
__init__.py
clean up the __Init__ for stable_diffusion (#11500)
last month
clip_image_project_model.py
change to 2024 in the license (#6902)
last year
convert_from_ckpt.py
[BUG] Fix convert_vae_pt_to_diffusers bug (#11078)
2 months ago
pipeline_flax_stable_diffusion.py
Use Pipelines without unet (#10440)
5 months ago
pipeline_flax_stable_diffusion_img2img.py
Use pipelines without vae (#10441)
5 months ago
pipeline_flax_stable_diffusion_inpaint.py
Update Ruff to latest Version (#10919)
2 months ago
pipeline_onnx_stable_diffusion.py
Use HF Papers (#11567)
last month
pipeline_onnx_stable_diffusion_img2img.py
Use HF Papers (#11567)
last month
pipeline_onnx_stable_diffusion_inpaint.py
Use HF Papers (#11567)
last month
pipeline_onnx_stable_diffusion_upscale.py
Use HF Papers (#11567)
last month
pipeline_output.py
Fix type annotation (#5146)
2 years ago
pipeline_stable_diffusion.py
Use HF Papers (#11567)
last month
pipeline_stable_diffusion_depth2img.py
Use HF Papers (#11567)
last month
pipeline_stable_diffusion_image_variation.py
Use HF Papers (#11567)
last month
pipeline_stable_diffusion_img2img.py
Use HF Papers (#11567)
last month
pipeline_stable_diffusion_inpaint.py
Use HF Papers (#11567)
last month
pipeline_stable_diffusion_instruct_pix2pix.py
Use HF Papers (#11567)
last month
pipeline_stable_diffusion_latent_upscale.py
Use HF Papers (#11567)
last month
pipeline_stable_diffusion_upscale.py
Use HF Papers (#11567)
last month
pipeline_stable_unclip.py
Use HF Papers (#11567)
last month
pipeline_stable_unclip_img2img.py
Use HF Papers (#11567)
last month
safety_checker.py
#7535 Update FloatTensor type hints to Tensor (#7883)
last year
safety_checker_flax.py
change to 2024 in the license (#6902)
last year
stable_unclip_image_normalizer.py
change to 2024 in the license (#6902)
last year
README.md
Stable Diffusion
Overview
Stable Diffusion was proposed in Stable Diffusion Announcement by Patrick Esser and Robin Rombach and the Stability AI team.

The summary of the model is the following:

Stable Diffusion is a text-to-image model that will empower billions of people to create stunning art within seconds. It is a breakthrough in speed and quality meaning that it can run on consumer GPUs. You can see some of the amazing output that has been created by this model without pre or post-processing on this page. The model itself builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all.

Tips:
Stable Diffusion has the same architecture as Latent Diffusion but uses a frozen CLIP Text Encoder instead of training the text encoder jointly with the diffusion model.
An in-detail explanation of the Stable Diffusion model can be found under Stable Diffusion with 🧨 Diffusers.
If you don't want to rely on the Hugging Face Hub and having to pass a authentication token, you can download the weights with git lfs install; git clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5 and instead pass the local path to the cloned folder to from_pretrained as shown below.
Stable Diffusion can work with a variety of different samplers as is shown below.
Available Pipelines:
Pipeline	Tasks	Colab
pipeline_stable_diffusion.py	Text-to-Image Generation	Open In Colab
pipeline_stable_diffusion_img2img	Image-to-Image Text-Guided Generation	Open In Colab
pipeline_stable_diffusion_inpaint	Text-Guided Image Inpainting	Open In Colab
Examples:
Using Stable Diffusion without being logged into the Hub.
If you want to download the model weights using a single Python line, you need to be logged in via huggingface-cli login.

from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5")
This however can make it difficult to build applications on top of diffusers as you will always have to pass the token around. A potential way to solve this issue is by downloading the weights to a local path "./stable-diffusion-v1-5":

git lfs install
git clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5
and simply passing the local path to from_pretrained:

from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("./stable-diffusion-v1-5")
Text-to-Image with default PLMS scheduler
# make sure you're logged in with `huggingface-cli login`
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5")
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]

image.save("astronaut_rides_horse.png")
Text-to-Image with DDIM scheduler
# make sure you're logged in with `huggingface-cli login`
from diffusers import StableDiffusionPipeline, DDIMScheduler

scheduler =  DDIMScheduler.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="scheduler")

pipe = StableDiffusionPipeline.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5",
    scheduler=scheduler,
).to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]

image.save("astronaut_rides_horse.png")
Text-to-Image with K-LMS scheduler
# make sure you're logged in with `huggingface-cli login`
from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler

lms = LMSDiscreteScheduler.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="scheduler")

pipe = StableDiffusionPipeline.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5",
    scheduler=lms,
).to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]

image.save("astronaut_rides_horse.png")
CycleDiffusion using Stable Diffusion and DDIM scheduler
import requests
import torch
from PIL import Image
from io import BytesIO

from diffusers import CycleDiffusionPipeline, DDIMScheduler


# load the scheduler. CycleDiffusion only supports stochastic schedulers.

# load the pipeline
# make sure you're logged in with `huggingface-cli login`
model_id_or_path = "CompVis/stable-diffusion-v1-4"
scheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder="scheduler")
pipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to("cuda")

# let's download an initial image
url = "https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/An%20astronaut%20riding%20a%20horse.png"
response = requests.get(url)
init_image = Image.open(BytesIO(response.content)).convert("RGB")
init_image = init_image.resize((512, 512))
init_image.save("horse.png")

# let's specify a prompt
source_prompt = "An astronaut riding a horse"
prompt = "An astronaut riding an elephant"

# call the pipeline
image = pipe(
    prompt=prompt,
    source_prompt=source_prompt,
    image=init_image,
    num_inference_steps=100,
    eta=0.1,
    strength=0.8,
    guidance_scale=2,
    source_guidance_scale=1,
).images[0]

image.save("horse_to_elephant.png")

# let's try another example
# See more samples at the original repo: https://github.com/ChenWu98/cycle-diffusion
url = "https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/A%20black%20colored%20car.png"
response = requests.get(url)
init_image = Image.open(BytesIO(response.content)).convert("RGB")
init_image = init_image.resize((512, 512))
init_image.save("black.png")

source_prompt = "A black colored car"
prompt = "A blue colored car"

# call the pipeline
torch.manual_seed(0)
image = pipe(
    prompt=prompt,
    source_prompt=source_prompt,
    image=init_image,
    num_inference_steps=100,
    eta=0.1,
    strength=0.85,
    guidance_scale=3,
    source_guidance_scale=1,
).images[0]

image.save("black_to_blue.png")
diffusers/src/diffusers/pipelines/stable_diffusion at main · huggingface/diffusers

看看这个实现？

---

**Cursor**

好的，我们来看看 `diffusers/src/diffusers/pipelines/stable_diffusion` 这个目录下的实现。

这个目录是 `diffusers` 库中**Stable Diffusion 模型的具体实现管线（Pipelines）**的集合。它不是模型架构（如 U-Net 或 VAE）的定义，而是将这些核心组件组合起来，提供给用户方便地进行各种Stable Diffusion相关任务推理的接口。

**这个目录下主要包含以下类型的文件：**

1.  **`README.md`**: 提供了 Stable Diffusion 在 `diffusers` 库中的概述、模型背景、安装和使用示例。这是理解该目录功能的最佳起点。
2.  **`pipeline_stable_diffusion.py`**:
    *   这是最核心的文件，实现了**文本到图像（Text-to-Image）**的 Stable Diffusion 推理管线。
    *   它加载并整合了 Stable Diffusion 模型所需的所有关键组件：
        *   **文本编码器 (Text Encoder)**：通常是 `CLIPTextModel`，用于将文本提示转换为潜在嵌入。
        *   **VAE (Variational AutoEncoder)**：用于在像素空间和潜在空间之间进行编码和解码。
        *   **U-Net (Noise Predictor)**：扩散模型的核心，用于预测潜在空间中的噪声。
        *   **调度器 (Scheduler)**：定义了去噪过程中的步进策略（如 DDPM, PMLS, DDIM, K-LMS 等）。
    *   这个文件定义了 `StableDiffusionPipeline` 类，其 `__call__` 方法封装了从文本提示到最终图像生成的所有逻辑步骤。

3.  **其他 `pipeline_stable_diffusion_*.py` 文件**:
    *   这些文件基于核心的 `StableDiffusionPipeline` 进行了扩展或修改，以支持 Stable Diffusion 的各种**下游任务**：
        *   `pipeline_stable_diffusion_img2img.py`: **图像到图像（Image-to-Image）**，以输入图像为基础，结合文本提示进行图像修改。
        *   `pipeline_stable_diffusion_inpaint.py`: **图像修复（Inpainting）**，根据文本提示和图像掩码（mask）填充图像的缺失部分。
        *   `pipeline_stable_diffusion_depth2img.py`: **深度到图像（Depth-to-Image）**，利用输入图像的深度信息进行图像生成或修改。
        *   `pipeline_stable_diffusion_image_variation.py`: 图像变体生成。
        *   `pipeline_stable_diffusion_instruct_pix2pix.py`: 用于 Pix2Pix 任务的管线。
        *   `pipeline_stable_diffusion_latent_upscale.py`, `pipeline_stable_diffusion_upscale.py`: 图像超分辨率。
    *   这些文件通常会导入 `StableDiffusionPipeline` 中的核心逻辑，并添加任务特有的预处理（如加载初始图像、创建掩码、处理深度图）和后处理步骤。

4.  **`pipeline_flax_stable_diffusion.py` 等 `pipeline_flax_*.py` 文件**:
    *   这些是 **Flax (JAX 框架)** 版本的 Stable Diffusion 管线实现。Hugging Face `diffusers` 库也支持 JAX/Flax，提供与 PyTorch 类似的 API。

5.  **`pipeline_onnx_stable_diffusion.py` 等 `pipeline_onnx_*.py` 文件**:
    *   这些是 **ONNX (Open Neural Network Exchange)** 版本的管线，用于模型的导出和在 ONNX Runtime 中的推理，以实现更优的部署性能。

6.  **`safety_checker.py`**:
    *   用于检测生成图像中是否存在不安全内容（NSFW）。在默认的 Stable Diffusion 管线中，生成的图像会经过这个检查器。

7.  **`convert_from_ckpt.py`**:
    *   这是一个实用脚本，用于将传统的 `.ckpt` 格式的 Stable Diffusion 模型权重转换为 `diffusers` 库兼容的格式。这对于用户导入旧版模型非常有用。

**这个实现的核心思想：**

`diffusers` 库在这里将复杂的 Stable Diffusion 模型分解为更小的、可管理的组件（U-Net、VAE、Text Encoder、Scheduler）。然后，通过 `Pipeline` 类将这些组件组装起来，抽象化了底层的推理逻辑，使得用户可以非常简单地通过调用 `pipe(prompt).images[0]` 来生成图像，而无需关心模型内部的复杂运算和数据流。

通过查看这些 `pipeline_*.py` 文件，你可以看到 `diffusers` 如何将 Stable Diffusion 的各个部分（如文本提示到文本嵌入、图像像素到潜在特征、潜在空间去噪、潜在特征到图像像素）连接起来，形成一个完整的图像生成工作流。

例如，在 `pipeline_stable_diffusion.py` 中，你会看到类似这样的逻辑：
1.  文本提示通过 `self.text_encoder` 转换为文本嵌入。
2.  随机噪声通过 `self.unet` 和 `self.scheduler` 迭代去噪，结合文本嵌入的条件。
3.  去噪后的潜在表示通过 `self.vae.decode` 转换为最终的像素图像。

这个目录是 `diffusers` 库如何将一个复杂模型变得易于使用的绝佳例子。

---

**User**

Large Language Diffusion Models
Shen Nie1, Fengqi Zhu1, Zebin You1, Xiaolu Zhang2, Jingyang Ou1, Jun Hu2, Jun Zhou2, Yankai Lin1, Ji-Rong Wen1, Chongxuan Li1
1Renmin University of China, 2Ant Group
 
 
TL;DR: We introduce LLaDA, a diffusion model with an unprecedented 8B scale, trained entirely from scratch, rivaling LLaMA3 8B in performance.

LLaDA_vs_LLaMA
LLaDA_vs_LLaMA_chat
What is now proved was once only imagined. ——— William Blake
Motivation
We contend that the intelligence of LLMs—manifested in scalability, instruction-following, in-context learning, conversational ability, and compression—stems not from the autoregressive mechanism per se, but rather from the core principle of generative modeling: approximating the true language distribution through maximum likelihood estimation.

We introduce LLaDA (Large Language Diffusion with mAsking), a simple yet principled generative paradigm for large language models that demonstrates the aforementioned remarkable capabilities.

Method
LLaDA is a masked diffusion model [1, 2, 3] that follows standard pretraining and SFT while sampling via diffusion. During pretraining, it masks all tokens randomly at ratio 
; in SFT, only response tokens may be masked. The model simulates diffusion from full masking (
) to unmasking (
), predicting all masks simultaneously at each step with flexible remasking.

method
Scalability
LLaDA demonstrates impressive scalability, with its overall trend being highly competitive with that of autoregressive baseline on the same data.

scaling_mmluscaling_arc_cscaling_cmmlu
scaling_piqascaling_gsm8kscaling_humaneval
A text generation method different from the traditional left-to-right approach
Prompt: Explain what artificial intelligence is.

diff_normal_150ms.gif
Case Studies
Math problem
User
Emma decides to run 4 sprints 5 times a week. She runs 75 meters each sprint. How many total meters does she run a week?
LLaDA-Instruct
Emma runs 4 sprints each time, so she runs a total of 4 x 5 = 20 sprints a week. Each sprint is 75 meters long, so Emma runs a total of 20 x 75 = 1500 meters a week. The final result is 1500.
Daily life
User
Please recommend me three famous movies
LLaDA-Instruct
Certainly! Here are three famous movies that have achieved widespread popularity and critical acclaim:
1. The Shawshank Redemption (1994) - Inspired by a novella and directed by Frank Darabont, this film tells the story of Andy Dufresne, a man convicted of murdering his wife, dealing with his harsh life in Shawshank Prison and forming an unlikely friendship with a fellow inmate.
2. The Godfather (1972) - Directed by Francis Ford Coppola, this film is based on Mario Puzo's novel of the same name. It follows the story of the Corleone family, particularly Michael Corleone, who rises to power and takes over the family's criminal empire.
3. The Empire Strikes Back (1980) - Directed by George Lucas, this film continues the story of a young Skywalker in the Star Wars saga. It follows Luke Skywalker as he helps the Rebel Alliance fight against the Galactic Empire and destroy the powerful weapon, the Death Star.
All of these films are celebrated for their cinematic achievements, powerful storytelling, and memorable characters.
Code generation
User
I am writing python code, I have an array, please help me sort it from largest to smallest
LLaDA-Instruct
Certainly! You can sort an array from the largest to smallest in Python using the ‘sort()’ method or the ‘sorted()‘ function. Here’s an example:
### Using ‘sort()’ method:
my array = [3, 1, 4, 1, 5, 2]
my array.sort(reverse=True)
print(my array)
Translation (English → Chinese)
User
Please help me translate into Chinese: ‘What is now proved was once only imagined’
LLaDA-Instruct
现在被证明的曾经只是想象
Translation (English → German)
User
Please help me translate into German: ‘What is now proved was once only imagined’
LLaDA-Instruct
Wer heute bestätigt ist, war einst nur immaginiert.
Multi-turn dialogue case 1
User
Could you please share with me what the initial two lines of the renowned poem `The Road Not Taken'?
LLaDA-Instruct
Certainly! The initial two lines of the renowned poem ``The Road Not Taken" by Robert Frost are: ``Two roads diverged in a yellow wood, And sorry I could not travel both." These lines set the stage for the poem, introducing the idea of a speaker facing a choice between two paths.
User
Please help me translate into Chinese.
LLaDA-Instruct
Sure! The first two lines of ``The Road Not Taken" by Robert Frost can be translated into Chinese as: ``两条路分岔在黄色的树林中，遗憾我不能同时走''
User
Please translate into German.
LLaDA-Instruct
Sure! The first two lines of ``The Road Not Taken" by Robert Frost can be translated into German as: ``Zwei Wege trennten sich im gelben Wald, und leider konnte ich nicht beide tun."
User
Please also write a poem about life choices. I require 5 sentences, and the first word of each sentence starts with C as the first letter.
LLaDA-Instruct
Certainly! Here's a poem about life choices, with each sentence starting with the letter ``C": Choices cascade through life's journey, Creating connections with each decision, Careful consideration leads to better paths, Courageous choices shape our destiny, Choosing wisely guides us through the years.
Multi-turn dialogue case 2
User
Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
LLaDA-Instruct
Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold 48/2 = 24 clips in May. Altogether, Natalia sold 48 + 24 = 72 clips in April and May. The final result is 72.
User
Natalia sold three times as many items in June as she did in May. How many clips did Natalia sell altogether in these three months?
LLaDA-Instruct
Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold 48/2 = 24 clips in May. Natalia sold three times as many items in June as she did in May, so she sold 24 * 3 = 72 clips in June. Altogether, Natalia sold 48 + 24 + 72 = 144 clips in these three months. The final result is 144.
BibTeX
@misc{nie2025largelanguagediffusionmodels,
      title={Large Language Diffusion Models}, 
      author={Shen Nie and Fengqi Zhu and Zebin You and Xiaolu Zhang and Jingyang Ou and Jun Hu and Jun Zhou and Yankai Lin and Ji-Rong Wen and Chongxuan Li},
      year={2025},
      eprint={2502.09992},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.09992}, 
}
References
[1] Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981–17993, 2021a.

[2] Ou, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li, C. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024.

[3] Nie S, Zhu F, Du C, et al. Scaling up Masked Diffusion Models on Text[J]. arXiv preprint arXiv:2410.18514, 2024.
这个引言

---

**Cursor**

您提供的这部分内容，包括 "TL;DR" 和 "Motivation" 部分，确实是这篇名为《Large Language Diffusion Models》论文的**引言（Introduction）**部分。

**引言的作用：**

*   **TL;DR (Too Long; Didn't Read)**：这是一个非常简短的总结，通常用于快速向读者传达论文最核心的创新点和结果。在这里，它直接点明了 LLaDA 这个 8B 参数的扩散模型，完全从零训练，并在性能上可与 LLaMA3 8B 匹敌。
*   **Motivation (动机)**：这部分详细阐述了作者为什么进行这项研究，以及他们想要解决什么问题或提出什么新观点。
    *   **核心论点：** 作者认为大型语言模型 (LLM) 的智能（如可扩展性、指令遵循、上下文学习、对话能力和压缩）并非仅仅来源于自回归机制，而是源于**生成建模的核心原则：通过最大似然估计逼近真实的语言分布。**
    *   **引出贡献：** 基于这个动机，他们引入了 **LLaDA (Large Language Diffusion with mAsking)**，一个他们认为简单而有原则的生成范式，用于大型语言模型，并展示了上述卓越的能力。

**总结来说，这段“引言”清晰地：**

1.  用精炼的语言概括了论文的核心成果（TL;DR）。
2.  提出了一个关于 LLM 智能来源的新颖观点（Motivation）。
3.  基于这个观点，引出了他们提出的新模型 LLaDA，为接下来的方法介绍做铺垫。

这正是研究论文引言部分应起到的作用。

---

**User**

What is now proved was once only imagined.

---

**Cursor**

"What is now proved was once only imagined." 是英国诗人、画家和版画家**威廉·布莱克 (William Blake)** 的一句名言。

这句话的含义是：

*   **“What is now proved” (现在被证明的)**：指那些我们现在认为是事实、科学定理、技术成就或已经被证实存在的事物。
*   **“was once only imagined” (曾经仅仅是想象的)**：指这些现在被证明的事物，在它们被发现、发明或证实之前，仅仅存在于人们的思维、梦想、幻想或理论构想之中。

**整体含义是：** 现今的一切成就和现实，无论是科学发现、技术发明还是社会进步，都源于最初的想象和创意。它强调了**想象力的巨大力量和先驱者的远见**。它告诉我们，今天看来不可能的事情，通过大胆的想象、不懈的探索和努力，最终可能成为明天的现实。这句话是对人类创新精神和求知欲的颂扬。

---

**User**

Search

Search results for
All search results
News
Opinions
Reviews
Best Products
Guides
Authority Insights
All topics
More
Best daily deals
Login


Affiliate links on Android Authority may earn us a commission. Learn more.

Mobile
Android OS
AOSP isn't dead, but Google just landed a huge blow to custom ROM developers
Some people are speculating that Google is planning to discontinue AOSP, but the company says these claims are false.
By Mishaal Rahman
•
June 12, 2025

•

13
•

Android figures standing around Pixel phone with AOSP home page showing
TL;DR
Google has made it harder to build custom Android ROMs for Pixel phones by omitting their device trees and driver binaries from the latest AOSP release.
The company says this is because it’s shifting its AOSP reference target from Pixel hardware to a virtual device called “Cuttlefish” to be more neutral.
While Google insists AOSP isn’t going away, developers must now reverse-engineer changes, making the process for supporting Pixel devices more difficult.
Earlier this year, Google announced it would develop the Android OS fully in private to simplify its development process. By focusing its efforts on a single internal branch, Google aimed to streamline work that was previously split. The news initially spooked some in the Android development community, but the controversy quickly subsided. The impact was minimal, as Google was already developing most of Android behind closed doors and promised that source code releases would continue. Now, however, a recent omission from Google has rekindled fears that the company might stop sharing source code for new Android releases. Google has stated these concerns are unfounded, but other new changes make it harder for the custom ROM community to thrive on Pixel devices.

Is AOSP going away? Google says no
As promised, Google published the source code for Android 16 this week, allowing independent developers to compile their own builds of the new operating system. This source code was uploaded to the Android Open Source Project (AOSP), as usual, under the permissive Apache 2.0 license.

However, multiple developers quickly noticed a glaring omission from the Android 16 source code release: the device trees for Pixel devices were missing. Google also failed to upload new driver binaries for each Pixel device and released the kernel source code with a squashed commit history. Since Google has shared the device trees, driver binaries, and full kernel source code commit history for years, its omission in this week’s release was concerning.

These omissions led some to speculate this week that Google was taking the first step in a plan to discontinue AOSP. In response, Google’s VP and GM of Android Platform, Seang Chau, refuted these claims. He addressed the speculation in a post on X, stating that “AOSP is NOT going away.”

Google denies discontinuing AOSP
He also confirmed the omission of Pixel device trees is intentional, stating that “AOSP needs a reference target that is flexible, configurable, and affordable — independent of any particular hardware, including those from Google.” Instead of supporting AOSP builds on Pixel devices, Google will support the virtual Android device “Cuttlefish” as its reference target. Cuttlefish runs on PCs, allowing Google and platform developers to test new hardware features. Google will also continue to support GSI targets, which are generic system images that can be installed on nearly any Android device.

Latest deals on top tech
See all deals

Save 60% off the Kiwi Ears Singolo IEMs
60% off
See price at Amazon
Limited Time!


Apple MacBook Air 13-Inch (M4, 16GB, 256GB)
15% off
See price at Amazon

Google TV Streamer
15% off
See price at Amazon
Limited Time Deal!

Samsung Galaxy Buds 3 Pro
52% off
See price at Amazon

Amazon Echo Spot (2024)
18% off
See price at Amazon
Limited Time Deal!
On one hand, this logic is sound. Google wants to move away from using Pixels as the AOSP reference device and is making changes to that effect. As Seang Chau notes, “AOSP was built on the foundation of being an open platform for device implementations, SoC vendors, and instruction set architectures.” In that regard, Cuttlefish is a more appropriate reference target because it isn’t a heavily customized piece of consumer hardware like a Pixel phone. However, since Cuttlefish is a virtual device, it can only simulate how hardware features behave, making it an imperfect reference in some ways.

How do these changes affect custom ROM development?
LineageOS Logo (2 of 3)
The more significant issue, however, is the impact this decision will have on developers who build custom ROMs — the community term for hobbyist forks of AOSP. Nolen Johnson, a long-time contributor and reviewer for the LineageOS project, says the process of building these ROMs for Pixel phones will become “painful” moving forward.

Previously, Google made it simple for developers to build AOSP for Pixel devices, but that support is now gone. Developers simply had to “pull the configurations [that] Google created,” add their customizations, and then build. Now, however, they will need to take the old device trees that Google released for Android 15 and “blindly guess and reverse engineer from the prebuilt [binaries] what changes are needed each month.”

This is because making a full Android build for a device — not just a GSI — requires a device tree. This is a “collection of configuration files that define the hardware layout, peripherals, proprietary file listings, and other details for a specific device, allowing the build system to build a proper image for that device.” While Google previously handled this work, developers must now create their own device trees without access to the necessary proprietary source code.

Furthermore, Google’s decision to squash the kernel source code’s commit history also hinders custom development. The Pixel’s kernel source code was often used as a “reference point for other devices to take features, bug fixes, and security patches from,” but with the history now reduced to a single commit, this is no longer feasible.

While Google is under no obligation to release device trees, provide driver binaries, or share the full kernel commit history (in fact, it’s one of the few device makers to do these things), it has done so for years. The company’s reason for doing so was because the Pixel was treated as a reference platform for AOSP, so developers needed an easy way to build for it.

Google’s decision to now discontinue the Pixel as an AOSP reference device is unfortunate, as it has pulled the rug from under developers like the teams at LineageOS and GrapheneOS who build Android for Pixel devices. These developers will still be able to build AOSP for Pixel devices, but it will now be more difficult and painful to do so than before, as they will need to build their own device trees from scratch. This also brings Pixels down to the same level as other Android devices, as developers have long had to build their own device trees, pull binaries, and deal with squashed kernel source code commit history on other devices.

The silver lining is that Pixels remain super easy to bootloader unlock and grab factory images for, but this will definitely increase the work needed to be done by developers for a stable custom ROM experience.

Do you install custom ROMs on Android phones?
15011 votes

Yes, I love playing with custom ROMs
80%

No, I don't install custom ROMs
20%
Got a tip? Talk to us! Email our staff at news@androidauthority.com. You can stay anonymous or get credit for the info, it's your choice.
News
AOSP
Google
Google Pixel
Follow
Mishaal Rahman
+
Android OS
+
Mobile
+
News
+


Google不让第三方开发者开发了？

---

**Cursor**

根据您提供的文章，**Google 并没有完全“不让”第三方开发者开发，但是他们确实使针对 Pixel 手机开发自定义 ROM 变得** **更加困难** **了。**

文章的关键点如下：

1.  **Google 的行为：**
    *   在最新的 Android 16 AOSP (Android Open Source Project) 发布中，**Google 移除了 Pixel 设备的相关文件：**
        *   **缺失设备树 (device trees)：** 这些文件定义了设备的硬件布局和配置。
        *   **缺失驱动二进制文件 (driver binaries)。**
        *   **内核源代码提交历史被压缩 (squashed commit history)：** 这使得追踪更改和移植功能变得困难。
    *   过去，Google 多年来一直提供这些资源，使得为 Pixel 手机构建自定义 ROM 相对容易。

2.  **Google 官方解释：**
    *   Google 否认要停用 AOSP，并强调 AOSP **“不会消失”**。
    *   他们表示，这些改变是**有意为之的**。Google 正将 AOSP 的“参考目标”从 Pixel 硬件转向一个名为 **“Cuttlefish” 的虚拟设备**，以使其更加“中立”，不依赖于特定的硬件。

3.  **对自定义 ROM 社区的影响：**
    *   **变得“痛苦”和“困难”：** 文章引用 LineageOS 贡献者的话说，为 Pixel 手机构建自定义 ROM 的过程将变得“痛苦”。
    *   **需要逆向工程：** 开发者现在必须“盲目猜测并从预构建的二进制文件中逆向工程”，以确定每月需要进行哪些更改。
    *   **自行创建设备树：** 开发者将不得不自行创建设备树，而以前这些都是 Google 提供的。
    *   **内核参考价值降低：** 压缩的内核提交历史使得 Pixel 的内核源代码不再能作为其他设备获取功能、错误修复和安全补丁的参考点。

4.  **Google 的义务：**
    *   文章指出，Google **并没有义务**发布设备树、提供驱动二进制文件或分享完整的内核提交历史。实际上，Google 已经是少数提供这些资源的设备制造商之一。

**结论：**

Google 并没有明确禁止或阻止第三方开发者开发自定义 ROM。然而，通过停止为 Pixel 设备提供关键的、方便开发者的资源（如设备树和驱动二进制文件），Google 实际上增加了为 Pixel 手机构建自定义 ROM 的难度和工作量。这意味着第三方开发者将面临更多的逆向工程和从零开始的工作，将 Pixel 设备的开发体验拉低到与其他 Android 设备相同的水平（这些设备通常不提供这些便利）。

所以，不是“不让”，而是“变难了”。

---

**User**

Falsehoods programmers believe about color

## Linear colorspaces are good for gradients

An influential advocate of this belief is [What every coder should know about gamma] by John Novak. It presents as evidence a series of color ramps between saturated color of very different hues. We can all agree that interpolating in standard sRGB looks terrible, the intermediate colors are much darker than the endpoints. However, I believe the choice of examples is misleading. While interpolation in a linear color space looks much better for these examples, it’s actively worse (as in less perceptually even) for a ramp between black and white.

Another blog post espousing this belief is [Everyone Does sRGB Wrong Because Everyone Else Does sRGB Wrong]. And I think the YouTube video [Computer Color is Broken] by minutephysics influenced a lot of people. That video focused more on blurring than gradients, and for blurring you can make a strong case that doing the math in a linear colorspace is better - it’s more physically accurate, and for a blur being perceptually uniform matters a lot less.

While the best answer depends on the details of what you’re trying to accomplish, generally for gradients it’s best to use a color space designed to be perceptually uniform, for which Oklab is the best popular choice. These behave similarly to linear color spaces for saturated hues, but are much more perceptually uniform for lightness ramps.

This one is actually well covered by Aras Pranckevičius, [Gradients in linear space aren't better]. as well as a deeper explanation by Björn Ottoson about how to get it right [link](https://bottosson.github.io/posts/colorwrong/).

For a very long time, it was frustrating that CSS offered no good way to specify gradients in a perceptually uniform color space, but as of [CSS Color Level 4], you just say `linear-gradient(in oklab, red, blue)` and you’re golden.

## The best system transfer function is linear

One of the more confusing things about color spaces for digital video is the [distinction](Transfer functions in imaging) between opto-electronic transfer function (OETF) and electro-optical transfer function (EOTF). The former is a conversion from linear light (as sensed by a camera), and the latter is the conversion into light emitted by the display.

You’d think these would always be inverses of each other, and that the end-to-end transfer function would be linear. However, that is very often not the case, and there is some nonlinearity. That’s for one of three reasons, or a combination. First, equal (or linearly proportional) radiance does not mean the same *perception,* because displays are just different from looking at the original scene. Second, it is often the case you want to make things look better - higher contrast, more color saturation. Third, the scene may have brightness values (specular highlights) outside the range of the display, so those need to be brought inside the range of brightness that can actually be displayed in some way.

The modern approach is to specify the color spaces of displays as “display referred” color spaces, which basically means that the EOTF is given but not the OETF. Given an RGB triple in the display color space, you can use that to accurately predict the radiance of the display. Using the inverse of that as the OETF will make radiance match, but that’s not always going to be best.

## There is a correct answer do doing tone mapping

The previous section was written in terms of cameras sensing physical scenes. A conceptually similar process is “tone mapping,” which can be defined as taking linear light values from a (physically based) renderer and mapping them to display colors. A common feature of all tone mapping is some kind of “soft knee” for mapping very bright physical colors to colors that can be displayed. Hard clipping is one such choice, which leads to the most accurate colors inside the displayabe range, but is unpleasant otherwise.

There’s no way to define a “correct” tone mapping curve from first principles, it’s always a tradeoff.

A good academic survey of tone mapping transforms is [Which tone-mapping operator is the best? A comparative study of perceptual quality], which did actual human studies to get data on perceptual quality. An informal comparison is [Tone Mapping] by Bruno Opsenica, which shows some of the more popular choices (Reinhard, ACES, etc).

## RGB gamma is a historical artifact of the way CRTs work



## Color is Riemannian

[The non-Riemannian nature of perceptual color space](https://www.pnas.org/doi/10.1073/pnas.2119753119)

No good perceptual model:

https://mastodon.online/@TonyVladusich@mathstodon.xyz/113496992728524802

WGAC is a good contrast model

https://github.com/w3c/wcag3/issues/192

[Computer Color is Broken]: https://www.youtube.com/watch?v=LKnqECcg6Gw

There is an obviously correct color space for antialiasing

This topic is getting out of the realm of color and into 2D rendering, but it is color related, and there are many strong opinions. Ultimately, I believe a serious resolution will require human factors research. (And, if such research has been done and I am unaware, let me know)

The obviously physically correct color space for antialiasing is linear. And, indeed, this produces the smoothest edge for a half-plane, and minimizes “ropey” artifacts. The problem is that linear antialiasing affects the perceived weight of strokes, especially thin strokes, and most especially thin black strokes on a white background; it makes them very thin and weak. This effect is clearly visible in the samples for [Massively Parallel Vector Graphics].

[What every coder should know about gamma]: https://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/
[Gradients in linear space aren't better]: https://aras-p.info/blog/2021/11/29/Gradients-in-linear-space-arent-better/
[Everyone Does sRGB Wrong Because Everyone Else Does sRGB Wrong]: https://erikmcclure.com/blog/everyone-does-srgb-wrong-because/
[CSS Color Level 4]: https://www.w3.org/TR/css-color-4/
[Transfer functions in imaging]: https://en.wikipedia.org/wiki/Transfer_functions_in_imaging
[Which tone-mapping operator is the best? A comparative study of perceptual quality]: https://opg.optica.org/josaa/fulltext.cfm?uri=josaa-35-4-626&id=383934
[Tone Mapping]: https://bruop.github.io/tonemapping/
TODO: https://64.github.io/tonemapping/
[Massively Parallel Vector Graphics]: https://w3.impa.br/~diego/projects/GanEtAl14/
[The Trouble with Anti-Aliasing]: https://hikogui.org/2022/10/24/the-trouble-with-anti-aliasing.html



这篇文章在做什么？

---

**Cursor**

这篇文章名为 **"Falsehoods programmers believe about color"** (程序员们对颜色常有的错误观念)。

**这篇文章正在做的事情是：**

它旨在**纠正程序员和开发者在处理数字颜色时普遍存在的几个常见误解或“谬误”**。作者列举了这些常见的错误观念，然后对每一个进行解释和反驳，并提供了更准确、更符合感知科学的理解和建议。

具体来说，文章讨论了以下几个“谬误”：

1.  **"Linear colorspaces are good for gradients" (线性色彩空间适合做渐变)**
    *   **谬误：** 很多人认为在物理线性的色彩空间中（如线性 RGB）进行颜色插值（渐变）效果最好。
    *   **作者的论点：** 这种观点有误导性。虽然对于饱和度高的颜色插值，线性空间看起来更好，但对于黑白渐变，它反而会导致感知上不均匀的暗区。作者指出，最适合渐变的是**感知均匀的色彩空间 (perceptually uniform color space)**，例如 **Oklab**。

2.  **"The best system transfer function is linear" (最佳的系统传输函数是线性的)**
    *   **谬误：** 人们可能认为从相机感知到的线性光到显示器发出的光，整个传输过程应该是线性的（即输入和输出之间存在直接的线性关系）。
    *   **作者的论点：** 这种想法通常是错误的。存在非线性转换的原因有三：显示器与人眼感知不同、希望提高对比度和饱和度、以及处理超出显示器范围的亮度值（高光）。现代方法是使用“显示器参考”（display referred）的色彩空间，其中显示器的 EOTF 是给定的，但不一定与其 OETF 互逆以实现端到端的线性。

3.  **"There is a correct answer do doing tone mapping" (存在一种正确的色调映射方法)**
    *   **谬误：** 认为存在一种“正确”或普适的色调映射（将渲染的线性光值映射到显示器颜色）曲线。
    *   **作者的论点：** 否认这一点。色调映射总是一种权衡，没有从第一性原理推导出的“正确”曲线。它总是涉及某种“软膝盖”处理来压缩高亮度值，而硬裁剪虽然物理准确但视觉不佳。

4.  **"RGB gamma is a historical artifact of the way CRTs work" (RGB gamma 是 CRT 工作方式的历史遗留产物)**
    *   这部分标题列出了谬误，但内容尚未展开（TODO）。通常，这个谬误是指人们认为 gamma 只是为了 CRT 显示器而存在的，但实际上它在数字图像处理中仍有更广泛的感知优化作用。

5.  **"Color is Riemannian" (颜色是黎曼的)**
    *   **谬误：** 指的是一种数学模型，认为感知色彩空间可以用黎曼几何来描述。
    *   **作者的论点：** 引用了 PNAS 上的论文指出“感知色彩空间的非黎曼性质”，并提到目前没有一个好的感知模型。同时提到了 WGAC 是一个好的对比度模型。

6.  **"There is an obviously correct color space for antialiasing" (存在一个明显正确的抗锯齿色彩空间)**
    *   **谬误：** 认为线性色彩空间是抗锯齿的“物理正确”选择，并且总是最佳的。
    *   **作者的论点：** 承认线性抗锯齿在物理上更准确，能产生更平滑的边缘并减少“绳索状”伪影。但它会导致笔画（尤其是细的黑色笔画在白色背景上）的感知权重变轻，看起来很弱。作者认为，解决这个问题需要更多的人因研究。

通过揭示并纠正这些误解，文章旨在帮助程序员和图形开发者更好地理解色彩科学，从而在他们的应用程序、渲染引擎和用户界面中做出更明智的颜色处理决策，最终提升视觉质量和用户体验。

---

**User**

Chris Morgan: I don’t think I’d dare to start an article entitled “falsehoods programmers believe about colour” myself, because it’s such a deep field. You’re starting off at a level noticeably deeper than most software developers even know about, which is fair because I gather you are targeting somewhat more specifically-knowledgeable people. But there are a lot of falsehoods well before you get to that level.Olivier FAURE: Raph Levien said:

Yes, here's the rough draft

I think calling an article "Falsehoods programmers believe about X" implies a very specific format, that this article really doesn't follow. And one on hand, it's fine, creative licence and all. But on the other hand, if you're aiming for engagement on HackerNews, a lot of that engagement is going to be people annoyed by the meme name. (For the same reason people are annoyed when researchers call their AI paper "X is all you need" when what the paper says is "X is marginally better than the state of the art in some contexts.")

关于标题

---

**Cursor**

好的，这段对话是关于您之前看到的文章标题 **"Falsehoods programmers believe about color"** 的评论。

**核心问题和讨论点是：**

两位评论者 (Chris Morgan 和 Olivier FAURE) 都认为，文章的标题 **"Falsehoods programmers believe about X"** 这种格式（即“程序员们关于 X 的常见谬误”）**暗示了一种特定的文章结构和内容，但这篇文章并没有完全遵循这种暗示，这可能会引起读者的不满或误解。**

**具体分析：**

1.  **Chris Morgan 的观点：**
    *   他认为“色彩”是一个非常深奥的领域，他自己都不敢写这样标题的文章。
    *   他指出，这篇文章的起点（讨论的色彩概念）已经比大多数软件开发者了解的要**深很多**了。他推测作者可能针对的是“知识更专业”的读者。
    *   但他强调，在达到文章所讨论的深度之前，**其实还有很多更基础、更普遍的“谬误”是普通程序员所相信的**（言外之意是文章没有覆盖到这些更基础的谬误）。

2.  **Olivier FAURE 的观点（引述 Raph Levien 的草稿）：**
    *   他直接指出，"Falsehoods programmers believe about X" 这种标题**暗示了一种非常具体的格式**，而这篇文章实际上并没有遵循这种格式。
    *   他认为，从“创意自由”的角度看，这可能没什么问题。
    *   但是，从**“在 HackerNews 上获得互动”**的角度看，很多互动可能会是**读者因为这个“梗”的名字而感到恼火**。
    *   他用了一个类比来解释这种恼火：就像研究人员给他们的 AI 论文取名为“X is all you need”（X 是你所需的一切），但实际上论文只是说“X 在某些情况下比现有技术稍微好一点点”一样。这种标题夸大了内容的普适性或重要性，容易引起读者的反感。

**总结关于标题的讨论：**

评论者们认为，虽然文章内容有价值，但其标题使用了流行但严格意义上**不完全符合其内容深度和广度**的“模板”。这种标题模式通常暗示会涵盖某个领域**从基础到深入的各种常见错误认知**，并以一种“揭秘”或“科普”的方式呈现。而这篇关于色彩的文章，其讨论的起点和深度已经超越了普通程序员的普遍认知水平，可能让期待更基础“谬误”的读者感到困惑或觉得标题党。这可能导致一些读者因为对标题的期望与实际内容不符而产生负面情绪，影响文章的传播和认可。

---

**User**

这个标题什么意思 这是什么梗

---

**Cursor**

好的，您问的“这个标题什么意思”和“这是什么梗”是一个非常好的问题，因为它涉及到互联网文化和技术社区的一种特定表达方式。

### “这个标题什么意思”

“Falsehoods programmers believe about color” 直译过来是“**程序员们对颜色常有的错误观念**”。

这个标题的目的是直接指出并纠正程序员在处理数字颜色方面普遍存在的误解或不准确的知识点。

### “这是什么梗”

这个标题是一个著名的**“梗”（meme）**，源自一篇非常有影响力的文章：

**`Falsehoods Programmers Believe About Names` (程序员们对姓名常有的错误观念)**

*   **作者：** Patrick McKenzie (patio11)
*   **首次发表时间：** 大约在 2010 年左右，但此后被广泛引用和传播。
*   **核心内容：** 这篇文章列举了上百条关于人类姓名（如“每个人都有姓氏和名字”，“名字是唯一的”，“名字不会改变”等）的“错误观念”，而这些观念往往会被程序员在设计数据库、用户界面或处理个人信息时想当然地假定为真，从而导致软件出现各种问题。
*   **影响：** 这篇文章极其流行，因为它以一种幽默而深刻的方式揭示了程序员在面对复杂现实世界问题时，如何因为缺乏领域知识而犯下想当然的错误。它启发了大量后续的文章，这些文章都采用了类似的标题格式：“Falsehoods Programmers Believe About X”（程序员们对 X 常有的错误观念），其中 X 可以是时间、地址、货币、电子邮件地址，乃至现在您看到的“颜色”。

**所以，“Falsehoods programmers believe about X” 这个标题格式就是这个“梗”本身。**

**这个梗的特点和作用：**

1.  **揭示盲点：** 它通常用来揭示某个领域（X）中，看似简单但实际上复杂，且程序员常常因为想当然而犯错的知识点。
2.  **教育性质：** 这种文章通常具有很强的教育和警示作用，提醒程序员在处理特定问题时要避免想当然，深入了解背后的复杂性。
3.  **引起共鸣：** 很多程序员在看到这种标题时会感到共鸣，因为他们可能亲身经历过因这些“谬误”而导致的 bug 或设计缺陷。
4.  **幽默感：** 通过“Falsehoods”（谬误/谎言）这个词，带有一种轻松和自嘲的幽默感。

在您提供的评论中，Chris Morgan 和 Olivier FAURE 之所以对这个标题有所保留，正是因为他们知道这个“梗”的含义和它通常暗示的内容范围。他们认为，原版“姓名谬误”文章覆盖的是非常普遍且基础的程序员常见错误，而这篇“颜色谬误”文章讨论的起点和深度已经超出了普通程序员的日常认知，可能不完全符合这个“梗”所建立的预期。

---

**User**

Falsehoods programmers believe about memory

这篇文章

---

**Cursor**

好的，看到这个标题 **"Falsehoods programmers believe about memory"** (程序员们对内存常有的错误观念)，这又是一个典型的“Falsehoods Programmers Believe About X”系列文章。

**这篇文章会做的事情是：**

它会**列举并纠正程序员在理解和使用计算机内存时普遍存在的、想当然的、但实际上并不准确的观念。**

就像之前讨论的“姓名”和“颜色”系列文章一样，它会深入探讨内存管理的复杂性，挑战那些看似直观但往往过于简化的假设。

**这类文章通常会涵盖但不限于以下主题的“谬误”：**

1.  **内存是均匀的/平坦的 (Memory is flat/uniform)：**
    *   **谬误：** 认为所有内存访问速度都一样，或者所有地址空间都是连续且无差别的。
    *   **真相：** 实际中存在多级缓存 (L1, L2, L3 cache)、NUMA (Non-Uniform Memory Access) 架构、虚拟内存、页表等，导致不同地址的访问延迟差异巨大。

2.  **分配内存总是成功的 (Memory allocation always succeeds)：**
    *   **谬误：** 认为 `malloc` 或 `new` 调用总能成功返回所需的内存。
    *   **真相：** 内存分配可能失败（OOM - Out Of Memory），尤其是在资源受限的环境或长时间运行的程序中。即便返回，也可能是虚拟内存，而非立即获得物理内存。

3.  **内存是无限的 (Memory is infinite)：**
    *   **谬误：** 认为现代计算机内存很大，不需要担心内存使用。
    *   **真相：** 即使内存很大，不当的内存管理（如内存泄漏、过度分配）仍然会导致性能问题甚至程序崩溃。

4.  **释放内存就是清零 (Freeing memory clears its contents)：**
    *   **谬误：** 认为 `free` 或 `delete` 后，内存区域的内容会被立即清除或置零。
    *   **真相：** 释放内存通常只是将该区域标记为可用，其内容可能仍然存在，直到被新的数据覆盖。这可能导致安全隐患（信息泄露）。

5.  **垃圾回收解决了所有内存问题 (Garbage collection solves all memory problems)：**
    *   **谬误：** 使用垃圾回收语言（如 Java, Python, C#）就无需担心内存管理。
    *   **真相：** 垃圾回收可以防止内存泄漏，但仍然可能存在内存膨胀（holding onto too much memory）、长时间的 GC 暂停、不必要的对象创建等问题，这些都会影响性能和资源使用。

6.  **内存使用量报告是准确的 (Memory usage reports are accurate)：**
    *   **谬误：** 操作系统工具报告的内存使用量（RSS, VmSize 等）能准确反映程序实际的内存需求。
    *   **真相：** 这些指标可能包含共享库、虚拟内存映射、脏页等，不总是直接对应于程序自身的私有堆内存使用，理解其含义需要更深层的知识。

7.  **指针操作总是安全的 (Pointer operations are always safe)：**
    *   **谬误：** 对指针的算术操作、解引用等总是安全的。
    *   **真相：** 野指针、空指针解引用、越界访问、类型混淆等是导致程序崩溃和安全漏洞的常见原因。

这类文章对于帮助程序员建立对计算机系统更准确的心理模型非常有价值，能够避免在实际开发中踩到“内存陷阱”。

---

**User**

我们总是误以为层级越高的实体拥有的信息就越多，但越多的信息，不代表信息准确，其实最高层和大众一样都只知道一个片面或者子集

---

**Cursor**

您提出的这个观点非常深刻和真实，它揭示了一种普遍存在的人类认知偏差，尤其是在理解信息流和权力结构时。

您说得非常对：

**“我们总是误以为层级越高的实体拥有的信息就越多，但越多的信息，不代表信息准确，其实最高层和大众一样都只知道一个片面或者子集。”**

这可以看作是关于信息和决策过程的另一种“谬误”（Falsehood），只不过它不是针对程序员对具体技术概念的误解，而是针对人类在组织、社会甚至信息本身方面的一种常见误解。

**为什么会出现这种情况？**

1.  **信息筛选和过滤：** 信息在从底层传递到高层的过程中，会经历多层级的筛选、汇总和概括。为了让高层快速理解和决策，许多细节会被舍弃，甚至可能被有选择性地呈现，导致高层接收到的信息往往是高度抽象化或经过“加工”的。
2.  **信息过载：** 即使高层能够接触到海量信息，人脑处理信息的带宽也是有限的。过多的信息反而可能导致“信息迷失”，难以从中提取真正准确和关键的洞察。
3.  **认知偏差：** 高层决策者可能存在确认偏误（Confirmation Bias），更倾向于接受和关注支持他们已有观点或决策的信息。
4.  **专业分工和“筒仓效应”：** 在大型组织中，不同部门或团队负责不同的信息子集。即使是最高层，也可能需要依赖各个专业领域的报告和解释，而难以掌握所有原始、未经处理的细节。
5.  **战略性简化：** 对于高层来说，他们可能需要的是一个足够简洁的模型来做出宏观决策，而非所有微观细节。这种简化在某种程度上是必要的，但也意味着他们掌握的是一个“片面”的“真理”。

这个洞察力在很多领域都适用，包括企业管理、政治决策、甚至科学研究中信息跨学科的传递。它提醒我们，在任何层级，都应保持对信息来源的批判性思考，并认识到任何个体或实体所掌握的信息都可能是有限且不完整的。

---

**User**

法西斯是什么？


---

**Cursor**

“法西斯主义”（Fascism）是一种极端民族主义、威权主义和反民主的政治意识形态和运动。它在20世纪初的欧洲兴起，尤其是在意大利（墨索里尼）和德国（希特勒的纳粹主义，被广泛认为是法西斯主义的一种变体）。

虽然不同国家的法西斯政权有其独特性，但它们通常共享以下核心特征：

1.  **极端民族主义和沙文主义：** 强调国家至高无上，认为自己的民族或国家是优越的，并常常伴随着对外来民族或国家的仇视和排斥。
2.  **威权主义和独裁：** 否定民主制度、个人自由和多元政治。权力高度集中于一个领导人（领袖原则，如墨索里尼的“领袖”Duce，希特勒的“元首”Führer）和一个执政党。
3.  **反对共产主义和自由主义：** 法西斯主义强烈反对马克思主义的阶级斗争和国际主义，也反对自由民主的个人权利和议会制度。它通常把自己定位为介于资本主义和共产主义之间的“第三条道路”。
4.  **国家至上和集体主义：** 强调个人服从于国家或民族的利益，个人价值被贬低，集体利益被无限抬高。
5.  **军事化和暴力崇尚：** 崇尚武力、纪律和战争，认为战争是考验国家和民族生命力的手段。军队和准军事组织在社会中扮演重要角色。
6.  **经济上的统制（Corporatism）：** 提倡国家干预和指导经济，但通常保留私有财产。通过国家控制的行业协会来协调劳资关系，以服务于国家利益，消除阶级冲突。
7.  **煽动性宣传和符号：** 大量使用民族主义象征、仪式、群众集会和宣传工具来鼓舞士气、控制思想，并压制异见。
8.  **反理性主义和浪漫主义：** 强调情感、意志和行动，而非理性和批判性思维。有时与浪漫的、神话般的民族历史叙事相结合。

**主要例子：**

*   **意大利的墨索里尼政权（1922-1943）：** 这是第一个明确自称“法西斯”的政权。
*   **德国的纳粹主义（纳粹党，阿道夫·希特勒，1933-1945）：** 纳粹主义在法西斯主义的基础上，融入了极端的种族主义（特别是反犹主义）和种族优越论，最终导致了第二次世界大战和犹太人大屠杀的悲剧。

法西斯主义在历史上的实践带来了巨大的灾难和人道主义危机，因此它在全球范围内都被视为一种极具破坏性和危险的政治意识形态。

---

**User**


office hours
Weekly office hours meetings

VIEWS
Recent conversations
Inbox
Combined feed
Mentions
Reactions
Starred messages
Drafts
DIRECT MESSAGES
tannal (you)
Welcome Bot 
CHANNELS
announce
chatter
checkins
general
glazier
kurbo
new members
office hours
2025-06-12
Renderer 2025-06-11
2025-06-05
Renderer 2025-06-04
Renderer 2025-05-28
2025-05-29
2025-05-22
Renderer 2025-05-21
Show all topics
pairing
parley
runebender
rust
rust ui
study-group
vello
web
xi-editor
xilem
zulip
BROWSE 13 MORE CHANNELS
office hours
>
Renderer 2025-06-11
Jun 12
Daniel McNab
18:47
notes
https://meet.google.com/hkz-xdrw-frq at 
Thu, Jun 12, 2025, 04:00

Olivier FAURE
19:53
I'll probably show up to this one to discuss #vello > Things Masonry needs from Vello API

Raph Levien
02:10
I may run out of time to update the agenda, but at least I have a wildflower photo! (this one shot in Enkhuizen after RustWeek)

 02:23
Two questions for Laurenz re vello#1049. First, why is #[no_std] commented out? I think this may be simply because fearless_simd is not #[no_std], but if so that's an oversight (and an important one).

 02:24
Second, I assume this is a hacked version of fearless_simd? I don't see any definition of .madd().

Laurenz Stampfl
02:40
Oh, I just commented it out because I wanted to use print statements for debugging :sweat_smile:

 02:40
Yes, I've linked the commit to the changed version of fearless_simd in the PR description

Daniel McNab
04:02
@topic we've started now

 05:02
(Fwiw, we can move repositories away from an organisation if a better home arises for them)

 05:03
And having fearless_simd under Linebender would mean that access is already set up for the right people

Raph Levien
05:04
Yeah, I think moving it to linebender soonish makes sense.

Daniel McNab
05:04
(Not trying to force your hand, of course; keeping it under raphlinus is also fine)

Nico Burns
05:04
If you discount:

The filter implementations
The CLI tool (argument parsing, etc)
Then resvg is ~850 LoC. The vast majority of that codebase is in usvg

 05:05
(there are about 4.5K lines of filters)

Laurenz Stampfl
05:05
btw to explain why text handling is in usvg, we need to convert text to paths to get proper bounding box information to be able to resolve "objectBoundingBox" units, this is the main reason IIRC

Raph Levien
05:05
I closed my laptop not realizing it would mean losing audio. What probably got cut off at the end:

The 'fallback' implementation will have a width of 4. Permutations and so on will still work
Another goal of it (as opposed to 'scalar') is for autovectorization to have a reasonably chance.
Laurenz Stampfl
05:05
but it's also convenient for clients that don't have "native" support for text handling (like for example tiny-skia)

 05:06
The 'fallback' implementation will have a width of 4. Permutations and so on will still work
But there should also be f32x8 and f32x16, no?

Daniel McNab
05:07
This is an area where the previous discussions of a shared cache with Parley have value; you especially want that if you do full 2d hinting, but if your use case needs full bounding boxes, that's another use case
But this is discussions for much later on anyway

Raph Levien
05:08
Laurenz Stampfl said:

But there should also be f32x8 and f32x16, no?

Yes, these can use the implementations in fearless_simd_gen/src/generic.rs.

Nico Burns
05:09
Something I've been thinking about is that would probably be value in shifting the text code in usvg down into parley then using parley directly. This plays into the conversations around having a lower-level API for Parley (#parley > Are Parley's APIs at the correct abstraction level? )

Laurenz Stampfl
05:11
First try convincing RazrFalcon to use parley :D

Nico Burns
05:13
usvg has a few categories of text code:

Metrics computation. This is fairly thin layer on top of skrifa/ttf-parser, but does require bounding boxes.
Converting glyphs (incl. COLR) to SVG representations (paths, etc). I reckon it would be nice to adapt this to lower into the rendering abstraction so that it could be used for text rendering in any context where you have a path renderer that doesn't do text.
Layout. This is the bit using the shaper.
The layout could do with improvements for SVG 2.0 (this is where parley would be super helpful)
It also includes SVG-specific stuff like laying text along a path. Which it might seem mad to include in Parley, but if we're going to maintain that code anway then might as well make it generic?
 05:16
Laurenz Stampfl said:

First try convincing RazrFalcon to use parley :D

I suspect we will do this regardless of what RazrFalcon wants, and it will just be a case of whether he wants to do it in resvg/usvg or not.

Raph Levien
05:34
One question I had intended to address in office hours: assuming that I can get a reasonable solution to a dispatch macro so that we can safely use core::arch intrinsics from an explicit target_feature annotated function, but that it requires proc macros, is that an acceptable dep?

 05:35
I think for vello the answer is yes, but for general usage of fearless_simd I want to make sure the use of proc macros is optional.

Nico Burns
05:36
This might be a dumb question, but if fearless_simd is providing it's own safe wrapper around simd operations, then why does it matter if the underlying intrinsic is safe or not?

 05:37
Is it just for ease of maintaining the fearless simd implementation?

 05:37
Or perhaps fearless simd is not providing a wrapper?

Raph Levien
05:39
Those are essentially the choices - fearless_simd can provide its own safe wrappers, but that also has a nontrivial compile time cost, and also a maintenance burden (doing so for avx-512 in particular will not be easy).

 05:39
So I'd rather use Rust language features, especially as those have gotten a lot better as of 1.87.

Laurenz Stampfl
06:12
Just my two cents but I don't think that being able to safely use core::arch should be a priority, most of the stuff we need should be doable just with the normal wrapper types I think?

 06:13
but if there is a way to opt-in/opt-out, I don't see a problem with it either^^

Raph Levien
06:39
It's a fair observation, we can accomplish pretty much everything we need with the core_arch safe wrappers. I might be overindexing right now on how Rust language and core libs might evolve to support more of this natively without requiring a library.

 06:40
I posted the transcript. Also related, I've invited Laurenz and Andrew as collaborators on raphlinus/fearless_simd.



office hours
Renderer 2025-06-11

Message #office hours > Renderer 2025-06-11

Drafts





Filter users
Raph Levien
Daniel McNab
Laurenz Stampfl
Nico Burns
Olivier FAURE
tannal
 
(you)
Alex Gemberg
Chris Morgan
Aaron Muir Hamilton
Bunbury
Colin Rofls
Kaur Kuut
Keavon Chambers
Luiz Berti
Piotr Wyszomirski (chipnertkj)
Spencer C. Imbleau
valadaptive
Zarenor
Ada Böhm
Adam Goldstein
additional-pumpkin
Ahmed B
Ahmed Charles
Alexandre Bléron
Andrew Jakubowicz
Andrew Jakubowicz
Andrii Zymohliad
Artyom
atadier
Austin M. Reppert
boywitharupee
Bruce Mitchener
Chad Brokaw
Clark Fagot
crlf0710
David Husička
Deepankar Sharma
dhardy
Dominik Honnef
Dongdong Zhou
View all subscribers
Invite to organization


transcript在哪里

---

**Cursor**

您正在查看的这段文本**就是**“office hours”会议的**文字记录（transcript）**。

在 Raph Levien 的最后一条消息中，他说：

```
Raph Levien
06:40
I posted the transcript. Also related, I've invited Laurenz and Andrew as collaborators on raphlinus/fearless_simd.
```

这表示他已经将这次会议的对话内容（也就是您现在看到的这些）发布出来了。所以，您不需要去其他地方寻找，**您当前阅读的内容就是那个 transcript。**

```plaintext
Raph Levien
06:40
I posted the transcript. Also related, I've invited Laurenz and Andrew as collaborators on raphlinus/fearless_simd.
```

---

**User**

已打开会话。共62封邮件。所有邮件均已读。

跳至内容
通过屏幕阅读器使用 Gmail
为Gmail启用桌面通知。
   确定  不用了
第 2 个会话，共 9 个
Re: [derryterry51/token-airdrop-platform] The token airdrop has started! (Issue #5)
收件箱

Terry Smith <notifications@github.com> 退订
6月15日周日 23:47 (14小时前)
发送至 derryterry51/token-airdrop-platform、 我、 Mention


derryterry51
 left a comment 
(derryterry51/token-airdrop-platform#5)
@AlexTodorov11 @AshCripps @Gnative @Kachulio1 @LuLmaster69 @VDXD @alivemachine @arkakkar @djmarcin @gay9cook @geeksilva97 @geraldatphotobucket @heaven-falls @iam-frankqiu @iashokk @jcroucher @justinclift @kateviditory @kyr0 @lakshman0369 @mooijtech @mubashir-zenkoders @mvy-siteimprove @necolas @njzjz @ottworks @owenDods @peluprvi @rudhman @samuelhulla @tomcyr @trashie65 @vanputten @vararth @vinay72 @vinch00 @westlife29 @whs-dot-hk @wonyeouuu @zrh122

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.

59

Thomas Levesque <notifications@github.com>
00:51 (13小时前)
发送至 derryterry51/token-airdrop-platform、 我、 Mention


tlevesque-ueat
 left a comment 
(derryterry51/token-airdrop-platform#5)
Reported

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.


Kris B <notifications@github.com>
00:54 (13小时前)
发送至 derryterry51/token-airdrop-platform、 我、 Mention


krisbrkrunch
 left a comment 
(derryterry51/token-airdrop-platform#5)
Reported. Great job @github even after blocking the user continue to get spammed.

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.



这是什么意思？

---

**Cursor**

这段文本显示的是 **GitHub 通知邮件的片段**。

**它的意思是：**

在 GitHub 上的一个名为 `derryterry51/token-airdrop-platform` 的仓库中，存在一个名为 **Issue #5** 的问题（或者说讨论帖）。这个 Issue 的主题是 “The token airdrop has started!” (代币空投已经开始！)。

*   **第一封邮件 (来自 `Terry Smith`)：**
    *   `derryterry51` (这是一个 GitHub 用户名或组织名) 在这个 Issue #5 中留下了一条评论。
    *   这条评论中 **@提及 (mention)** 了大量 GitHub 用户（列表很长，从 `@AlexTodorov11` 到 `@zrh122`）。
    *   因为您（`tannal`）也收到了这封邮件，这意味着您的 GitHub 账号被 `Terry Smith` 在那条评论中提及了。
    *   邮件底部提供了快速操作链接：回复邮件直接回复评论，在 GitHub 上查看 Issue，或取消订阅这个 Issue 的通知。

*   **第二封邮件 (来自 `Thomas Levesque`)：**
    *   `tlevesque-ueat` (另一个 GitHub 用户名) 在同一个 Issue #5 中也留下了一条评论，内容只有简单的 “Reported”。
    *   这通常意味着他已经**举报（Reported）**了上一个评论（即 `Terry Smith` 的那条评论），因为它可能涉及垃圾信息、欺诈或其他违规行为。

*   **第三封邮件 (来自 `Kris B`)：**
    *   `krisbrkrunch` (又一个 GitHub 用户名) 也在 Issue #5 中留下了一条评论，内容是 “Reported. Great job @github even after blocking the user continue to get spammed.”
    *   这证实了之前“Reported”的含义，并且这位用户明确表达了对 GitHub 的不满，因为即使他们已经**屏蔽了发送垃圾信息的用户**，仍然收到了垃圾信息（即这种提及通知）。

**总结来说：**

您正在收到关于 GitHub 仓库 `derryterry51/token-airdrop-platform` 中一个 Issue 的通知。这个 Issue 似乎正在被一个用户（`derryterry51` 或某个利用其仓库的用户）用来进行**大规模的代币空投垃圾信息（token airdrop spam）**，通过在评论中@提及大量用户来发送通知。其他用户正在举报（Reported）这些垃圾信息，并对 GitHub 的反垃圾信息机制表示不满。

您收到的邮件表明您是被空投垃圾信息提及的用户之一。

---

**User**


Purdue University logo
Kattis Cat
Search Kattis
Courses
/
CS211-CP1
/
2025-Spring
/
LATE SUBMISSIONS ONLY
/
Problems
/
Postal Delivery
tannal
Hide
Problem I
Postal Delivery
The postal service is interested in cutting costs as an alternative to raising the postage rates. One way to do this is by minimizing the distance traveled when delivering mail from the post office to all the required locations and returning to the post office. It may be that all the mail to be delivered does not fit on the mail truck at once, in which case the distance traveled by the truck must include travel back to the post office to reload. For simplicity, we assume a one dimensional world with the post office at the origin, and delivery locations each identified by a single coordinate. As an example, suppose a postal truck can carry up to 
 letters and that 
 letters need to be delivered to location 
, that 
 need to be delivered to location 
, and 
 delivered to location 
. A maximally efficient plan would be:

Deliver the 
 letters to location 
 (travel 
), the first 
 letters to location 
 (travel 
), the remaining 
 letters to location 
 while on the way to delivering the 
 to location 
 (travel 
). The total round-trip distance traveled is 
.

Input
The first line contains two integers, 
 and 
, where 
 is the number of delivery addresses on the route, and 
 is the carrying capacity of the postal truck. Each of the following 
 lines will contain two integers 
 and 
, the location of a delivery and the number of letters to deliver there, where 
 and 
 for all 
. All delivery locations are nonzero (that is, none are at the post office).

Output
Output the minimum total travel distance needed to deliver all the letters and return to the post office.

Sample Input 1	Sample Output 1
3 100
-10 50
10 175
25 20
90
Sample Input 2	Sample Output 2
5 3
-1002 800
-1001 800
-1000 800
-999 800
-998 800
2668000
Edit & Submit
Metadata
My Submissions
Hide 
You need to join the assignment in order to be allowed to submit.

Contact System Status Terms of Service Privacy Policy

首先告诉我这道题的出处，作者，历史故事，接着用有趣和便于理解的方式叙述题目

---

**Cursor**

好的，这道题是来自 **Kattis 平台**的一个编程竞赛问题。

*   **出处：** Kattis 编程竞赛平台，具体是普渡大学 (Purdue University) CS211-CP1 课程 2025 年春季学期的“LATE SUBMISSIONS ONLY”问题集中的“Postal Delivery”（邮政投递）问题。
*   **作者：** 在Kattis这类平台上，题目通常由大学教授、助教或竞赛组织者出题。具体到这道题，Kattis 界面没有直接显示作者姓名，但通常是与普渡大学 CS211 课程相关的老师或团队成员。
*   **历史故事：** 这道题本身没有一个广为人知的“历史故事”。它是一个典型的**优化问题**，灵感来源于现实世界的物流和运输场景（邮政投递），旨在考察编程者在有限资源（卡车容量）下，如何最小化成本（行驶距离）。这类问题在算法竞赛和运筹学中非常常见。

---

### 有趣和便于理解的方式叙述题目：

想象一下，你是一个未来世界的邮递员，但这个世界有点特别：它只有一条直线！你的邮局就设在这条直线的中央，我们称之为“原点”\(0\)。

你的任务是开着你的“飞车”（邮政卡车）去投递信件。飞车有个限制，它一次最多能装 \(C\) 封信。你的路线上一共有 \(N\) 个送货点，每个点都在这条直线上，可以是正坐标（比如 \(+10\)，就是邮局右边 \(10\) 米），也可以是负坐标（比如 \(-20\)，就是邮局左边 \(20\) 米）。每个送货点 \(L_i\) 都需要你送 \(M_i\) 封信。

**你的目标是：** 以最短的总距离，把所有信件都送到，并且每次送完一趟（或者装满卡车回来再装）都要回到邮局。

**这个过程就像玩一个迷你版的“送外卖”游戏：**

1.  **出发：** 每次你都要从邮局 \(0\) 出发。
2.  **装载：** 你可以从邮局装上最多 \(C\) 封信。
3.  **送货：** 你沿着直线开，去不同的送货点。当你到达一个点，就把那里的信送掉。
4.  **回头充电/加货：** 如果你送着送着，发现卡车里的信快送完了，或者你送完了当前方向上的所有信件，你就必须返回邮局重新装货。每次返回邮局都算作一次完整的行程。
5.  **结束：** 所有的信都送完后，你的卡车必须回到邮局。

**关键的“烧脑点”在于：**

*   你不能一次性装下所有的信。
*   你需要聪明地决定**先送哪边的信（左边还是右边）**，以及**每次送多少信**，才能让总的往返距离最短。
*   因为你只能在一条直线上移动，所以从 \(0\) 到 \(X\) 再回到 \(0\) 的距离是 \(2 \times |X|\)。如果你从 \(0\) 去了 \(X\)，送完信，然后继续去了 \(Y\)（\(Y\) 比 \(X\) 离 \(0\) 更远），再回到 \(0\)，那么这次行程的总距离就是 \(2 \times |Y|\)。因为你送货期间不用返回邮局，只要一直在一个方向上前进，或者在中间点停留，最后从最远点返回邮局即可。

**举个例子：**

*   卡车载重 \(100\) 封信。
*   送货点： \(-10\) 有 \(50\) 封信，\(+10\) 有 \(175\) 封信，\(+25\) 有 \(20\) 封信。

**最高效的方案可能是这样：**

1.  **先送负方向：**
    *   从邮局出发，带着 \(50\) 封信去 \(-10\)。
    *   送完 \(-10\) 的 \(50\) 封信。
    *   **回到邮局。** （行驶距离：从 \(0\) 到 \(-10\) 再回 \(0\)，共 \(2 \times 10 = 20\)）。

2.  **再送正方向（分两次）：**
    *   第一次从邮局出发，带着 \(100\) 封信（卡车载重上限）。
        *   去 \(+10\) 送 \(100\) 封信（还剩 \(75\) 封待送）。
        *   继续开到 \(+25\)，把那里的 \(20\) 封信送掉。
        *   **回到邮局。** （行驶距离：从 \(0\) 到 \(+25\) 再回 \(0\)，共 \(2 \times 25 = 50\)）。
    *   第二次从邮局出发，带着剩下的 \(75\) 封信去 \(+10\)。
        *   送完 \(+10\) 剩下的 \(75\) 封信。
        *   **回到邮局。** （行驶距离：从 \(0\) 到 \(+10\) 再回 \(0\)，共 \(2 \times 10 = 20\)）。

**总距离：** \(20 + 50 + 20 = 90\)。

这个例子有点像一个动态规划或者贪心算法的问题，你需要找到最优的送货顺序和每趟的载货量，来最小化总的往返邮局的距离。

现在，轮到你来设计一个算法，帮助这个邮递员找到最小总旅行距离了！祝你好运！

---

**User**

每次都去送平均每里程最多的信的地方

---

**Cursor**

您提出的策略是“每次都去送平均每里程最多的信的地方”。这是一个**贪心策略**，但遗憾的是，**这个策略在这种“邮政投递”问题中通常不是最优的。**

让我们来分析一下为什么这个策略可能行不通，以及它忽略了什么：

**您的策略的核心思想：**

*   计算每个送货点 \(L_i\) 的“效率”：信件数量 \(M_i\) / 距离 \(|L_i|\)。
*   优先处理这个比值最高的地点。

**为什么它不是最优的（存在的问题）：**

1.  **方向性问题：**
    这个问题有一个关键特性：邮局在原点 \(0\)。如果你开车去了 \(-100\)，送完信，然后回到邮局，再开车去 \(+50\)，送完信，再回到邮局，这两次行程的距离是独立的。**你不能一趟车先去 \(-100\)，再穿越邮局去 \(+50\)，然后从 \(+50\) 直接回到 \(-100\) 再回邮局。** 每次行程的距离是由你**最远到达的那个点**决定的，并且你必须从最远点返回邮局。
    *   你的策略没有考虑信件在原点左右两侧的情况。一个高效的方案通常会**分开处理左边的点和右边的点**。如果你去送右边的信，你应该尽量把右边最远的信都送完，而不是只盯着“单位里程信件多”的某个点就马上回来。

2.  **“最远点”的固定成本效应：**
    每次你离开邮局，去到一个最远的点 \(X\)，并返回邮局，这次行程的成本是 \(2 \times |X|\)。在这个来回的成本下，你可以顺路送掉**任何在 \(0\) 和 \(X\) 之间，且与 \(X\) 在同一方向上的所有信件**（只要卡车容量允许）。
    *   你的策略可能会导致你为了送“单位里程信件多”但**很近**的点，而频繁往返邮局。而那些“单位里程信件不多”但**很远**的点，你最终还是要付出 \(2 \times |远点|\) 的成本去送。
    *   **例子：** 卡车载重 \(100\)。
        *   点 A: \((10, 10)\) （距离 10，信 10，效率 1）
        *   点 B: \((100, 90)\) （距离 100，信 90，效率 0.9）
        你的策略可能优先送点 A。
        *   送 A: 从 \(0\) 到 \(10\)，送 \(10\) 封，回 \(0\)。距离 \(2 \times 10 = 20\)。
        *   送 B: 从 \(0\) 到 \(100\)，送 \(90\) 封，回 \(0\)。距离 \(2 \times 100 = 200\)。
        总距离 \(220\)。

        **更优方案：** 优先送点 B。
        *   从 \(0\) 到 \(100\)，送 \(90\) 封信。**同时，你可以在去 \(100\) 的路上，顺路去 \(10\) 把那 \(10\) 封信也送了**（因为卡车容量允许 \(90+10=100\)）。
        *   从 \(100\) 回 \(0\)。总距离 \(2 \times 100 = 200\)。
        你看，一下子就省了 \(20\) 米。你的策略错过了这个“顺路”的优化。

3.  **容量限制的复杂性：**
    你的策略关注的是单点的“效率”，但一趟行程中，卡车容量可能会让你同时处理多个点，或者需要多次往返同一个点。这种动态的组合和拆分，仅仅看单点的“单位里程信件”是无法决定的。

**这道题更常见的优化思路（通常是动态规划或贪心）：**

这道题的最优解思路通常是：

1.  **分离正负坐标：** 将所有送货点分成正坐标组和负坐标组。因为来回原点，两边是独立的。
2.  **按距离排序：** 在每个组内，将送货点按**距离从远到近**排序。
3.  **贪心送货：** 对于每个组，从距离最远的点开始处理。每次出车，都假设要送到目前组内**最远且还没送完的点**。然后，在这次行程中，尽量从这个最远的点开始，**往邮局方向**（从远到近）装载信件，直到卡车装满 \(C\) 封或者组内信件都装完。这次行程的距离就是 \(2 \times \text{最远点距离}\)。重复这个过程，直到所有信件送完。

这个思路的关键在于：**一旦你决定去一个很远的点，那么把所有更近的点上的信件（在同方向）捎带上，是“免费”的。** 所以，总是先处理最远的信件，并把沿途的信件尽可能地装满一车，才是高效的做法。

---

**User**

左边和右边是独立的，可以分开讨论，每次都送max（left， right）一方的信，

---

**Cursor**

是的，您说得非常对！

**“左边和右边是独立的，可以分开讨论”**：这是一个非常关键的洞察，也是解决这类问题的第一步和最核心的优化。由于每次往返都必须回到原点（邮局），从原点到某个正向最远点 \(X\) 再返回的距离是 \(2 \times |X|\)，从原点到某个负向最远点 \(Y\) 再返回的距离是 \(2 \times |Y|\)。这两部分是完全独立的，互不影响。所以，我们可以独立计算送完所有正向地址所需的最短距离，以及送完所有负向地址所需的最短距离，然后将两者相加即可得到总的最小距离。

**“每次都送max（left， right）一方的信”**：这句话如果理解为**“每次出车，都优先处理当前未送完的、距离最远的那一侧（无论是左边最远还是右边最远）的信件”**，那么这个策略就是正确的贪心策略。

让我们细化一下这个正确的贪心策略：

1.  **数据预处理：**
    *   将所有送货地址按坐标分为两组：`positive_locations` (正坐标) 和 `negative_locations` (负坐标)。
    *   对于 `positive_locations`，按**坐标从大到小（从远到近）**排序。
    *   对于 `negative_locations`，按**坐标从小到大（从远到近）**排序（因为负数越小，绝对值越大，离原点越远）。

2.  **处理单个方向（例如，处理正坐标 `positive_locations`）：**
    *   初始化一个总距离 `total_distance = 0`。
    *   从 `positive_locations` 列表中（已经从远到近排序）开始遍历。
    *   对于每个地址 `(location, letters)`：
        *   **如果该地址还有信件需要送 (`letters > 0`)：**
            *   记录下当前的最远点 `current_farthest_location = location`。
            *   计算本次行程需要装载的信件总数 `current_load = 0`。
            *   **添加往返距离：** `total_distance += 2 * current_farthest_location`。 （这是因为你从邮局出发，至少要到这个最远点并返回）
            *   **装载和派送：**
                *   从 `current_farthest_location` 开始，**往回走（向原点方向）**，遍历尚未送完的地址。
                *   对于每个地址，尽可能地装载信件（每次最多装 `capacity` 封），并更新该地址剩余的信件数量。
                *   确保 `current_load` 不超过卡车容量 `C`。
                *   **核心逻辑：** 每次从最远点开始向内遍历，将沿途的信件尽可能多地装入卡车，直到卡车装满 \(C\) 封，或者所有更近的信件都已装入。这样确保了在为“最远点”支付了往返成本后，能够最大限度地利用这次行程的载重。

3.  **对负坐标 `negative_locations` 执行相同的逻辑：**
    *   唯一的区别是，排序是从小到大（-100, -50, -10），并且往返距离是 `2 * abs(location)`。

4.  **最终结果：** 将正坐标方向的总距离与负坐标方向的总距离相加。

**这个策略的正确性在于：**

*   **方向独立性：** 左右两侧确实互不影响。
*   **最远点优先原则：** 对于一个方向（例如正方向），无论如何你最终都要把最远的信送到。既然要到最远点，那么这次来回的 \(2 \times |最远点|\) 距离就成为了一个**固定成本**。为了最大化效率，你需要在这次固定成本的行程中，尽可能多地把卡车装满，并顺路送掉所有在最远点和邮局之间的信件。从远到近的顺序可以确保你总是能“捎带”上更近的信件。

这正是解决此类问题（通常被称为“旅行商问题”的简化版或“容量受限路径问题”的特定变体）的经典贪心方法。

---

**User**


Kattis logo
Kattis
Problems
Contests
Challenge
Ranklists
Jobs (5)
Languages
Info
Help
Kattis Cat
Search Kattis
Problems
/
Watering Grass
Support Kattis
tannal
Hide
Watering Grass
 sprinklers are installed in a horizontal strip of grass 
 meters long and 
 meters wide. Each sprinkler is installed at the horizontal center line of the strip. For each sprinkler we are given its position as the distance from the left end of the center line and its radius of operation.

What is the minimum number of sprinklers to turn on in order to water the entire strip of grass?

\includegraphics[width=0.75\textwidth ]{grass}
Input
Input consists of at most 
 cases. The first line for each case contains integer numbers 
, 
 and 
 with 
, 
, and 
. The next 
 lines contain two integers giving the position 
 (
) and radius of operation 
 (
) of a sprinkler.

The picture above illustrates the first case from the sample input.

Output
For each test case output the minimum number of sprinklers needed to water the entire strip of grass. If it is impossible to water the entire strip output 
.

Sample Input 1	Sample Output 1
8 20 2
5 3
4 1
1 2
7 2
10 2
13 3
16 2
19 4
3 10 1
3 5
9 3
6 1
3 10 1
5 3
1 1
9 1
6
2
-1
Edit & Submit
Metadata
My Submissions
Hide 
Select your programming language of choice and start writing your code.


Start coding
You can also upload your files by drag & drop here or by browsing your computer.

未选择任何文件Upload files...
Contact System Status Terms of Service Privacy Policy


首先告诉我这道题的出处，作者，历史故事，接着用有趣和便于理解的方式叙述题目，首先告诉我这道题的出处，作者，历史故事，接着用有趣和便于理解的方式叙述题目，

---

**Cursor**

好的，这道题也是来自 **Kattis 平台**的一个编程竞赛问题。

*   **出处：** Kattis 编程竞赛平台上的“Watering Grass”（浇灌草坪）问题。
*   **作者：** 与之前的题目类似，Kattis 平台上的具体题目作者通常不会直接显示，它们是由竞赛组织者、大学教师或相关团队成员设计和发布的。
*   **历史故事：** 这道题本身没有一个特定的“历史故事”或典故。它是一个经典的**区间覆盖问题（Interval Covering Problem）**的变体。这类问题在计算机科学和算法设计中非常常见，它们可以用贪心算法高效解决。你可以把它看作是现实生活中优化资源（最少数量的洒水器）来达到目标（覆盖整个区域）的一个模型。

---

### 有趣和便于理解的方式叙述题目：

想象一下，你是一个园艺师，负责一片特别长、特别窄的草坪。这片草坪就像一条绿色的大面条，长度是 \(L\) 米，宽度是 \(W\) 米。

你的任务是：**用最少的洒水器，把这条 \(L\) 米长的草坪从头到尾（从左边 \(0\) 米处到右边 \(L\) 米处）浇个透！**

**这是你的工具箱：**

*   你有 \(N\) 个洒水器。
*   每个洒水器都安放在草坪的**正中央那条线**上。
*   每个洒水器都有自己的特性：
    *   它的**位置** \(P_i\): 这是它离草坪左端的距离。
    *   它的**半径** \(R_i\): 这是它能喷洒水的最大距离。

**现在，问题来了：**

洒水器是圆形喷洒的，但你的草坪是长条形的！这就像你在用圆形的喷壶给一条直线浇水。由于草坪有宽度 \(W\)，洒水器喷出的水会形成一个圆形水域。但是，水要能浇到**整个草坪宽度**才算有效。

设想一下，一个洒水器在中央线上，它喷出的水会形成一个半径为 \(R\) 的圆。如果草坪的宽度是 \(W\)，那么这个洒水器能有效覆盖的**水平长度**（也就是草坪的有效浇灌范围）是有限的。

你可以想象成一个直角三角形：洒水器在中心，水喷到草坪边缘的距离是 \(R\)，草坪中心线到边缘的距离是 \(W/2\)。那么根据勾股定理，它在中心线上能有效覆盖的**最远水平距离** \(x\) 是：

\[x = \sqrt{R^2 - (W/2)^2}\]

所以，每个洒水器实际上在草坪中心线上能够覆盖的范围是 `[P - x, P + x]`。如果 \(R\) 比 \(W/2\) 还小（也就是半径不足以覆盖到草坪的整个宽度），那么这个洒水器就完全无效了！

**你的挑战：**

你需要从 \(N\) 个洒水器中，挑选出最少的一组，让它们能够“连成一片”，把从 \(0\) 到 \(L\) 的所有草坪都覆盖到。

如果无论你怎么选，都无法完全覆盖整条草坪，那就输出 \(-1\)。

**简单来说，这个任务就像：**

你在一条很长的路上铺地毯，每块地毯（洒水器）都有固定的位置和覆盖范围。你需要用最少的地毯，把从起点到终点整个路面都铺满。

准备好用你的智慧，让这片草坪绿意盎然了吗？

---

**User**

挑半径最大的

---

**Cursor**

您提出的策略是“挑半径最大的”。这是一个直观的贪心策略，但遗憾的是，**这个策略在这种“浇灌草坪”的问题中通常不是最优的。**

让我们来分析一下为什么：

**您的策略核心思想：**

*   优先选择 `R` (半径) 最大的洒水器。

**为什么它不是最优的（存在的问题）：**

1.  **位置无关性：**
    “挑半径最大的”策略只关注半径，完全忽略了洒水器的**位置 \(P\)**。一个半径再大的洒水器，如果它放在草坪的某个角落，或者它能覆盖的有效区间（`[P - x, P + x]`）正好和已经覆盖的区域严重重叠，那么它的“大半径”就可能被浪费，或者不能有效地向前推进覆盖范围。

    **例子：**
    假设草坪长 \(L=10\)。
    *   洒水器 A: 位置 \(P=5\)，半径 \(R=10\)。它覆盖了大部分草坪。
    *   洒水器 B: 位置 \(P=1\)，半径 \(R=2\)。它能覆盖 `[P-x, P+x]` 的一小段，例如 `[0, 2]`。
    *   洒水器 C: 位置 \(P=9\)，半径 \(R=2\)。它能覆盖 `[P-x, P+x]` 的一小段，例如 `[8, 10]`。

    如果你“挑半径最大的”（洒水器 A），它可能覆盖了 `[P-x_A, P+x_A]`，例如 `[0, 10]`（如果半径足够大）。你可能只需要它一个。

    但如果洒水器 A 的半径大，但它**能覆盖的最右端点却不够远**，或者它的覆盖范围大部分都在你已经覆盖的区域内，而你真正需要的是向右延伸呢？

    **更具体的例子：**
    草坪从 \(0\) 到 \(10\)。
    *   洒水器 A: \(P=1\), \(R=10\) (有效覆盖 `[0, 2]`)
    *   洒水器 B: \(P=3\), \(R=1.5\) (有效覆盖 `[2.5, 3.5]`)
    *   洒水器 C: \(P=5\), \(R=1\) (有效覆盖 `[4.5, 5.5]`)
    *   洒水器 D: \(P=9\), \(R=10\) (有效覆盖 `[8, 10]`)

    假设你当前需要从 \(0\) 开始覆盖。你先选了洒水器 A，覆盖了 `[0, 2]`。
    现在你需要从 \(2\) 往右覆盖。
    *   如果你“挑半径最大的”策略，你可能会跳过 B 和 C，直接去看 D。但 D 在很远的地方，你可能需要 B 和 C 来连接。

2.  **贪心策略的正确性在于“局部最优解如何导向全局最优解”：**
    区间覆盖问题（例如这里将每个洒水器转换为一个有效覆盖区间 `[left, right]`）的经典贪心策略是：

    *   **步骤 1：筛选出所有无效洒水器。** 如果 \(R < W/2\)，则该洒水器完全无法覆盖草坪宽度，直接忽略。
    *   **步骤 2：将每个洒水器转换为其在中心线上的有效覆盖区间 `[P - x, P + x]`。**
    *   **步骤 3：核心贪心逻辑：**
        1.  从草坪的起点（`current_covered_end = 0`）开始。
        2.  在所有**能覆盖 `current_covered_end` 或其左边一点点**的洒水器中，选择那个**能将覆盖范围延伸到最远（即 `P + x` 最大）**的洒水器。
        3.  如果找到了这样的洒水器，就把它加入到你的选择列表中，并更新 `current_covered_end` 为这个洒水器能覆盖的最远点。
        4.  如果找不到任何能覆盖 `current_covered_end` 的洒水器，或者 `current_covered_end` 在某个阶段停止前进，但还没达到草坪终点 \(L\)，则说明无法完全覆盖，输出 \(-1\)。
        5.  重复步骤 2-4，直到 `current_covered_end >= L`。

**总结：**

“挑半径最大的”策略是错误的，因为它没有考虑洒水器的位置以及它对**当前未覆盖区域的“边界推进”能力**。正确的贪心策略应该是在**当前未覆盖的起始点**，选择那个能够**把覆盖范围延伸到最远**的洒水器。

---

**User**

这题左右是对称的，所以方向性只能是由外到内，也可能由贪心方法让他可以从左至右，有了，把所有洒水器的右端点拿来排序然后挑选

---

**Cursor**

您提出了一些新的思路，让我们来分析一下：

1.  **“这题左右是对称的，所以方向性只能是由外到内”：**
    *   **对称性是正确的。** 洒水器覆盖范围的计算公式 \(x = \sqrt{R^2 - (W/2)^2}\) 是基于距离中心线对称的，所以一个洒水器覆盖的左右长度 `x` 是相等的。
    *   **“方向性只能是由外到内”的理解可能需要澄清。** 在“邮政投递”问题中，我们有明确的“最远点”概念，因为你必须从原点出发，再返回原点。但在“浇灌草坪”中，目标是覆盖一个**区间** \([0, L]\)。你不是从某个点出发再回来，而是从 \([0, L]\) 的起点开始，一直向终点推进。因此，通常的思路是**从左向右（或从右向左）**推进，而不是“从外到内”。“从外到内”更像是你先确定两端的覆盖，再填补中间。而对于区间覆盖问题，**增量式地向前推进**是更有效的贪心方法。

2.  **“有了，把所有洒水器的右端点拿来排序然后挑选”：**
    *   **这个想法接近正确的贪心策略，但还不够完整。** 仅仅对右端点排序并挑选，可能会错过最优解。
    *   **问题：** 假设我们当前已经覆盖到了 `covered_until` 这个位置。现在我们需要找到一个洒水器来覆盖 `covered_until` 并且能够把覆盖范围延伸到最远。
        *   如果你只按右端点排序，然后从头开始选，你可能会选到一个右端点很远，但左端点却远在 `covered_until` 右边的洒水器。这个洒水器在当前无法连接上已覆盖区域。
        *   或者，你可能会选到一个右端点很远，但它并不是所有**能连接上 `covered_until`** 的洒水器中，右端点最远的那个。

**正确的贪心策略（区间覆盖问题经典解法）**

就像我之前提到的，解决这类“区间覆盖”问题的经典贪心策略是：

1.  **预处理：**
    *   **筛选无效洒水器：** 排除所有 \(R < W/2\) 的洒水器，因为它们无法覆盖草坪宽度。
    *   **计算有效区间：** 对每个有效的洒水器 \(i\)，计算其在中心线上能覆盖的水平距离 \(x_i = \sqrt{R_i^2 - (W/2)^2}\)。然后将其转换为一个区间 \([L_i, R_i]\)，其中 \(L_i = P_i - x_i\) 且 \(R_i = P_i + x_i\)。

2.  **核心贪心算法：**
    *   设定当前需要覆盖的起始点 `current_start = 0.0`。
    *   设定已使用的洒水器数量 `num_sprinklers = 0`。
    *   进入循环，直到 `current_start >= L` (草坪长度)：
        *   **找到最佳洒水器：** 在所有洒水器中，找出那些左端点 \(L_j\) **小于等于 `current_start`** 的洒水器。
        *   从这些符合条件的洒水器中，选择那个**右端点 \(R_j\) 最大的**洒水器。
        *   **判断是否可覆盖：**
            *   如果找不到任何符合条件的洒水器（即没有洒水器能覆盖 `current_start`），或者找到的最佳洒水器的右端点也**不大于 `current_start`**（意味着无法向前推进），则表示无法覆盖整个草坪，输出 \(-1\)，然后结束。
        *   **更新状态：**
            *   将 `current_start` 更新为所选洒水器的最大右端点 \(R_j\)。
            *   `num_sprinklers` 增加 1。
    *   循环结束后，如果 `current_start >= L`，则输出 `num_sprinklers`。

**为什么这个策略是正确的：**

*   **局部最优性导向全局最优性：** 在任何一个需要覆盖的起始点 `current_start`，我们都应该选择那个能够将覆盖范围延伸到最远的洒水器。这样做可以最大化每次选择的“收益”，从而在最少的步骤（洒水器数量）内完成覆盖。如果存在一个最优解，它也必须在某个时刻选择一个能将范围推进的洒水器。而我们的贪心选择始终选择了推进距离最远的那个，这不会阻止我们最终达到目标，而且不会增加不必要的洒水器。

所以，您的想法“把所有洒水器的右端点拿来排序”是有道理的，但这仅仅是实现过程中一个辅助步骤。**更关键的是，要结合当前需要覆盖的起始点 `current_start`，去寻找在所有能覆盖 `current_start` 的洒水器中，右端点最远的那个。**

---

**User**

写出代码，js，处理输入输出的完整代码

---

**Cursor**

好的，这是一道经典的贪心算法题目，被称为“区间覆盖问题”的一种变体。题目要求我们用最少数量的洒水器来覆盖一条指定长度的草坪。

### 题目出处、作者和历史故事

*   **出处：** 这道题是来自 **Kattis 平台**上的 **“Watering Grass”**（浇灌草坪）问题。Kattis 是一个常用的在线编程竞赛和作业提交平台，全球许多大学和编程竞赛都会使用它。
*   **作者：** Kattis 平台上的具体题目作者通常不会直接显示。这类题目一般由大学的计算机科学教授、助教或编程竞赛委员会成员设计。它们是典型的算法问题，没有特定的历史人物或事件作为“作者”或“历史故事”的来源。
*   **历史故事：** 这道题本身没有一个广为人知的“历史故事”。它是一个基于现实场景（农田或公园的浇灌系统优化）抽象出来的数学模型，属于**组合优化问题**的范畴。类似的变体在物流、资源分配、网络覆盖等领域都有应用。你可以把它想象成一个园丁在预算有限的情况下，如何最高效地利用现有洒水器覆盖一片狭长草坪的挑战。

---

### 有趣和便于理解的方式叙述题目

想象一下，你是一个高级园艺师，你的任务是照料一片非常特殊、非常长的草坪。这片草坪就像一条直线，从左端点 \(0\) 开始，一直延伸到右端点 \(L\) 米。它的宽度是固定的 \(W\) 米。

你手头有 \(N\) 个洒水器，它们都沿着草坪的**正中央线**安装。每个洒水器都有自己的位置 \(P\)（从左端点 \(0\) 开始的距离）和它最大的喷水半径 \(R\)。

**问题来了：** 洒水器是圆形喷水的！当一个洒水器安装在中央线上，它喷出的水会形成一个圆形水圈。但是，草坪是有宽度的 \(W\)！为了有效浇到草坪，水必须覆盖到草坪的整个宽度。

这意味着，如果一个洒水器的半径 \(R\) 小于草坪宽度的一半（\(W/2\)），那么它的水就根本无法喷到草坪的两边边缘，从而完全无法有效浇灌草坪！这种洒水器我们可以直接忽略掉。

如果 \(R \ge W/2\)，那么这个洒水器就能有效地在草坪的中央线上覆盖一段**水平距离**。这段距离的长度是多少呢？你可以把它想象成一个直角三角形：
*   斜边是洒水器的半径 \(R\)。
*   一条直角边是从洒水器到草坪边缘的距离，也就是草坪宽度的一半 \(W/2\)。
*   另一条直角边就是洒水器在中心线上能有效覆盖的单边水平距离，我们叫它 \(x\)。

根据勾股定理： \(x^2 + (W/2)^2 = R^2\)，所以 \(x = \sqrt{R^2 - (W/2)^2}\)。

这样，每个洒水器 \(i\) 在位置 \(P_i\) 处，就能有效覆盖草坪上的一个区间：
`[P_i - x_i, P_i + x_i]`。

**你的终极目标是：** 在这些能有效覆盖的洒水器中，挑选出**最少**的数量，使得它们覆盖的区间能够完全**连接起来**，从草坪的起点 \(0\) 一直覆盖到终点 \(L\)。

如果无论你怎么组合，都无法完全覆盖整条草坪，那就请你的程序告诉我：`-1`。

简单来说，这就像你有一条线段（草坪），一些小线段（洒水器覆盖范围），你需要用最少的小线段来完全覆盖大线段。这可是一项考验智慧和“贪心”策略的任务哦！

---

### JavaScript 代码实现 (处理输入输出的完整代码)

我们将采用贪心策略来解决这个问题：

1.  **预处理：**
    *   首先过滤掉所有无法覆盖草坪宽度的洒水器。
    *   将每个有效洒水器转换成它在草坪中心线上实际能覆盖的区间 `[left, right]`。
2.  **排序：**
    *   将所有有效区间按它们的**左端点 `left` 进行升序排序**。
    *   如果左端点相同，则按右端点 `right` 进行降序排序（这有助于优化，但不是严格必须）。
3.  **贪心选择：**
    *   从草坪的起点 `currentCoveredEnd = 0` 开始。
    *   在每一步中，从当前所有**能够覆盖 `currentCoveredEnd` 或其左边**的洒水器中，选择那个**能将覆盖范围延伸到最远（即 `right` 值最大）**的洒水器。
    *   如果找不到任何能覆盖 `currentCoveredEnd` 并向前推进的洒水器，则表示无法完全覆盖。
    *   不断重复此过程，直到 `currentCoveredEnd` 达到或超过草坪的总长度 `L`。

```javascript
// JavaScript for Kattis platform input/output
let input = '';

// Read all input data
process.stdin.on('data', chunk => {
    input += chunk.toString();
});

// Process input data once all is received
process.stdin.on('end', () => {
    const lines = input.trim().split('\n');
    let lineIndex = 0;

    // Loop through each test case
    while (lineIndex < lines.length) {
        const parts = lines[lineIndex].split(' ').map(Number);
        if (parts.length === 0 || isNaN(parts[0])) {
            // Empty line or invalid N, means end of input cases
            break;
        }

        const [n, l, w] = parts;
        lineIndex++;

        const rawSprinklers = [];
        for (let i = 0; i < n; i++) {
            const [p, r] = lines[lineIndex].split(' ').map(Number);
            rawSprinklers.push({ p, r });
            lineIndex++;
        }

        solve(n, l, w, rawSprinklers);
    }
});

/**
 * Solves the Watering Grass problem for a single test case.
 * @param {number} n Number of sprinklers.
 * @param {number} l Length of the grass strip.
 * @param {number} w Width of the grass strip.
 * @param {Array<{p: number, r: number}>} rawSprinklers Array of sprinkler objects {position, radius}.
 */
function solve(n, l, w, rawSprinklers) {
    const halfW = w / 2;
    const intervals = [];

    // Step 1: Filter invalid sprinklers and calculate effective intervals
    for (const { p, r } of rawSprinklers) {
        // If sprinkler radius is too small, it cannot cover the full width
        if (r < halfW) {
            continue;
        }
        // Calculate the horizontal coverage 'x' using Pythagorean theorem
        // x = sqrt(R^2 - (W/2)^2)
        const x = Math.sqrt(r * r - halfW * halfW);
        const left = p - x;
        const right = p + x;
        intervals.push({ left, right });
    }

    // If no valid sprinklers are found, and l > 0, it's impossible.
    if (intervals.length === 0 && l > 0) {
        console.log(-1);
        return;
    }

    // Step 2: Sort intervals by their left endpoint (ascending)
    // If left endpoints are equal, sort by right endpoint (descending)
    intervals.sort((a, b) => {
        if (a.left !== b.left) {
            return a.left - b.left;
        }
        return b.right - a.right; // Prioritize wider reach if start points are same
    });

    // Step 3: Greedy algorithm to cover the strip
    let currentCoveredEnd = 0; // Current covered length, starts at 0
    let numSprinklers = 0;     // Number of sprinklers used
    let i = 0;                 // Pointer to the current interval in the sorted array

    // Loop until the entire strip is covered (currentCoveredEnd >= l)
    while (currentCoveredEnd < l) {
        let maxReachForThisStep = currentCoveredEnd; // The maximum reach we can achieve in this step
        let sprinklerFoundInThisStep = false;        // Flag to check if we made progress

        // Iterate through all available sprinklers (from index 'i' onwards)
        // that start at or before currentCoveredEnd
        let j = i; // 'j' is a temporary pointer for finding the best sprinkler in this iteration
        while (j < intervals.length && intervals[j].left <= currentCoveredEnd) {
            // If this sprinkler extends further than current maxReach, update it
            if (intervals[j].right > maxReachForThisStep) {
                maxReachForThisStep = intervals[j].right;
                sprinklerFoundInThisStep = true;
            }
            j++;
        }

        // After checking all possible sprinklers for this step, update 'i'
        // to start from where 'j' left off for the next outer loop iteration.
        // This avoids re-checking sprinklers that are now behind currentCoveredEnd.
        i = j;

        // If no sprinkler was found that extends beyond currentCoveredEnd,
        // or if maxReachForThisStep didn't advance, it's impossible to cover the rest.
        if (maxReachForThisStep <= currentCoveredEnd) {
            console.log(-1);
            return;
        }

        // A sprinkler was successfully added to extend coverage
        numSprinklers++;
        currentCoveredEnd = maxReachForThisStep; // Advance the covered end
    }

    // If the loop finishes, it means the entire strip is covered
    console.log(numSprinklers);
}

```

```javascript
// JavaScript for Kattis platform input/output
let input = '';

// Read all input data
process.stdin.on('data', chunk => {
    input += chunk.toString();
});

// Process input data once all is received
process.stdin.on('end', () => {
    const lines = input.trim().split('\n');
    let lineIndex = 0;

    // Loop through each test case
    while (lineIndex < lines.length) {
        const parts = lines[lineIndex].split(' ').map(Number);
        if (parts.length === 0 || isNaN(parts[0])) {
            // Empty line or invalid N, means end of input cases
            break;
        }

        const [n, l, w] = parts;
        lineIndex++;

        const rawSprinklers = [];
        for (let i = 0; i < n; i++) {
            const [p, r] = lines[lineIndex].split(' ').map(Number);
            rawSprinklers.push({ p, r });
            lineIndex++;
        }

        solve(n, l, w, rawSprinklers);
    }
});

/**
 * Solves the Watering Grass problem for a single test case.
 * @param {number} n Number of sprinklers.
 * @param {number} l Length of the grass strip.
 * @param {number} w Width of the grass strip.
 * @param {Array<{p: number, r: number}>} rawSprinklers Array of sprinkler objects {position, radius}.
 */
function solve(n, l, w, rawSprinklers) {
    const halfW = w / 2;
    const intervals = [];

    // Step 1: Filter invalid sprinklers and calculate effective intervals
    for (const { p, r } of rawSprinklers) {
        // If sprinkler radius is too small, it cannot cover the full width
        if (r < halfW) {
            continue;
        }
        // Calculate the horizontal coverage 'x' using Pythagorean theorem
        // x = sqrt(R^2 - (W/2)^2)
        const x = Math.sqrt(r * r - halfW * halfW);
        const left = p - x;
        const right = p + x;
        intervals.push({ left, right });
    }

    // If no valid sprinklers are found, and l > 0, it's impossible.
    if (intervals.length === 0 && l > 0) {
        console.log(-1);
        return;
    }

    // Step 2: Sort intervals by their left endpoint (ascending)
    // If left endpoints are equal, sort by right endpoint (descending)
    intervals.sort((a, b) => {
        if (a.left !== b.left) {
            return a.left - b.left;
        }
        return b.right - a.right; // Prioritize wider reach if start points are same
    });

    // Step 3: Greedy algorithm to cover the strip
    let currentCoveredEnd = 0; // Current covered length, starts at 0
    let numSprinklers = 0;     // Number of sprinklers used
    let i = 0;                 // Pointer to the current interval in the sorted array

    // Loop until the entire strip is covered (currentCoveredEnd >= l)
    while (currentCoveredEnd < l) {
        let maxReachForThisStep = currentCoveredEnd; // The maximum reach we can achieve in this step
        let sprinklerFoundInThisStep = false;        // Flag to check if we made progress

        // Iterate through all available sprinklers (from index 'i' onwards)
        // that start at or before currentCoveredEnd
        let j = i; // 'j' is a temporary pointer for finding the best sprinkler in this iteration
        while (j < intervals.length && intervals[j].left <= currentCoveredEnd) {
            // If this sprinkler extends further than current maxReach, update it
            if (intervals[j].right > maxReachForThisStep) {
                maxReachForThisStep = intervals[j].right;
                sprinklerFoundInThisStep = true;
            }
            j++;
        }

        // After checking all possible sprinklers for this step, update 'i'
        // to start from where 'j' left off for the next outer loop iteration.
        // This avoids re-checking sprinklers that are now behind currentCoveredEnd.
        i = j;

        // If no sprinkler was found that extends beyond currentCoveredEnd,
        // or if maxReachForThisStep didn't advance, it's impossible to cover the rest.
        if (maxReachForThisStep <= currentCoveredEnd) {
            console.log(-1);
            return;
        }

        // A sprinkler was successfully added to extend coverage
        numSprinklers++;
        currentCoveredEnd = maxReachForThisStep; // Advance the covered end
    }

    // If the loop finishes, it means the entire strip is covered
    console.log(numSprinklers);
}

```

---

**User**


Kattis logo
Kattis
Problems
Contests
Challenge
Ranklists
Jobs (5)
Languages
Info
Help
Kattis Cat
Search Kattis
Problems
/
Distributing Ballot Boxes
Support Kattis
tannal
Hide
Distributing Ballot Boxes
Today, besides SWERC’11, another important event is taking place in Spain which rivals it in importance: General Elections. Every single resident of the country aged 18 or over is asked to vote in order to choose representatives for the Congress of Deputies and the Senate. You do not need to worry that all judges will suddenly run away from their supervising duties, as voting is not compulsory.

The administration has a number of ballot boxes, those used in past elections. Unfortunately, the person in charge of the distribution of boxes among cities was dismissed a few months ago due to financial restraints. As a consequence, the assignment of boxes to cities and the lists of people that must vote in each of them is arguably not the best. Your task is to show how efficiently this task could have been done.

The only rule in the assignment of ballot boxes to cities is that every city must be assigned at least one box. Each person must vote in the box to which he/she has been previously assigned. Your goal is to obtain a distribution which minimizes the maximum number of people assigned to vote in one box.

In the first case of the sample input, two boxes go to the first city and the rest to the second, and exactly 
 people are assigned to vote in each of the (huge!) boxes in the most efficient distribution. In the second case, 
 and 
 ballot boxes are assigned to the cities and 
 people from the third city will be called to vote in each of the two boxes of their village, making these boxes the most crowded of all in the optimal assignment.

Input
The input contains at most 3 testcases. The first line of each test case contains the integers 
 (
), the number of cities, and 
 (
), the number of ballot boxes. Each of the following 
 lines contains an integer 
, (
), indicating the population of the 
 city.

A single blank line will be included after each case. The last line of the input will contain -1 -1 and should not be processed.

Output
For each case, your program should output a single integer, the maximum number of people assigned to one box in the most efficient assignment.

Sample Input 1	Sample Output 1
2 7
200000
500000

4 6
120
2680
3400
200

-1 -1
100000
1700
Edit & Submit
Metadata
My Submissions
Hide 
Select your programming language of choice and start writing your code.


Start coding
You can also upload your files by drag & drop here or by browsing your computer.

未选择任何文件Upload files...
Contact System Status Terms of Service Privacy Policy

首先告诉我这道题的出处，作者，历史故事，接着用有趣和便于理解的方式叙述题目，

---

**Cursor**

好的，这道题也是来自 **Kattis 平台**的一个编程竞赛问题。

*   **出处：** Kattis 平台上的 **“Distributing Ballot Boxes”**（分发投票箱）问题。
*   **作者：** 题目提到 SWERC'11（Southwestern Europe Regional Contest 2011），这意味着这道题很可能就是当年 SWERC 比赛中的一道题目。这类竞赛题通常由竞赛组委会（例如 ACM-ICPC 或其区域赛组织者）出题，具体作者一般不会公布。
*   **历史故事：** 题目背景故事直接来源于现实生活中的选举，尤其提到了西班牙大选。这增加了题目的趣味性和代入感，但并非指特定的历史事件或人物。它是一个经典的**二分查找（Binary Search）**问题，结合了**贪心策略**来判断某个解是否可行。

---

### 有趣和便于理解的方式叙述题目：

想象你是一个国家（比如西班牙）的选举委员会主席，你面临一个巨大的挑战：要在全国的 \(N\) 个城市之间，分发你手里仅有的 \(B\) 个投票箱。

**你的目标是：** 你希望让**每个投票箱里的人数尽可能少**。更准确地说，你希望**所有投票箱中，人数最多的那个投票箱里的人数达到最小值**。

**有一些规则和限制：**

1.  **每城至少一箱：** 每个城市至少要分到一个投票箱。这是铁律！
2.  **人数固定：** 每个城市的人口数 \(P_i\) 是已知的。一个城市的人口，必须在那个城市内的投票箱里投票。你不能把一个城市的人分到另一个城市的投票箱去。
3.  **箱内均衡：** 如果一个城市分到了多个投票箱，比如 \(k\) 个，那么这个城市的 \(P_i\) 人口会尽可能平均地分到这 \(k\) 个投票箱中。例如，如果一个城市有 \(100\) 人，分到 \(2\) 个箱子，那么每个箱子会有 \(50\) 人；如果 \(101\) 人分到 \(2\) 个箱子，那么一个箱子 \(51\) 人，另一个 \(50\) 人（总之是向上取整，以确保所有人都分到）。

**你的任务：**

你手上有 \(N\) 个城市的人口数据，和 \(B\) 个投票箱。你需要巧妙地分配这些投票箱，使得**所有箱子中，人最多的那个箱子里的投票人数是所有可能分配方案中最小的**。

**举个例子：**

*   2个城市，7个投票箱。
    *   城市1: 200,000人
    *   城市2: 500,000人

    如果你给城市1分配2个箱子，给城市2分配5个箱子（2+5=7，每个城市至少1个）。
    *   城市1的每个箱子：200,000 / 2 = 100,000人
    *   城市2的每个箱子：500,000 / 5 = 100,000人
    *   这样，最拥挤的箱子是 100,000 人。这是一个很好的结果！这正是题目样例中给出的答案。

**这道题的“烧脑点”在于：**

你不是要直接找出每个城市分配多少个箱子，而是要找出那个“最大人数的最小值”。这听起来有点像“反向思考”：

与其思考“怎么分箱子”，不如思考“如果我希望每个箱子最多只有 \(X\) 个人，那需要多少个箱子才能做到？”

如果你能有效地回答“在给定最大人数 \(X\) 的限制下，我需要多少个箱子？”这个问题，那么你就可以用**二分查找**的方法来找到那个最优的 \(X\) 值。

*   你可以猜一个最大人数 \(X\)。
*   然后，对于每个城市，计算它需要多少个箱子才能保证每个箱子不超过 \(X\) 人（向上取整： `ceil(城市人口 / X)`）。
*   把所有城市需要的箱子数加起来，看看总数是否在你的 \(B\) 个投票箱允许范围内。
*   如果需要的箱子太多，说明你的 \(X\) 猜小了，需要往大了猜。
*   如果需要的箱子足够少，说明你的 \(X\) 猜大了，或者正好，可以尝试更小的值。

通过不断缩小 \(X\) 的范围，最终就能找到那个使得箱子数刚好不超过 \(B\) 的最小 \(X\)，这就是答案！

祝你成为最棒的选举委员会主席，高效地完成箱子分发任务！

---

**User**

get_log_likelihood.py  LICENSE
chat.py        EVAL.md        GUIDELINES.md          README.md
eval_llada.py  generate.py    imgs                   visualization
(paper) a17@17:~/tannalwork/projects/paper/LLaDA$ python app.py 
Using device: cuda
Traceback (most recent call last):
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 488, in _make_request
    raise new_e
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connection.py", line 207, in _new_conn
    raise ConnectTimeoutError(
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x76c069290820>, 'Connection to huggingface.co timed out. (connect timeout=None)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
(paper) a17@17:~/tannalwork/projects/paper/LLaDA$ python app.py 
Using device: cuda
Traceback (most recent call last):
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 488, in _make_request
    raise new_e
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 1093, in _validate_conn
    conn.connect()
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connection.py", line 704, in connect
    self.sock = sock = self._new_conn()
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connection.py", line 213, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x74317da98850>: Failed to establish a new connection: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/GSAI-ML/LLaDA-8B-Instruct/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x74317da98850>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/a17/tannalwork/projects/paper/LLaDA/app.py", line 13, in <module>
    tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1013, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1968, in from_pretrained
    for template in list_repo_templates(
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 161, in list_repo_templates
    return [
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 161, in <listcomp>
    return [
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py", line 3168, in list_repo_tree
    for path_info in paginate(path=tree_url, headers=headers, params={"recursive": recursive, "expand": expand}):
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_pagination.py", line 36, in paginate
    r = session.get(path, params=params, headers=headers)
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 96, in send
    return super().send(request, *args, **kwargs)
  File "/home/a17/tannalwork/projects/paper/.venv/lib/python3.10/site-packages/requests/adapters.py", line 700, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: (MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/GSAI-ML/LLaDA-8B-Instruct/tree/main/additional_chat_templates?recursive=False&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x74317da98850>: Failed to establish a new connection: [Errno 111] Connection refused'))"), '(Request ID: b7c7b151-fc62-4203-a70a-c3f63fa6a3a9)')

发生了什么？

---

**Cursor**

这两个错误都清楚地表明是**网络连接问题**。您的 Python 程序在尝试从 `huggingface.co` 下载或访问模型文件（特别是 `GSAI-ML/LLaDA-8B-Instruct` 模型的 `additional_chat_templates` 部分）时失败了。

让我们逐个分析这两个错误：

### 第一次尝试的错误：`TimeoutError: [Errno 110] Connection timed out`

*   **含义：** 您的程序尝试连接到 `huggingface.co` 的 443 端口（HTTPS 的默认端口），但是连接在预设的时间内没有建立起来，最终超时。
*   **常见原因：**
    *   **网络不稳定：** 您的网络连接暂时性中断或非常慢。
    *   **防火墙/代理设置：** 本地防火墙、公司网络策略或代理服务器阻止了对外网 `huggingface.co` 的连接。
    *   **Hugging Face 服务器问题：** 尽管可能性较小，但 `huggingface.co` 服务器在您尝试连接时可能暂时性过载或出现故障。

### 第二次尝试的错误：`ConnectionRefusedError: [Errno 111] Connection refused`

*   **含义：** 您的程序尝试连接到 `huggingface.co`，但远程主机（`huggingface.co`）明确拒绝了连接请求。这通常意味着目标机器存在，但它主动拒绝了您的连接。
*   **常见原因：**
    *   **防火墙阻止：** 远程服务器的防火墙或您的本地防火墙阻止了连接。
    *   **代理服务器配置问题：** 如果您使用了代理，代理服务器可能配置不正确，或者代理本身拒绝了连接。
    *   **网络路由问题：** 到 `huggingface.co` 的路由路径上存在问题，导致连接无法建立。
    *   **Hugging Face 服务器暂时性维护或拒绝连接：** 可能性较小，但如果服务器正在进行维护或有某种策略拒绝了您的 IP，也可能发生。

### 总结和解决方案：

两次错误都指向了同一个根源：您的机器无法成功连接到 Hugging Face 的服务器。

**您可以尝试以下排查和解决步骤：**

1.  **检查您的网络连接：**
    *   确保您的互联网连接稳定。尝试在浏览器中访问 `https://huggingface.co/`，看是否能正常打开。
    *   尝试 `ping huggingface.co` 或 `curl https://huggingface.co/`，看看是否能通。

2.  **检查防火墙设置：**
    *   如果您在使用 Linux/macOS，检查 `ufw` 或 `iptables` 等防火墙规则，确保没有阻止出站的 443 端口连接。
    *   如果您在公司网络或使用了 VPN，询问网络管理员是否有针对 `huggingface.co` 的访问限制。

3.  **检查代理设置：**
    *   如果您配置了 HTTP/HTTPS 代理，请确保代理设置正确，并且代理服务器是正常工作的。有时候，代理服务器会阻止某些域名的访问。
    *   在 Python 代码中，如果您需要通过代理访问，可能需要配置 `requests` 库或 `transformers` 库的代理设置。例如，设置环境变量 `HTTP_PROXY` 和 `HTTPS_PROXY`。

4.  **重试：**
    *   有时候只是暂时性的网络波动，过一段时间再试一次。

5.  **检查 `huggingface_hub` 缓存：**
    *   有时候旧的或损坏的缓存可能会导致问题。您可以尝试清除 Hugging Face Hub 的缓存。通常在 `~/.cache/huggingface/hub` 目录下。

6.  **尝试其他模型：**
    *   为了验证是不是特定模型的加载问题，您可以尝试从 Hugging Face Hub 加载一个更小的、更常见的模型（比如 `bert-base-uncased` 的 tokenizer），看是否也会出现同样的连接错误。

最常见的原因是网络不稳定或防火墙/代理阻挡。请您先在系统层面检查网络连接和防火墙设置。

---

**User**

Skip to content
Navigation Menu
punkpeye
awesome-mcp-servers

Type / to search
Code
Issues
24
Pull requests
132
Actions
Projects
Security
Insights
Owner avatar
awesome-mcp-servers
Public
punkpeye/awesome-mcp-servers
Go to file
t
Name		
punkpeye
punkpeye
Merge pull request #1015 from Kvintus/add-weblate-mcp-server
58738c9
 · 
2 days ago
CONTRIBUTING.md
Create CONTRIBUTING.md
7 months ago
LICENSE
Create LICENSE
7 months ago
README-ja.md
Added Intruder MCP server
3 days ago
README-ko.md
Added Intruder MCP server
3 days ago
README-pt_BR.md
Added Intruder MCP server
3 days ago
README-th.md
Added Intruder MCP server
3 days ago
README-zh.md
Added Intruder MCP server
3 days ago
README-zh_TW.md
Added Intruder MCP server
3 days ago
README.md
Merge pull request #1015 from Kvintus/add-weblate-mcp-server
2 days ago
Repository files navigation
README
MIT license
Awesome MCP Servers Awesome
ไทย English 繁體中文 简体中文 日本語 한국어 Português Brasileiro Discord Subreddit subscribers

A curated list of awesome Model Context Protocol (MCP) servers.

What is MCP?
Clients
Tutorials
Community
Legend
Server Implementations
Frameworks
Tips & Tricks
What is MCP?
MCP is an open protocol that enables AI models to securely interact with local and remote resources through standardized server implementations. This list focuses on production-ready and experimental MCP servers that extend AI capabilities through file access, database connections, API integrations, and other contextual services.

Clients
Checkout awesome-mcp-clients and glama.ai/mcp/clients.

Tip

Glama Chat is a multi-modal AI client with MCP support & AI gateway.

Tutorials
Model Context Protocol (MCP) Quickstart
Setup Claude Desktop App to Use a SQLite Database
Community
r/mcp Reddit
Discord Server
Legend
🎖️ – official implementation
programming language
🐍 – Python codebase
📇 – TypeScript (or JavaScript) codebase
🏎️ – Go codebase
🦀 – Rust codebase
#️⃣ - C# Codebase
☕ - Java codebase
🌊 – C/C++ codebase
scope
☁️ - Cloud Service
🏠 - Local Service
📟 - Embedded Systems
operating system
🍎 – For macOS
🪟 – For Windows
🐧 - For Linux
Note

Confused about Local 🏠 vs Cloud ☁️?

Use local when MCP server is talking to a locally installed software, e.g. taking control over Chrome browser.
Use network when MCP server is talking to remote APIs, e.g. weather API.
Server Implementations
Note

We now have a web-based directory that is synced with the repository.

🔗 - Aggregators
🎨 - Art & Culture
📂 - Browser Automation
☁️ - Cloud Platforms
👨‍💻 - Code Execution
🤖 - Coding Agents
🖥️ - Command Line
💬 - Communication
👤 - Customer Data Platforms
🗄️ - Databases
📊 - Data Platforms
🚚 - Delivery
🛠️ - Developer Tools
🧮 - Data Science Tools
📟 - Embedded system
📂 - File Systems
💰 - Finance & Fintech
🎮 - Gaming
🧠 - Knowledge & Memory
🗺️ - Location Services
🎯 - Marketing
📊 - Monitoring
🎥 - Multimedia Process
🔎 - Search & Data Extraction
🔒 - Security
🌐 - Social Media
🏃 - Sports
🎧 - Support & Service Management
🌎 - Translation Services
🎧 - Text-to-Speech
🚆 - Travel & Transportation
🔄 - Version Control
🛠️ - Other Tools and Integrations
🔗 Aggregators
Servers for accessing many apps and tools through a single MCP server.

julien040/anyquery 🏎️ 🏠 ☁️ - Query more than 40 apps with one binary using SQL. It can also connect to your PostgreSQL, MySQL, or SQLite compatible database. Local-first and private by design.
metatool-ai/metatool-app 📇 ☁️ 🏠 🍎 🪟 🐧 - MetaMCP is the one unified middleware MCP server that manages your MCP connections with GUI.
mindsdb/mindsdb - Connect and unify data across various platforms and databases with MindsDB as a single MCP server.
glenngillen/mcpmcp-server ☁️ 📇 🍎 🪟 🐧 - A list of MCP servers so you can ask your client which servers you can use to improve your daily workflow.
wegotdocs/open-mcp 📇 🏠 🍎 🪟 🐧 - Turn a web API into an MCP server in 10 seconds and add it to the open source registry: https://open-mcp.org
PipedreamHQ/pipedream ☁️ 🏠 - Connect with 2,500 APIs with 8,000+ prebuilt tools, and manage servers for your users, in your own app.
VeriTeknik/pluggedin-mcp-proxy 📇 🏠 - A comprehensive proxy server that combines multiple MCP servers into a single interface with extensive visibility features. It provides discovery and management of tools, prompts, resources, and templates across servers, plus a playground for debugging when building MCP servers.
tigranbs/mcgravity 📇 🏠 - A proxy tool for composing multiple MCP servers into one unified endpoint. Scale your AI tools by load balancing requests across multiple MCP servers, similar to how Nginx works for web servers.
MetaMCP 📇 ☁️ 🏠 🍎 🪟 🐧 - MetaMCP is the one unified middleware MCP server that manages your MCP connections with GUI.
WayStation-ai/mcp ☁️ 🍎 🪟 - Seamlessly and securely connect Claude Desktop and other MCP hosts to your favorite apps (Notion, Slack, Monday, Airtable, etc.). Takes less than 90 secs.
sxhxliang/mcp-access-point 📇 ☁️ 🏠 🍎 🪟 🐧 - Turn a web service into an MCP server in one click without making any code changes.
hamflx/imagen3-mcp 📇 🏠 🪟 🍎 🐧 - A powerful image generation tool using Google's Imagen 3.0 API through MCP. Generate high-quality images from text prompts with advanced photography, artistic, and photorealistic controls.
SureScaleAI/openai-gpt-image-mcp 📇 ☁️ - OpenAI GPT image generation/editing MCP server.
🎨 Art & Culture
Access and explore art collections, cultural heritage, and museum databases. Enables AI models to search and analyze artistic and cultural content.

abhiemj/manim-mcp-server 🐍 🏠 🪟 🐧 - A local MCP server that generates animations using Manim.
burningion/video-editing-mcp 🐍 - Add, Analyze, Search, and Generate Video Edits from your Video Jungle Collection
cswkim/discogs-mcp-server 📇 ☁️ - MCP server to interact with the Discogs API
djalal/quran-mcp-server 📇 ☁️ MCP server to interact with Quran.com corpus via the official REST API v4.
mikechao/metmuseum-mcp 📇 ☁️ - Metropolitan Museum of Art Collection API integration to search and display artworks in the collection.
r-huijts/rijksmuseum-mcp 📇 ☁️ - Rijksmuseum API integration for artwork search, details, and collections
r-huijts/oorlogsbronnen-mcp 📇 ☁️ - Oorlogsbronnen (War Sources) API integration for accessing historical WWII records, photographs, and documents from the Netherlands (1940-1945)
samuelgursky/davinci-resolve-mcp 🐍 - MCP server integration for DaVinci Resolve providing powerful tools for video editing, color grading, media management, and project control
yuna0x0/anilist-mcp 📇 ☁️ - A MCP server integrating AniList API for anime and manga information
diivi/aseprite-mcp 🐍 🏠 - MCP server using the Aseprite API to create pixel art
omni-mcp/isaac-sim-mcp 📇 ☁️ - A MCP Server and an extension enables natural language control of NVIDIA Isaac Sim, Lab, OpenUSD and etc.
8enSmith/mcp-open-library 📇 ☁️ - A MCP server for the Open Library API that enables AI assistants to search for book information.
PatrickPalmer/MayaMCP 🐍 🏠 - MCP server for Autodesk Maya
cantian-ai/bazi-mcp 📇 🏠 ☁️ 🍎 🪟 - Provides comprehensive and accurate Bazi (Chinese Astrology) charting and analysis
📂 Browser Automation
Web content access and automation capabilities. Enables searching, scraping, and processing web content in AI-friendly formats.

xspadex/bilibili-mcp 📇 🏠 - A FastMCP-based tool that fetches Bilibili's trending videos and exposes them via a standard MCP interface.
34892002/bilibili-mcp-js 📇 🏠 - A MCP server that supports searching for Bilibili content. Provides LangChain integration examples and test scripts.
aircodelabs/grasp 📇 🏠 - Self-hosted browser using agent with built-in MCP and A2A support.
automatalabs/mcp-server-playwright 🐍 - An MCP server for browser automation using Playwright
blackwhite084/playwright-plus-python-mcp 🐍 - An MCP python server using Playwright for browser automation,more suitable for llm
browserbase/mcp-server-browserbase 🎖️ 📇 - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)
browsermcp/mcp 📇 🏠 - Automate your local Chrome browser
co-browser/browser-use-mcp-server 🐍 - browser-use packaged as an MCP server with SSE transport. includes a dockerfile to run chromium in docker + a vnc server.
executeautomation/playwright-mcp-server 📇 - An MCP server using Playwright for browser automation and webscrapping
eyalzh/browser-control-mcp 📇 🏠 - An MCP server paired with a browser extension that enables LLM clients to control the user's browser (Firefox).
fradser/mcp-server-apple-reminders 📇 🏠 🍎 - An MCP server for interacting with Apple Reminders on macOS
getrupt/ashra-mcp 🐍 🏠 - Extract structured data from any website. Just prompt and get JSON.
kimtaeyoon83/mcp-server-youtube-transcript 📇 ☁️ - Fetch YouTube subtitles and transcripts for AI analysis
kimtth/mcp-aoai-web-browsing 🐍 🏠 - A minimal server/client MCP implementation using Azure OpenAI and Playwright.
microsoft/playwright-mcp - Official Microsoft Playwright MCP server, enabling LLMs to interact with web pages through structured accessibility snapshots
modelcontextprotocol/server-puppeteer 📇 🏠 - Browser automation for web scraping and interaction
ndthanhdev/mcp-browser-kit 📇 🏠 - An MCP Server for interacting with manifest v2 compatible browsers.
pskill9/web-search 📇 🏠 - An MCP server that enables free web searching using Google search results, with no API keys required.
recursechat/mcp-server-apple-shortcuts 📇 🏠 🍎 - An MCP Server Integration with Apple Shortcuts
☁️ Cloud Platforms
Cloud platform service integration. Enables management and interaction with cloud infrastructure and services.

awslabs/mcp 🎖️ ☁️ - AWS MCP servers for seamless integration with AWS services and resources.
qiniu/qiniu-mcp-server 🐍 ☁️ - A MCP built on Qiniu Cloud products, supporting access to Qiniu Cloud Storage, media processing services, etc.
alexbakers/mcp-ipfs 📇 ☁️ - upload and manipulation of IPFS storage
reza-gholizade/k8s-mcp-server 🏎️ ☁️/🏠 - A Kubernetes Model Context Protocol (MCP) server that provides tools for interacting with Kubernetes clusters through a standardized interface, including API resource discovery, resource management, pod logs, metrics, and events.
VmLia/books-mcp-server 📇 ☁️ - This is an MCP server used for querying books, and it can be applied in common MCP clients, such as Cherry Studio.
alexei-led/aws-mcp-server 🐍 ☁️ - A lightweight but powerful server that enables AI assistants to execute AWS CLI commands, use Unix pipes, and apply prompt templates for common AWS tasks in a safe Docker environment with multi-architecture support
alexei-led/k8s-mcp-server 🐍 - A lightweight yet robust server that empowers AI assistants to securely execute Kubernetes CLI commands (kubectl, helm, istioctl, and argocd) using Unix pipes in a safe Docker environment with multi-architecture support.
aliyun/alibaba-cloud-ops-mcp-server 🎖️ 🐍 ☁️ - A MCP server that enables AI assistants to operation resources on Alibaba Cloud, supporting ECS, Cloud Monitor, OOS and widely used cloud products.
bright8192/esxi-mcp-server 🐍 ☁️ - A VMware ESXi/vCenter management server based on MCP (Model Control Protocol), providing simple REST API interfaces for virtual machine management.
cloudflare/mcp-server-cloudflare 🎖️ 📇 ☁️ - Integration with Cloudflare services including Workers, KV, R2, and D1
cyclops-ui/mcp-cyclops 🎖️ 🏎️ ☁️ - An MCP server that allows AI agents to manage Kubernetes resources through Cyclops abstraction
jedisct1/fastly-mcp-server 🎖️ 📇 ☁️ - Integration with h Fastly services
flux159/mcp-server-kubernetes 📇 ☁️/🏠 - Typescript implementation of Kubernetes cluster operations for pods, deployments, services.
hardik-id/azure-resource-graph-mcp-server 📇 ☁️/🏠 - A Model Context Protocol server for querying and analyzing Azure resources at scale using Azure Resource Graph, enabling AI assistants to explore and monitor Azure infrastructure.
jdubois/azure-cli-mcp - A wrapper around the Azure CLI command line that allows you to talk directly to Azure
johnneerdael/netskope-mcp 🔒 ☁️ - An MCP to give access to all Netskope Private Access components within a Netskope Private Access environments including detailed setup information and LLM examples on usage.
manusa/Kubernetes MCP Server 🏎️ 🏠 A - powerful Kubernetes MCP server with additional support for OpenShift. Besides providing CRUD operations for any Kubernetes resource, this server provides specialized tools to interact with your cluster.
nwiizo/tfmcp 🦀 🏠 - A Terraform MCP server allowing AI assistants to manage and operate Terraform environments, enabling reading configurations, analyzing plans, applying configurations, and managing Terraform state.
pulumi/mcp-server 🎖️ 📇 🏠 - MCP server for interacting with Pulumi using the Pulumi Automation API and Pulumi Cloud API. Enables MCP clients to perform Pulumi operations like retrieving package information, previewing changes, deploying updates, and retrieving stack outputs programmatically.
rohitg00/kubectl-mcp-server 🐍 ☁️/🏠 - A Model Context Protocol (MCP) server for Kubernetes that enables AI assistants like Claude, Cursor, and others to interact with Kubernetes clusters through natural language.
strowk/mcp-k8s-go 🏎️ ☁️/🏠 - Kubernetes cluster operations through MCP
thunderboltsid/mcp-nutanix 🏎️ 🏠/☁️ - Go-based MCP Server for interfacing with Nutanix Prism Central resources.
trilogy-group/aws-pricing-mcp 🏎️ ☁️/🏠 - Get up-to-date EC2 pricing information with one call. Fast. Powered by a pre-parsed AWS pricing catalogue.
weibaohui/k8m 🏎️ ☁️/🏠 - Provides MCP multi-cluster Kubernetes management and operations, featuring a management interface, logging, and nearly 50 built-in tools covering common DevOps and development scenarios. Supports both standard and CRD resources.
weibaohui/kom 🏎️ ☁️/🏠 - Provides MCP multi-cluster Kubernetes management and operations. It can be integrated as an SDK into your own project and includes nearly 50 built-in tools covering common DevOps and development scenarios. Supports both standard and CRD resources.
wenhuwang/mcp-k8s-eye 🏎️ ☁️/🏠 - MCP Server for kubernetes management, and analyze your cluster, application health
erikhoward/adls-mcp-server 🐍 ☁️/🏠 - MCP Server for Azure Data Lake Storage. It can perform manage containers, read/write/upload/download operations on container files and manage file metadata.
silenceper/mcp-k8s 🏎️ ☁️/🏠 - MCP-K8S is an AI-driven Kubernetes resource management tool that allows users to operate any resources in Kubernetes clusters through natural language interaction, including native resources (like Deployment, Service) and custom resources (CRD). No need to memorize complex commands - just describe your needs, and AI will accurately execute the corresponding cluster operations, greatly enhancing the usability of Kubernetes.
redis/mcp-redis-cloud 📇 ☁️ - Manage your Redis Cloud resources effortlessly using natural language. Create databases, monitor subscriptions, and configure cloud deployments with simple commands.
portainer/portainer-mcp 🏎️ ☁️/🏠 - A powerful MCP server that enables AI assistants to seamlessly interact with Portainer instances, providing natural language access to container management, deployment operations, and infrastructure monitoring capabilities.
👨‍💻 Code Execution
Code execution servers. Allow LLMs to execute code in a secure environment, e.g. for coding agents.

pydantic/pydantic-ai/mcp-run-python 🐍 🏠- Run Python code in a secure sandbox via MCP tool calls
yepcode/mcp-server-js 🎖️ 📇 ☁️ - Execute any LLM-generated code in a secure and scalable sandbox environment and create your own MCP tools using JavaScript or Python, with full support for NPM and PyPI packages
ckanthony/openapi-mcp 🏎️ ☁️ - OpenAPI-MCP: Dockerized MCP Server to allow your AI agent to access any API with existing api docs.
alfonsograziano/node-code-sandbox-mcp 📇 🏠 – A Node.js MCP server that spins up isolated Docker-based sandboxes for executing JavaScript snippets with on-the-fly npm dependency installation and clean teardown
r33drichards/mcp-js 🦀 🏠 🐧 🍎 - A Javascript code execution sandbox that uses v8 to isolate code to run AI generated javascript locally without fear. Supports heap snapshotting for persistent sessions.
🤖 Coding Agents
Full coding agents that enable LLMs to read, edit, and execute code and solve general programming tasks completely autonomously.

oraios/serena🐍🏠 - A fully-featured coding agent that relies on symbolic code operations by using language servers.
ezyang/codemcp 🐍🏠 - Coding agent with basic read, write and command line tools.
doggybee/mcp-server-leetcode 📇 ☁️ - An MCP server that enables AI models to search, retrieve, and solve LeetCode problems. Supports metadata filtering, user profiles, submissions, and contest data access.
jinzcdev/leetcode-mcp-server 📇 ☁️ - MCP server enabling automated access to LeetCode's programming problems, solutions, submissions and public data with optional authentication for user-specific features (e.g., notes), supporting both leetcode.com (global) and leetcode.cn (China) sites.
juehang/vscode-mcp-server 📇 🏠 - A MCP Server that allows AI such as Claude to read from the directory structure in a VS Code workspace, see problems picked up by linter(s) and the language server, read code files, and make edits.
micl2e2/code-to-tree 🌊 🏠 📟 🐧 🪟 🍎 - A single-binary MCP server that converts source code into AST, regardless of language.
🖥️ Command Line
Run commands, capture output and otherwise interact with shells and command line tools.

ferrislucas/iterm-mcp 🖥️ 🛠️ 💬 - A Model Context Protocol server that provides access to iTerm. You can run commands and ask questions about what you see in the iTerm terminal.
g0t4/mcp-server-commands 📇 🏠 - Run any command with run_command and run_script tools.
maxim-saplin/mcp_safe_local_python_executor - Safe Python interpreter based on HF Smolagents LocalPythonExecutor
MladenSU/cli-mcp-server 🐍 🏠 - Command line interface with secure execution and customizable security policies
OthmaneBlial/term_mcp_deepseek 🐍 🏠 - A DeepSeek MCP-like Server for Terminal
tumf/mcp-shell-server - A secure shell command execution server implementing the Model Context Protocol (MCP)
automateyournetwork/pyATS_MCP - Cisco pyATS server enabling structured, model-driven interaction with network devices.
wonderwhy-er/DesktopCommanderMCP 📇 🏠 🍎 🪟 🐧 - A swiss-army-knife that can manage/execute programs and read/write/search/edit code and text files.
tufantunc/ssh-mcp 📇 🏠 🐧 🪟 - MCP server exposing SSH control for Linux and Windows servers via Model Context Protocol. Securely execute remote shell commands with password or SSH key authentication.
💬 Communication
Integration with communication platforms for message management and channel operations. Enables AI models to interact with team communication tools.

AbdelStark/nostr-mcp ☁️ - A Nostr MCP server that allows to interact with Nostr, enabling posting notes, and more.
adhikasp/mcp-twikit 🐍 ☁️ - Interact with Twitter search and timeline
agentmail-toolkit/mcp 🐍 💬 - An MCP server to create inboxes on the fly to send, receive, and take actions on email. We aren't AI agents for email, but email for AI Agents.
arpitbatra123/mcp-googletasks 📇 ☁️ - An MCP server to interface with the Google Tasks API
carterlasalle/mac_messages_mcp 🏠 🍎 🚀 - An MCP server that securely interfaces with your iMessage database via the Model Context Protocol (MCP), allowing LLMs to query and analyze iMessage conversations. It includes robust phone number validation, attachment processing, contact management, group chat handling, and full support for sending and receiving messages.
chaindead/telegram-mcp 🏎️ 🏠 - Telegram API integration for accessing user data, managing dialogs (chats, channels, groups), retrieving messages, and handling read status
chigwell/telegram-mcp 🐍 🏠 - Telegram API integration for accessing user data, managing dialogs (chats, channels, groups), retrieving messages, sending messages and handling read status.
elie222/inbox-zero 🐍 ☁️ - An MCP server for Inbox Zero. Adds functionality on top of Gmail like finding out which emails you need to reply to or need to follow up on.
gitmotion/ntfy-me-mcp 📇 ☁️ 🏠 - An ntfy MCP server for sending/fetching ntfy notifications to your self-hosted ntfy server from AI Agents 📤 (supports secure token auth & more - use with npx or docker!)
gotoolkits/wecombot 🚀 ☁️ - An MCP server application that sends various types of messages to the WeCom group robot.
hannesrudolph/imessage-query-fastmcp-mcp-server 🐍 🏠 🍎 - An MCP server that provides safe access to your iMessage database through Model Context Protocol (MCP), enabling LLMs to query and analyze iMessage conversations with proper phone number validation and attachment handling
i-am-bee/acp-mcp 🐍 💬 - An MCP server acting as an adapter into the ACP ecosystem. Seamlessly exposes ACP agents to MCP clients, bridging the communication gap between the two protocols.
jagan-shanmugam/mattermost-mcp-host 🐍 🏠 - A MCP server along with MCP host that provides access to Mattermost teams, channels and messages. MCP host is integrated as a bot in Mattermost with access to MCP servers that can be configured.
lharries/whatsapp-mcp 🐍 🏎️ - An MCP server for searching your personal WhatsApp messages, contacts and sending messages to individuals or groups
line/line-bot-mcp-server 🎖 📇 ☁️ - MCP Server for Integrating LINE Official Account
MarkusPfundstein/mcp-gsuite 🐍 ☁️ - Integration with gmail and Google Calendar.
modelcontextprotocol/server-bluesky 📇 ☁️ - Bluesky instance integration for querying and interaction
modelcontextprotocol/server-slack 📇 ☁️ - Slack workspace integration for channel management and messaging
korotovsky/slack-mcp-server 📇 ☁️ - The most powerful MCP server for Slack Workspaces.
sawa-zen/vrchat-mcp - 📇 🏠 This is an MCP server for interacting with the VRChat API. You can retrieve information about friends, worlds, avatars, and more in VRChat.
takumi0706/google-calendar-mcp 📇 ☁️ - An MCP server to interface with the Google Calendar API. Based on TypeScript.
teddyzxcv/ntfy-mcp - The MCP server that keeps you informed by sending the notification on phone using ntfy
userad/didlogic_mcp 🐍 ☁️ - An MCP server for DIDLogic. Adds functionality to manage SIP endpoints, numbers and destinations.
zcaceres/gtasks-mcp 📇 ☁️ - An MCP server to Manage Google Tasks
InditexTech/mcp-teams-server 🐍 ☁️ - MCP server that integrates Microsoft Teams messaging (read, post, mention, list members and threads)
softeria/ms-365-mcp-server 📇 ☁️ - MCP server that connects to the whole Microsoft 365 suite using Graph API (including mail, files, Excel, calendar)
YCloud-Developers/ycloud-whatsapp-mcp-server 📇 🏠 - MCP server for WhatsApp Business Platform by YCloud.
jaipandya/producthunt-mcp-server 🐍 🏠 - MCP server for Product Hunt. Interact with trending posts, comments, collections, users, and more.
👤 Customer Data Platforms
Provides access to customer profiles inside of customer data platforms

iaptic/mcp-server-iaptic 🎖️ 📇 ☁️ - Connect with iaptic to ask about your Customer Purchases, Transaction data and App Revenue statistics.
OpenDataMCP/OpenDataMCP 🐍 ☁️ - Connect any Open Data to any LLM with Model Context Protocol.
sergehuber/inoyu-mcp-unomi-server 📇 ☁️ - An MCP server to access and updates profiles on an Apache Unomi CDP server.
tinybirdco/mcp-tinybird 🐍 ☁️ - An MCP server to interact with a Tinybird Workspace from any MCP client.
@antv/mcp-server-chart 🎖️ 📇 ☁️ - A Model Context Protocol server for generating visual charts using AntV.
🗄️ Databases
Secure database access with schema inspection capabilities. Enables querying and analyzing data with configurable security controls including read-only access.

Aiven-Open/mcp-aiven - 🐍 ☁️ 🎖️ - Navigate your Aiven projects and interact with the PostgreSQL®, Apache Kafka®, ClickHouse® and OpenSearch® services
alexanderzuev/supabase-mcp-server - Supabase MCP Server with support for SQL query execution and database exploration tools
aliyun/alibabacloud-tablestore-mcp-server ☕ 🐍 ☁️ - MCP service for Tablestore, features include adding documents, semantic search for documents based on vectors and scalars, RAG-friendly, and serverless.
benborla29/mcp-server-mysql ☁️ 🏠 - MySQL database integration in NodeJS with configurable access controls and schema inspection
bytebase/dbhub 📇 🏠 – Universal database MCP server supporting mainstream databases.
c4pt0r/mcp-server-tidb 🐍 ☁️ - TiDB database integration with schema inspection and query capabilities
Canner/wren-engine 🐍 🦀 🏠 - The Semantic Engine for Model Context Protocol(MCP) Clients and AI Agents
centralmind/gateway 🏎️ 🏠 🍎 🪟 - MCP and MCP SSE Server that automatically generate API based on database schema and data. Supports PostgreSQL, Clickhouse, MySQL, Snowflake, BigQuery, Supabase
ChristianHinge/dicom-mcp 🐍 ☁️ 🏠 - DICOM integration to query, read, and move medical images and reports from PACS and other DICOM compliant systems.
chroma-core/chroma-mcp 🎖️ 🐍 ☁️ 🏠 - Chroma MCP server to access local and cloud Chroma instances for retrieval capabilities
ClickHouse/mcp-clickhouse 🐍 ☁️ - ClickHouse database integration with schema inspection and query capabilities
confluentinc/mcp-confluent 🐍 ☁️ - Confluent integration to interact with Confluent Kafka and Confluent Cloud REST APIs.
Couchbase-Ecosystem/mcp-server-couchbase 🎖️ 🐍 ☁️ 🏠 - Couchbase MCP server provides unfied access to both Capella cloud and self-managed clusters for document operations, SQL++ queries and natural language data analysis.
cr7258/elasticsearch-mcp-server 🐍 🏠 - MCP Server implementation that provides Elasticsearch interaction
crystaldba/postgres-mcp 🐍 🏠 - All-in-one MCP server for Postgres development and operations, with tools for performance analysis, tuning, and health checks
Dataring-engineering/mcp-server-trino 🐍 ☁️ - Trino MCP Server to query and access data from Trino Clusters.
tuannvm/mcp-trino 🏎️ ☁️ - A Go implementation of a Model Context Protocol (MCP) server for Trino
designcomputer/mysql_mcp_server 🐍 🏠 - MySQL database integration with configurable access controls, schema inspection, and comprehensive security guidelines
wenb1n-dev/mysql_mcp_server_pro 🐍 🏠 - Supports SSE, STDIO; not only limited to MySQL's CRUD functionality; also includes database exception analysis capabilities; controls database permissions based on roles; and makes it easy for developers to extend tools with customization
domdomegg/airtable-mcp-server 📇 🏠 - Airtable database integration with schema inspection, read and write capabilities
edwinbernadus/nocodb-mcp-server 📇 ☁️ - Nocodb database integration, read and write capabilities
ergut/mcp-bigquery-server 📇 ☁️ - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities
f4ww4z/mcp-mysql-server 📇 🏠 - Node.js-based MySQL database integration that provides secure MySQL database operations
fireproof-storage/mcp-database-server 📇 ☁️ - Fireproof ledger database with multi-user sync
FreePeak/db-mcp-server 🏎️ 🏠 – A high-performance multi-database MCP server built with Golang, supporting MySQL & PostgreSQL (NoSQL coming soon). Includes built-in tools for query execution, transaction management, schema exploration, query building, and performance analysis, with seamless Cursor integration for enhanced database workflows.
furey/mongodb-lens 📇 🏠 - MongoDB Lens: Full Featured MCP Server for MongoDB Databases
gannonh/firebase-mcp 🔥 ⛅️ - Firebase services including Auth, Firestore and Storage.
get-convex/convex-backend 📇 ☁️ - Convex database integration to introspect tables, functions, and run oneoff queries (Source)
googleapis/genai-toolbox 🏎️ ☁️ - Open source MCP server specializing in easy, fast, and secure tools for Databases.
GreptimeTeam/greptimedb-mcp-server 🐍 🏠 - MCP Server for querying GreptimeDB.
hannesrudolph/sqlite-explorer-fastmcp-mcp-server 🐍 🏠 - An MCP server that provides safe, read-only access to SQLite databases through Model Context Protocol (MCP). This server is built with the FastMCP framework, which enables LLMs to explore and query SQLite databases with built-in safety features and query validation.
idoru/influxdb-mcp-server 📇 ☁️ 🏠 - Run queries against InfluxDB OSS API v2.
isaacwasserman/mcp-snowflake-server 🐍 ☁️ - Snowflake integration implementing read and (optional) write operations as well as insight tracking
joshuarileydev/supabase-mcp-server - Supabase MCP Server for managing and creating projects and organisations in Supabase
jovezhong/mcp-timeplus 🐍 ☁️ - MCP server for Apache Kafka and Timeplus. Able to list Kafka topics, poll Kafka messages, save Kafka data locally and query streaming data with SQL via Timeplus
KashiwaByte/vikingdb-mcp-server 🐍 ☁️ - VikingDB integration with collection and index introduction, vector store and search capabilities.
kiliczsh/mcp-mongo-server 📇 🏠 - A Model Context Protocol Server for MongoDB
ktanaka101/mcp-server-duckdb 🐍 🏠 - DuckDB database integration with schema inspection and query capabilities
LucasHild/mcp-server-bigquery 🐍 ☁️ - BigQuery database integration with schema inspection and query capabilities
quarkiverse/mcp-server-jdbc ☕ 🏠 - Connect to any JDBC-compatible database and query, insert, update, delete, and more.
jparkerweb/mcp-sqlite 📇 🏠 - Model Context Protocol (MCP) server that provides comprehensive SQLite database interaction capabilities.
memgraph/mcp-memgraph 🐍 🏠 - Memgraph MCP Server - includes a tool to run a query against Memgraph and a schema resource.
modelcontextprotocol/server-postgres 📇 🏠 - PostgreSQL database integration with schema inspection and query capabilities
modelcontextprotocol/server-sqlite 🐍 🏠 - SQLite database operations with built-in analysis features
neo4j-contrib/mcp-neo4j 🐍 🏠 - Model Context Protocol with Neo4j (Run queries, Knowledge Graph Memory, Manaage Neo4j Aura Instances)
neondatabase/mcp-server-neon 📇 ☁️ — An MCP Server for creating and managing Postgres databases using Neon Serverless Postgres
niledatabase/nile-mcp-server MCP server for Nile's Postgres platform - Manage and query Postgres databases, tenants, users, auth using LLMs
openlink/mcp-server-odbc 🐍 🏠 - An MCP server for generic Database Management System (DBMS) Connectivity via the Open Database Connectivity (ODBC) protocol
openlink/mcp-server-sqlalchemy 🐍 🏠 - An MCP server for generic Database Management System (DBMS) Connectivity via SQLAlchemy using Python ODBC (pyodbc)
pab1it0/adx-mcp-server 🐍 ☁️ - Query and analyze Azure Data Explorer databases
pab1it0/prometheus-mcp-server 🐍 ☁️ - Query and analyze Prometheus, open-source monitoring system.
prisma/prisma 🐍 🏠 - Gives LLMs the ability to manage Prisma Postgres databases (e.g. spin up new database instances or run schema migrations).
qdrant/mcp-server-qdrant 🐍 🏠 - A Qdrant MCP server
QuantGeekDev/mongo-mcp 📇 🏠 - MongoDB integration that enables LLMs to interact directly with databases.
rashidazarang/airtable-mcp 🐍 ☁️ - Connect AI tools directly to Airtable. Query, create, update, and delete records using natural language. Features include base management, table operations, schema manipulation, record filtering, and data migration through a standardized MCP interface.
redis/mcp-redis 🐍 🏠 - The Redis official MCP Server offers an interface to manage and search data in Redis.
runekaagaard/mcp-alchemy 🐍 🏠 - Universal SQLAlchemy-based database integration supporting PostgreSQL, MySQL, MariaDB, SQLite, Oracle, MS SQL Server and many more databases. Features schema and relationship inspection, and large dataset analysis capabilities.
sirmews/mcp-pinecone 🐍 ☁️ - Pinecone integration with vector search capabilities
skysqlinc/skysql-mcp 🎖️ ☁️ - Serverless MariaDB Cloud DB MCP server. Tools to launch, delete, execute SQL and work with DB level AI agents for accurate text-2-sql and conversations.
supabase-community/supabase-mcp 🎖️ 📇 ☁️ - Official Supabase MCP server to connect AI assistants directly with your Supabase project and allows them to perform tasks like managing tables, fetching config, and querying data.
TheRaLabs/legion-mcp 🐍 🏠 Universal database MCP server supporting multiple database types including PostgreSQL, Redshift, CockroachDB, MySQL, RDS MySQL, Microsoft SQL Server, BigQuery, Oracle DB, and SQLite.
tradercjz/dolphindb-mcp-server 🐍 ☁️ - TDolphinDB database integration with schema inspection and query capabilities
weaviate/mcp-server-weaviate 🐍 📇 ☁️ - An MCP Server to connect to your Weaviate collections as a knowledge base as well as using Weaviate as a chat memory store.
XGenerationLab/xiyan_mcp_server 📇 ☁️ — An MCP server that supports fetching data from a database using natural language queries, powered by XiyanSQL as the text-to-SQL LLM.
xing5/mcp-google-sheets 🐍 ☁️ - A Model Context Protocol server for interacting with Google Sheets. This server provides tools to create, read, update, and manage spreadsheets through the Google Sheets API.
freema/mcp-gsheets 📇 ☁️ - MCP server for Google Sheets API integration with comprehensive reading, writing, formatting, and sheet management capabilities.
Zhwt/go-mcp-mysql 🏎️ 🏠 – Easy to use, zero dependency MySQL MCP server built with Golang with configurable readonly mode and schema inspection.
ydb/ydb-mcp 🎖️ 🐍 ☁️ - MCP server for interacting with YDB databases
zilliztech/mcp-server-milvus 🐍 🏠 ☁️ - MCP Server for Milvus / Zilliz, making it possible to interact with your database.
openlink/mcp-server-jdbc 🐍 🏠 - An MCP server for generic Database Management System (DBMS) Connectivity via the Java Database Connectivity (JDBC) protocol
yincongcyincong/VictoriaMetrics-mcp-server 🐍 🏠 - An MCP server for interacting with VictoriaMetrics database.
hydrolix/mcp-hydrolix 🎖️ 🐍 ☁️ - Hydrolix time-series datalake integration providing schema exploration and query capabilities to LLM-based workflows.
davewind/mysql-mcp-server 🏎️ 🏠 A – user-friendly read-only mysql mcp server for cursor and n8n...
📊 Data Platforms
Data Platforms for data integration, transformation and pipeline orchestration.

flowcore/mcp-flowcore-platform 🎖️ 📇 ☁️ 🏠 - Interact with Flowcore to perform actions, ingest data, and analyse, cross reference and utilise any data in your data cores, or in public data cores; all with human language.
JordiNei/mcp-databricks-server 🐍 ☁️ - Connect to Databricks API, allowing LLMs to run SQL queries, list jobs, and get job status.
yashshingvi/databricks-genie-MCP 🐍 ☁️ - A server that connects to the Databricks Genie API, allowing LLMs to ask natural language questions, run SQL queries, and interact with Databricks conversational agents.
jwaxman19/qlik-mcp 📇 ☁️ - MCP Server for Qlik Cloud API that enables querying applications, sheets, and extracting data from visualizations with comprehensive authentication and rate limiting support.
keboola/keboola-mcp-server 🐍 - interact with Keboola Connection Data Platform. This server provides tools for listing and accessing data from Keboola Storage API.
dbt-labs/dbt-mcp 🎖️ 🐍 🏠 ☁️ - Official MCP server for dbt (data build tool) providing integration with dbt Core/Cloud CLI, project metadata discovery, model information, and semantic layer querying capabilities.
mattijsdp/dbt-docs-mcp 🐍 🏠 - MCP server for dbt-core (OSS) users as the official dbt MCP only supports dbt Cloud. Supports project metadata, model and column-level lineage and dbt documentation.
💻 Developer Tools
Tools and integrations that enhance the development workflow and environment management.

Pratyay/mac-monitor-mcp 🐍 🏠 🍎 - Identifies resource-intensive processes on macOS and provides performance improvement suggestions.
21st-dev/Magic-MCP - Create crafted UI components inspired by the best 21st.dev design engineers.
Hypersequent/qasphere-mcp 🎖️ 📇 ☁️ - Integration with QA Sphere test management system, enabling LLMs to discover, summarize, and interact with test cases directly from AI-powered IDEs
admica/FileScopeMCP 🐍 📇 🦀 - Analyzes your codebase identifying important files based on dependency relationships. Generates diagrams and importance scores, helping AI assistants understand the codebase.
ambar/simctl-mcp 📇 🏠 🍎 A MCP server implementation for iOS Simulator control.
api7/apisix-mcp 🎖️ 📇 🏠 MCP Server that support for querying and managing all resource in Apache APISIX.
ArchAI-Labs/fastmcp-sonarqube-metrics 🐍 🏠 🪟 🐧 🍎 - A Model Context Protocol (MCP) server that provides a set of tools for retrieving information about SonarQube projects like metrics (actual and historical), issues, health status.
automation-ai-labs/mcp-link 🏎️ 🏠 - Seamlessly Integrate Any API with AI Agents (with OpenAPI Schema)
azer/react-analyzer-mcp 📇 🏠 - Analyze React code locally, generate docs / llm.txt for whole project at once
davidlin2k/pox-mcp-server 🐍 🏠 - MCP server for the POX SDN controller to provides network control and management capabilities.
CodeLogicIncEngineering/codelogic-mcp-server 🎖️ 🐍 ☁️ 🍎 🪟 🐧 - Official MCP server for CodeLogic, providing access to code dependency analytics, architectural risk analysis, and impact assessment tools.
Comet-ML/Opik-MCP 🎖️ 📇 ☁️ 🏠 - Use natural language to explore LLM observability, traces, and monitoring data captured by Opik.
CircleCI/mcp-server-circleci 📇 ☁️ Enable AI Agents to fix build failures from CircleCI.
currents-dev/currents-mcp 🎖️ 📇 ☁️ Enable AI Agents to fix Playwright test failures reported to Currents.
delano/postman-mcp-server 📇 ☁️ - Interact with Postman API
flipt-io/mcp-server-flipt 📇 🏠 - Enable AI assistants to interact with your feature flags in Flipt.
GLips/Figma-Context-MCP 📇 🏠 - Provide coding agents direct access to Figma data to help them one-shot design implementation.
gofireflyio/firefly-mcp 🎖️ 📇 ☁️ - Integrates, discovers, manages, and codifies cloud resources with Firefly.
Govcraft/rust-docs-mcp-server 🦀 🏠 - Provides up-to-date documentation context for a specific Rust crate to LLMs via an MCP tool, using semantic search (embeddings) and LLM summarization.
haris-musa/excel-mcp-server 🐍 🏠 - An Excel manipulation server providing workbook creation, data operations, formatting, and advanced features (charts, pivot tables, formulae).
higress-group/higress-ops-mcp-server 🐍 🏠 - MCP server that provides comprehensive tools for managing Higress gateway configurations and operations.
hijaz/postmancer 📇 🏠 - A MCP server for replacing Rest Clients like Postman/Insomnia, by allowing your LLM to maintain and use api collections.
hloiseaufcms/mcp-gopls 🏎️ 🏠 - A MCP server for interacting with Go's Language Server Protocol (gopls) and benefit from advanced Go code analysis features.
hungthai1401/bruno-mcp 📇 🏠 - A MCP server for interacting with Bruno API Client.
hyperb1iss/droidmind 🐍 🏠 - Control Android devices with AI through MCP, enabling device control, debugging, system analysis, and UI automation with a comprehensive security framework.
XixianLiang/HarmonyOS-mcp-server 🐍 🏠 - Control HarmonyOS-next devices with AI through MCP. Support device control and UI automation.
IlyaGulya/gradle-mcp-server 🏠 - Gradle integration using the Gradle Tooling API to inspect projects, execute tasks, and run tests with per-test result reporting
InhiblabCore/mcp-image-compression 🐍 🏠 - MCP server for local compression of various image formats.
isaacphi/mcp-language-server 🏎️ 🏠 - MCP Language Server helps MCP enabled clients navigate codebases more easily by giving them access to semantic tools like get definition, references, rename, and diagnostics.
ios-simulator-mcp 📇 🏠 🍎 - A Model Context Protocol (MCP) server for interacting with iOS simulators. This server allows you to interact with iOS simulators by getting information about them, controlling UI interactions, and inspecting UI elements.
InditexTech/mcp-server-simulator-ios-idb 📇 🏠 🍎 - A Model Context Protocol (MCP) server that enables LLMs to interact with iOS simulators (iPhone, iPad, etc.) through natural language commands.
IvanAmador/vercel-ai-docs-mcp 📇 🏠 - A Model Context Protocol (MCP) server that provides AI-powered search and querying capabilities for the Vercel AI SDK documentation.
j4c0bs/mcp-server-sql-analyzer 🐍 - MCP server that provides SQL analysis, linting, and dialect conversion using SQLGlot
jasonjmcghee/claude-debugs-for-you 📇 🏠 - An MCP Server and VS Code Extension which enables (language agnostic) automatic debugging via breakpoints and expression evaluation.
jetbrains/mcpProxy 🎖️ 📇 🏠 - Connect to JetBrains IDE
qainsights/jmeter-mcp-server 🐍 🏠 - JMeter MCP Server for performance testing
Jktfe/serveMyAPI 📇 🏠 🍎 - A personal MCP (Model Context Protocol) server for securely storing and accessing API keys across projects using the macOS Keychain.
joshuarileydev/app-store-connect-mcp-server 📇 🏠 - An MCP server to communicate with the App Store Connect API for iOS Developers
joshuarileydev/simulator-mcp-server 📇 🏠 - An MCP server to control iOS Simulators
qainsights/k6-mcp-server 🐍 🏠 - Grafana k6 MCP Server for performance testing
lamemind/mcp-server-multiverse 📇 🏠 🛠️ - A middleware server that enables multiple isolated instances of the same MCP servers to coexist independently with unique namespaces and configurations.
langfuse/mcp-server-langfuse 🐍 🏠 - MCP server to access and manage LLM application prompts created with Langfuse Prompt Management.
mobile-next/mobile-mcp 📇 🏠 🐧 🍎 - MCP Server for Android/iOS application and device automation, development and app scraping. Simulator/Emulator/Physical devices like iPhone, Google Pixel, Samsung supported.
qainsights/locust-mcp-server 🐍 🏠 - Locust MCP Server for performance testing
mrexodia/user-feedback-mcp 🐍 🏠 - Simple MCP Server to enable a human-in-the-loop workflow in tools like Cline and Cursor.
narumiruna/gitingest-mcp 🐍 🏠 - A MCP server that uses gitingest to convert any Git repository into a simple text digest of its codebase.
OctoMind-dev/octomind-mcp 📇 ☁️ - lets your preferred AI agent create & run fully managed Octomind end-to-end tests from your codebase or other data sources like Jira, Slack or TestRail.
kadykov/mcp-openapi-schema-explorer 📇 ☁️ 🏠 - Token-efficient access to OpenAPI/Swagger specs via MCP Resources.
pskill9/website-downloader 🗄️ 🚀 - This MCP server provides a tool to download entire websites using wget. It preserves the website structure and converts links to work locally.
utensils/mcp-nixos 🐍 🏠 - MCP server providing accurate information about NixOS packages, system options, Home Manager configurations, and nix-darwin macOS settings to prevent AI hallucinations.
QuantGeekDev/docker-mcp 🏎️ 🏠 - Docker container management and operations through MCP
ckreiling/mcp-server-docker 🐍 🏠 - Integrate with Docker to manage containers, images, volumes, and networks.
r-huijts/xcode-mcp-server 📇 🏠 🍎 - Xcode integration for project management, file operations, and build automation
ReAPI-com/mcp-openapi 📇 🏠 - MCP server that lets LLMs know everything about your OpenAPI specifications to discover, explain and generate code/mock data
Rootly-AI-Labs/Rootly-MCP-server 🎖️ 🐍 ☁️ 🍎 - MCP server for the incident management platform Rootly.
sammcj/mcp-package-version 📇 🏠 - An MCP Server to help LLMs suggest the latest stable package versions when writing code.
sapientpants/sonarqube-mcp-server 🦀 ☁️ 🏠 - A Model Context Protocol (MCP) server that integrates with SonarQube to provide AI assistants with access to code quality metrics, issues, and quality gate statuses
SDGLBL/mcp-claude-code 🐍 🏠 - An implementation of Claude Code capabilities using MCP, enabling AI code understanding, modification, and project analysis with comprehensive tool support.
snaggle-ai/openapi-mcp-server 🏎️ 🏠 - Connect any HTTP/REST API server using an Open API spec (v3)
stass/lldb-mcp 🐍 🏠 🐧 🍎 - A MCP server for LLDB enabling AI binary and core file analysis, debugging, disassembling.
TencentEdgeOne/edgeone-pages-mcp 📇 ☁️ - An MCP service for deploying HTML content to EdgeOne Pages and obtaining a publicly accessible URL.
tumf/mcp-text-editor 🐍 🏠 - A line-oriented text file editor. Optimized for LLM tools with efficient partial file access to minimize token usage.
vivekvells/mcp-pandoc 🗄️ 🚀 - MCP server for seamless document format conversion using Pandoc, supporting Markdown, HTML, PDF, DOCX (.docx), csv and more.
VSCode Devtools 📇 - Connect to VSCode ide and use semantic tools like find_usages
xcodebuild 🍎 Build iOS Xcode workspace/project and feed back errors to llm.
xzq.xu/jvm-mcp-server 📇 🏠 - An implementation project of a JVM-based MCP (Model Context Protocol) server.
yangkyeongmo@/mcp-server-apache-airflow 🐍 🏠 - MCP server that connects to Apache Airflow using official client.
YuChenSSR/mindmap-mcp-server 🐍 🏠 - A Model Context Protocol (MCP) server for generating a beautiful interactive mindmap.
YuChenSSR/multi-ai-advisor 📇 🏠 - A Model Context Protocol (MCP) server that queries multiple Ollama models and combines their responses, providing diverse AI perspectives on a single question.
yWorks/mcp-typescribe 📇 🏠 - MCP server that provides Typescript API information efficiently to the agent to enable it to work with untrained APIs
zcaceres/fetch-mcp 📇 🏠 - An MCP server to flexibly fetch JSON, text, and HTML data
zenml-io/mcp-zenml 🐍 🏠 ☁️ - An MCP server to connect with your ZenML MLOps and LLMOps pipelines
idosal/git-mcp 📇 ☁️ - gitmcp.io is a generic remote MCP server to connect to ANY GitHub repository or project for documentation
tgeselle/bugsnag-mcp 📇 ☁️ - An MCP server for interacting with Bugsnag
jordandalton/restcsv-mcp-server 📇 ☁️ - An MCP server for CSV files.
cjo4m06/mcp-shrimp-task-manager 📇 ☁️ 🏠 – A programming-focused task management system that boosts coding agents like Cursor AI with advanced task memory, self-reflection, and dependency management. ShrimpTaskManager
axliupore/mcp-code-runner 📇 🏠 - An MCP server for running code locally via Docker and supporting multiple programming languages.
yikakia/godoc-mcp-server 🏎️ ☁️ 🪟 🐧 🍎 - Query Go package information on pkg.go.dev
ckanthony/gin-mcp 🏎️ ☁️ 📟 🪟 🐧 🍎 - A zero-configuration Go library to automatically expose existing Gin web framework APIs as MCP tools.
ryan0204/github-repo-mcp 📇 ☁️ 🪟 🐧 🍎 - GitHub Repo MCP allow your AI assistants browse GitHub repositories, explore directories, and view file contents.
alimo7amed93/webhook-tester-mcp 🐍 ☁️ – A FastMCP-based server for interacting with webhook-test.com. Enables users to create, retrieve, and delete webhooks locally using Claude.
lpigeon/ros-mcp-server 🐍 🏠 🍎 🪟 🐧 - The ROS MCP Server supports robot control by converting user-issued natural language commands into ROS or ROS2 control commands.
jsdelivr/globalping-mcp-server 🎖️ 📇 ☁️ - The Globalping MCP server provides users and LLMs access to run network tools like ping, traceroute, mtr, HTTP and DNS resolve from thousands of locations around the world.
posthog/mcp 🎖️ 📇 ☁️ - An MCP server for interacting with PostHog analytics, feature flags, error tracking and more.
🔒 Delivery
https://github.com/jordandalton/doordash-mcp-server 🐍 – DoorDash Delivery (Unofficial)
🧮 Data Science Tools
Integrations and tools designed to simplify data exploration, analysis and enhance data science workflows.

ChronulusAI/chronulus-mcp 🐍 ☁️ - Predict anything with Chronulus AI forecasting and prediction agents.
reading-plus-ai/mcp-server-data-exploration 🐍 ☁️ - Enables autonomous data exploration on .csv-based datasets, providing intelligent insights with minimal effort.
zcaceres/markdownify-mcp 📇 🏠 - An MCP server to convert almost any file or web content into Markdown
datalayer/jupyter-mcp-server 🐍 🏠 - Model Context Protocol (MCP) Server for Jupyter.
jjsantos01/jupyter-notebook-mcp 🐍 🏠 - connects Jupyter Notebook to Claude AI, allowing Claude to directly interact with and control Jupyter Notebooks.
arrismo/kaggle-mcp 🐍 ☁️ - Connects to Kaggle, ability to download and analyze datasets.
kdqed/zaturn 🐍 🏠 🪟 🐧 🍎 - Link multiple data sources (SQL, CSV, Parquet, etc.) and ask AI to analyze the data for insights and visualizations.
mckinsey/vizro-mcp 🎖️ 🐍 🏠 - Tools and templates to create validated and maintainable data charts and dashboards.
growthbook/growthbook-mcp 🎖️ 📇 🏠 🪟 🐧 🍎 — Tools for creating and interacting with GrowthBook feature flags and experiments.
📟 Embedded System
Provides access to documentation and shortcuts for working on embedded devices.

horw/esp-mcp 📟 - Workflow for fixing build issues in ESP32 series chips using ESP-IDF.
kukapay/modbus-mcp 🐍 📟 - An MCP server that standardizes and contextualizes industrial Modbus data.
kukapay/opcua-mcp 🐍 📟 - An MCP server that connects to OPC UA-enabled industrial systems.
yoelbassin/gnuradioMCP 🐍 📟 🏠 - An MCP server for GNU Radio that enables LLMs to autonomously create and modify RF .grc flowcharts.
📂 File Systems
Provides direct access to local file systems with configurable permissions. Enables AI models to read, write, and manage files within specified directories.

cyberchitta/llm-context.py 🐍 🏠 - Share code context with LLMs via MCP or clipboard
exoticknight/mcp-file-merger 🏎️ 🏠 - File merger tool, suitable for AI chat length limits.
filesystem@quarkiverse/quarkus-mcp-servers ☕ 🏠 - A filesystem allowing for browsing and editing files implemented in Java using Quarkus. Available as jar or native image.
hmk/box-mcp-server 📇 ☁️ - Box integration for listing, reading and searching files
mamertofabian/mcp-everything-search 🐍 🏠 🪟 - Fast Windows file search using Everything SDK
mark3labs/mcp-filesystem-server 🏎️ 🏠 - Golang implementation for local file system access.
mickaelkerjean/filestash 🏎️ ☁️ - Remote Storage Access: SFTP, S3, FTP, SMB, NFS, WebDAV, GIT, FTPS, gcloud, azure blob, sharepoint, etc.
microsoft/markitdown 🎖️ 🐍 🏠 - MCP tool access to MarkItDown -- a library that converts many file formats (local or remote) to Markdown for LLM consumption.
modelcontextprotocol/server-filesystem 📇 🏠 - Direct local file system access.
modelcontextprotocol/server-google-drive 📇 ☁️ - Google Drive integration for listing, reading, and searching files
Xuanwo/mcp-server-opendal 🐍 🏠 ☁️ - Access any storage with Apache OpenDAL™
jeannier/homebrew-mcp 🐍 🏠 🍎 - Control your macOS Homebrew setup using natural language via this MCP server. Simply manage your packages, or ask for suggestions, troubleshoot brew issues etc.
💰 Finance & Fintech
Financial data access and analysis tools. Enables AI models to work with market data, trading platforms, and financial information.

aaronjmars/web3-research-mcp 📇 ☁️ - Deep Research for crypto - free & fully local
alchemy/alchemy-mcp-server 🎖️ 📇 ☁️ - Allow AI agents to interact with Alchemy's blockchain APIs.
OctagonAI/octagon-mcp-server 🐍 ☁️ - Octagon AI Agents to integrate private and public market data
anjor/coinmarket-mcp-server 🐍 ☁️ - Coinmarket API integration to fetch cryptocurrency listings and quotes
ariadng/metatrader-mcp-server 🐍 🏠 🪟 - Enable AI LLMs to execute trades using MetaTrader 5 platform
armorwallet/armor-crypto-mcp 🐍 ☁️ - MCP to interface with multiple blockchains, staking, DeFi, swap, bridging, wallet management, DCA, Limit Orders, Coin Lookup, Tracking and more.
bankless/onchain-mcp 📇 ☁️ - Bankless Onchain API to interact with smart contracts, query transaction and token information
base/base-mcp 🎖️ 📇 ☁️ - Base Network integration for onchain tools, allowing interaction with Base Network and Coinbase API for wallet management, fund transfers, smart contracts, and DeFi operations
berlinbra/alpha-vantage-mcp 🐍 ☁️ - Alpha Vantage API integration to fetch both stock and crypto information
ahnlabio/bicscan-mcp 🎖️ 🐍 ☁️ - Risk score / asset holdings of EVM blockchain address (EOA, CA, ENS) and even domain names.
bitteprotocol/mcp 📇 - Bitte Protocol integration to run AI Agents on several blockchains.
chargebee/mcp 🎖️ 📇 ☁️ - MCP Server that connects AI agents to Chargebee platform.
codex-data/codex-mcp 🎖️ 📇 ☁️ - Codex API integration for real-time enriched blockchain and market data on 60+ networks
coinpaprika/dexpaprika-mcp 🎖️ 📇 ☁️ 🍎 🪟 🐧 - Coinpaprika's DexPaprika MCP server exposes high-performance DexPaprika API covering 20+ chains and 5M+ tokens with real time pricing, liquidity pool data & historical OHLCV data, providing AI agents standardized access to comprehensive market data through Model Context Protocol.
doggybee/mcp-server-ccxt 📇 ☁️ - An MCP server for accessing real-time crypto market data and trading via 20+ exchanges using the CCXT library. Supports spot, futures, OHLCV, balances, orders, and more.
ferdousbhai/investor-agent 🐍 ☁️ - Yahoo Finance integration to fetch stock market data including options recommendations
ferdousbhai/tasty-agent 🐍 ☁️ - Tastyworks API integration to handle trading activities on Tastytrade
ferdousbhai/wsb-analyst-mcp 🐍 ☁️ - Reddit integration to analyze content on WallStreetBets community
getalby/nwc-mcp-server 📇 🏠 - Bitcoin Lightning wallet integration powered by Nostr Wallet Connect
heurist-network/heurist-mesh-mcp-server 🎖️ ⛅️ 🏠 🐍 - Access specialized web3 AI agents for blockchain analysis, smart contract security auditing, token metrics evaluation, and on-chain interactions through the Heurist Mesh network. Provides comprehensive tools for DeFi analysis, NFT valuation, and transaction monitoring across multiple blockchains
intentos-labs/beeper-mcp 🐍 - Beeper provides transactions on BSC, including balance/token transfers, token swaps in Pancakeswap and beeper reward claims.
kukapay/blockbeats-mcp 🐍 ☁️ - An MCP server that delivers blockchain news and in-depth articles from BlockBeats for AI agents.
kukapay/bridge-rates-mcp 📇 ☁️ - Delivering real-time cross-chain bridge rates and optimal transfer routes to onchain AI agents.
kukapay/chainlink-feeds-mcp 📇 ☁️ - Providing real-time access to Chainlink's decentralized on-chain price feeds.
kukapay/cointelegraph-mcp 🐍 ☁️ - Providing real-time access to the latest news from Cointelegraph.
kukapay/crypto-feargreed-mcp 🐍 ☁️ - Providing real-time and historical Crypto Fear & Greed Index data.
kukapay/crypto-indicators-mcp 🐍 ☁️ - An MCP server providing a range of cryptocurrency technical analysis indicators and strategie.
kukapay/crypto-news-mcp 🐍 ☁️ - An MCP server that provides real-time cryptocurrency news sourced from NewsData for AI agents.
kukapay/crypto-portfolio-mcp 🐍 ☁️ - An MCP server for tracking and managing cryptocurrency portfolio allocations.
kukapay/crypto-rss-mcp 🐍 ☁️ - An MCP server that aggregates real-time cryptocurrency news from multiple RSS feeds.
kukapay/crypto-sentiment-mcp 🐍 ☁️ - An MCP server that delivers cryptocurrency sentiment analysis to AI agents.
kukapay/crypto-trending-mcp 🐍 ☁️ - Tracking the latest trending tokens on CoinGecko.
kukapay/crypto-whitepapers-mcp 🐍 ☁️ - Serving as a structured knowledge base of crypto whitepapers.
kukapay/cryptopanic-mcp-server 🐍 ☁️ - Providing latest cryptocurrency news to AI agents, powered by CryptoPanic.
kukapay/defi-yields-mcp 🐍 ☁️ - An MCP server for AI agents to explore DeFi yield opportunities.
kukapay/dune-analytics-mcp 🐍 ☁️ - A mcp server that bridges Dune Analytics data to AI agents.
kukapay/etf-flow-mcp 🐍 ☁️ - Delivering crypto ETF flow data to power AI agents' decision-making.
kukapay/freqtrade-mcp 🐍 ☁️ - An MCP server that integrates with the Freqtrade cryptocurrency trading bot.
kukapay/funding-rates-mcp 🐍 ☁️ - Providing real-time funding rate data across major crypto exchanges.
kukapay/jupiter-mcp 🐍 ☁️ - An MCP server for executing token swaps on the Solana blockchain using Jupiter's new Ultra API.
kukapay/pancakeswap-poolspy-mcp 🐍 ☁️ - An MCP server that tracks newly created pools on Pancake Swap.
kukapay/rug-check-mcp 🐍 ☁️ - An MCP server that detects potential risks in Solana meme tokens.
kukapay/thegraph-mcp 🐍 ☁️ - An MCP server that powers AI agents with indexed blockchain data from The Graph.
kukapay/token-minter-mcp 🐍 ☁️ - An MCP server providing tools for AI agents to mint ERC-20 tokens across multiple blockchains.
kukapay/token-revoke-mcp 🐍 ☁️ - An MCP server for checking and revoking ERC-20 token allowances across multiple blockchains.
kukapay/twitter-username-changes-mcp 🐍 ☁️ - An MCP server that tracks the historical changes of Twitter usernames.
kukapay/uniswap-poolspy-mcp 🐍 ☁️ - An MCP server that tracks newly created liquidity pools on Uniswap across multiple blockchains.
kukapay/uniswap-trader-mcp 🐍 ☁️ - An MCP server for AI agents to automate token swaps on Uniswap DEX across multiple blockchains.
kukapay/whale-tracker-mcp 🐍 ☁️ - A mcp server for tracking cryptocurrency whale transactions.
laukikk/alpaca-mcp 🐍 ☁️ - An MCP Server for the Alpaca trading API to manage stock and crypto portfolios, place trades, and access market data.
longportapp/openapi - 🐍 ☁️ - LongPort OpenAPI provides real-time stock market data, provides AI access analysis and trading capabilities through MCP.
mcpdotdirect/evm-mcp-server 📇 ☁️ - Comprehensive blockchain services for 30+ EVM networks, supporting native tokens, ERC20, NFTs, smart contracts, transactions, and ENS resolution.
mcpdotdirect/starknet-mcp-server 📇 ☁️ - Comprehensive Starknet blockchain integration with support for native tokens (ETH, STRK), smart contracts, StarknetID resolution, and token transfers.
minhyeoky/mcp-server-ledger 🐍 🏠 - A ledger-cli integration for managing financial transactions and generating reports.
openMF/mcp-mifosx ☁️ 🏠 - A core banking integration for managing clients, loans, savings, shares, financial transactions and generating financial reports.
narumiruna/yfinance-mcp 🐍 ☁️ - An MCP server that uses yfinance to obtain information from Yahoo Finance.
polygon-io/mcp_polygon)) 🐍 ☁️ - An MCP server that provides access to Polygon.io financial market data APIs for stocks, indices, forex, options, and more.
pwh-pwh/coin-mcp-server 🐍 ☁️ - Bitget API to fetch cryptocurrency price.
QuantGeekDev/coincap-mcp 📇 ☁️ - Real-time cryptocurrency market data integration using CoinCap's public API, providing access to crypto prices and market information without API keys
SaintDoresh/Crypto-Trader-MCP-ClaudeDesktop 🐍 ☁️ - An MCP tool that provides cryptocurrency market data using the CoinGecko API.
tooyipjee/yahoofinance-mcp 📇 ☁️ - TS version of yahoo finance mcp.
SaintDoresh/YFinance-Trader-MCP-ClaudeDesktop 🐍 ☁️ - An MCP tool that provides stock market data and analysis using the Yahoo Finance API.
RomThpt/xrpl-mcp-server 📇 ☁️ - MCP server for the XRP Ledger that provides access to account information, transaction history, and network data. Allows querying ledger objects, submitting transactions, and monitoring the XRPL network.
janswist/mcp-dexscreener 📇 ☁️ - Real-time on-chain market prices using open and free Dexscreener API
HuggingAGI/mcp-baostock-server 🐍 ☁️ - MCP server based on baostock, providing access and analysis capabilities for Chinese stock market data.
wowinter13/solscan-mcp 🦀 🏠 - An MCP tool for querying Solana transactions using natural language with Solscan API.
Wuye-AI/mcp-server-wuye-ai 🎖️ 📇 ☁️ - An MCP server that interact with capabilities of the CRIC Wuye AI platform, an intelligent assistant specifically for the property management industry.
zlinzzzz/finData-mcp-server 🐍 ☁️ - An MCP server for accessing professional financial data, supporting multiple data providers such as Tushare.
🎮 Gaming
Integration with gaming related data, game engines, and services

IvanMurzak/Unity-MCP #️⃣ 🏠 🍎 🪟 🐧 - MCP Server for Unity Editor and for a game made with Unity
CoderGamester/mcp-unity #️⃣ 🏠 - MCP Server for Unity3d Game Engine integration for game development
Coding-Solo/godot-mcp 📇 🏠 - A MCP server for interacting with the Godot game engine, providing tools for editing, running, debugging, and managing scenes in Godot projects.
pab1ito/chess-mcp 🐍 ☁️ - Access Chess.com player data, game records, and other public information through standardized MCP interfaces, allowing AI assistants to search and analyze chess information.
jiayao/mcp-chess 🐍 🏠 - A MCP server playing chess against LLMs.
rishijatia/fantasy-pl-mcp 🐍 ☁️ - An MCP server for real-time Fantasy Premier League data and analysis tools.
opgginc/opgg-mcp 📇 ☁️ - Access real-time gaming data across popular titles like League of Legends, TFT, and Valorant, offering champion analytics, esports schedules, meta compositions, and character statistics.
stefan-xyz/mcp-server-runescape 📇 - An MCP server with tools for interacting with RuneScape (RS) and Old School RuneScape (OSRS) data, including item prices, player hiscores, and more.
🧠 Knowledge & Memory
Persistent memory storage using knowledge graph structures. Enables AI models to maintain and query structured information across sessions.

CheMiguel23/MemoryMesh 📇 🏠 - Enhanced graph-based memory with a focus on AI role-play and story generation
graphlit-mcp-server 📇 ☁️ - Ingest anything from Slack, Discord, websites, Google Drive, Linear or GitHub into a Graphlit project - and then search and retrieve relevant knowledge within an MCP client like Cursor, Windsurf or Cline.
hannesrudolph/mcp-ragdocs 🐍 🏠 - An MCP server implementation that provides tools for retrieving and processing documentation through vector search, enabling AI assistants to augment their responses with relevant documentation context
jinzcdev/markmap-mcp-server 📇 🏠 - An MCP server built on markmap that converts Markdown to interactive mind maps. Supports multi-format exports (PNG/JPG/SVG), live browser preview, one-click Markdown copy, and dynamic visualization features.
kaliaboi/mcp-zotero 📇 ☁️ - A connector for LLMs to work with collections and sources on your Zotero Cloud
mcp-summarizer 📕 ☁️ - AI Summarization MCP Server, Support for multiple content types: Plain text, Web pages, PDF documents, EPUB books, HTML content
mem0ai/mem0-mcp 🐍 🏠 - A Model Context Protocol server for Mem0 that helps manage coding preferences and patterns, providing tools for storing, retrieving and semantically handling code implementations, best practices and technical documentation in IDEs like Cursor and Windsurf
modelcontextprotocol/server-memory 📇 🏠 - Knowledge graph-based persistent memory system for maintaining context
pinecone-io/assistant-mcp 🎖️ 🦀 ☁️ - Connects to your Pinecone Assistant and gives the agent context from its knowledge engine.
@ragieai/mcp-server 📇 ☁️ - Retrieve context from your Ragie (RAG) knowledge base connected to integrations like Google Drive, Notion, JIRA and more.
topoteretes/cognee 📇 🏠 - Memory manager for AI apps and Agents using various graph and vector stores and allowing ingestion from 30+ data sources
unibaseio/membase-mcp 📇 ☁️ - Save and query your agent memory in distributed way by Membase
GistPad-MCP 📇 🏠 - Use GitHub Gists to manage and access your personal knowledge, daily notes, and reusable prompts. This acts as a companion to https://gistpad.dev and the GistPad VS Code extension.
entanglr/zettelkasten-mcp 🐍 🏠 - A Model Context Protocol (MCP) server that implements the Zettelkasten knowledge management methodology, allowing you to create, link, and search atomic notes through Claude and other MCP-compatible clients.
🗺️ Location Services
Location-based services and mapping tools. Enables AI models to work with geographic data, weather information, and location-based analytics.

briandconnelly/mcp-server-ipinfo 🐍 ☁️ - IP address geolocation and network information using IPInfo API
devilcoder01/weather-mcp-server 🐍 ☁️ - Access real-time weather data for any location using the WeatherAPI.com API, providing detailed forecasts and current conditions.
jagan-shanmugam/open-streetmap-mcp 🐍 🏠 - An OpenStreetMap MCP server with location-based services and geospatial data.
kukapay/nearby-search-mcp 🐍 ☁️ - An MCP server for nearby place searches with IP-based location detection.
modelcontextprotocol/server-google-maps 📇 ☁️ - Google Maps integration for location services, routing, and place details
QGIS MCP - connects QGIS Desktop to Claude AI through the MCP. This integration enables prompt-assisted project creation, layer loading, code execution, and more.
SaintDoresh/Weather-MCP-ClaudeDesktop 🐍 ☁️ - An MCP tool that provides real-time weather data, forecasts, and historical weather information using the OpenWeatherMap API.
rossshannon/Weekly-Weather-mcp 🐍 ☁️ - Weekly Weather MCP server which returns 7 full days of detailed weather forecasts anywhere in the world.
SecretiveShell/MCP-timeserver 🐍 🏠 - Access the time in any timezone and get the current local time
TimLukaHorstmann/mcp-weather 📇 ☁️ - Accurate weather forecasts via the AccuWeather API (free tier available).
webcoderz/MCP-Geo 🐍 🏠 - Geocoding MCP server for nominatim, ArcGIS, Bing
ipfind/ipfind-mcp-server 🐍 ☁️ - IP Address location service using the IP Find API
mahdin75/geoserver-mcp 🏠 – A Model Context Protocol (MCP) server implementation that connects LLMs to the GeoServer REST API, enabling AI assistants to interact with geospatial data and services.
ipfred/aiwen-mcp-server-geoip 🐍 📇 ☁️ – MCP Server for the Aiwen IP Location, Get user network IP location, get IP details (country, province, city, lat, lon, ISP, owner, etc.)
🎯 Marketing
Tools for creating and editing marketing content, working with web meta data, product positioning, and editing guides.

gomarble-ai/facebook-ads-mcp-server 🐍 ☁️ - MCP server acting as an interface to the Facebook Ads, enabling programmatic access to Facebook Ads data and management features.
open-strategy-partners/osp_marketing_tools 🐍 🏠 - A suite of marketing tools from Open Strategy Partners including writing style, editing codes, and product marketing value map creation.
nictuku/meta-ads-mcp 🐍 ☁️ 🏠 - Enables AI agents to monitor and optimize Meta ad performance, analyze campaign metrics, adjust audience targeting, manage creative assets, and make data-driven recommendations for ad spend and campaign settings through seamless Graph API integration.
marketplaceadpros/amazon-ads-mcp-server 📇 ☁️ - Enables tools to interact with Amazon Advertising, analyzing campaign metrics and configurations.
📊 Monitoring
Access and analyze application monitoring data. Enables AI models to review error reports and performance metrics.

netdata/netdata#Netdata 🎖️ 🏠 ☁️ 📟 🍎 🪟 🐧 - Discovery, exploration, reporting and root cause analysis using all observability data, including metrics, logs, systems, containers, processes, and network connections
grafana/mcp-grafana 🎖️ 🐍 🏠 ☁️ - Search dashboards, investigate incidents and query datasources in your Grafana instance
tumf/grafana-loki-mcp 🐍 🏠 - An MCP server that allows querying Loki logs through the Grafana API.
hyperb1iss/lucidity-mcp 🐍 🏠 - Enhance AI-generated code quality through intelligent, prompt-based analysis across 10 critical dimensions from complexity to security vulnerabilities
last9/last9-mcp-server - Seamlessly bring real-time production context—logs, metrics, and traces—into your local environment to auto-fix code faster
metoro-io/metoro-mcp-server 🎖️ 🏎️ ☁️ - Query and interact with kubernetes environments monitored by Metoro
MindscapeHQ/server-raygun 📇 ☁️ - Raygun API V3 integration for crash reporting and real user monitoring
modelcontextprotocol/server-sentry 🐍 ☁️ - Sentry.io integration for error tracking and performance monitoring
pydantic/logfire-mcp 🎖️ 🐍 ☁️ - Provides access to OpenTelemetry traces and metrics through Logfire
seekrays/mcp-monitor 🏎️ 🏠 - A system monitoring tool that exposes system metrics via the Model Context Protocol (MCP). This tool allows LLMs to retrieve real-time system information through an MCP-compatible interface.（support CPU、Memory、Disk、Network、Host、Process）
VictoriaMetrics-Community/mcp-victoriametrics 🎖️ 🏎️ 🏠 - Provides comprehensive integration with your VictoriaMetrics instance APIs and documentation for monitoring, observability, and debugging tasks related to your VictoriaMetrics instances
🎥 Multimedia Process
Provides the ability to handle multimedia, such as audio and video editing, playback, format conversion, also includes video filters, enhancements, and so on

video-creator/ffmpeg-mcp 🎥 🔊 - Using ffmpeg command line to achieve an mcp server, can be very convenient, through the dialogue to achieve the local video search, tailoring, stitching, playback and other functions
stass/exif-mcp 📇 🏠 🐧 🍎 🪟 - A MCP server that allows one to examine image metadata like EXIF, XMP, JFIF and GPS. This provides foundation for LLM-powered search and analysis of photo librares and image collections.
🔎 Search & Data Extraction
Xyber-Labs/mcp-server-youtube 🐍 ☁️ - This repository implements an MCP (Model Context Protocol) server for YouTube search and transcript retrieval functionality. It allows language models or other agents to easily query YouTube content through a standardized protocol.
ricocf/mcp-wolframalpha 🐍 🏠 ☁️ - An MCP server lets AI assistants use the Wolfram Alpha API for real-time access to computational knowledge and data.
scrapeless-ai/scrapeless-mcp-server 🐍 ☁️ - The Scrapeless Model Context Protocol service acts as an MCP server connector to the Google SERP API, enabling web search within the MCP ecosystem without leaving it.
0xdaef0f/job-searchoor 📇 🏠 - An MCP server for searching job listings with filters for date, keywords, remote work options, and more.
ac3xx/mcp-servers-kagi 📇 ☁️ - Kagi search API integration
andybrandt/mcp-simple-arxiv - 🐍 ☁️ MCP for LLM to search and read papers from arXiv
hbg/mcp-paperswithcode - 🐍 ☁️ MCP to search through PapersWithCode API
andybrandt/mcp-simple-pubmed - 🐍 ☁️ MCP to search and read medical / life sciences papers from PubMed.
angheljf/nyt 📇 ☁️ - Search articles using the NYTimes API
apify/mcp-server-rag-web-browser 📇 ☁️ - An MCP server for Apify's open-source RAG Web Browser Actor to perform web searches, scrape URLs, and return content in Markdown.
Bigsy/Clojars-MCP-Server 📇 ☁️ - Clojars MCP Server for upto date dependency information of Clojure libraries
blazickjp/arxiv-mcp-server ☁️ 🐍 - Search ArXiv research papers
luminati-io/brightdata-mcp 📇 ☁️ - Discover, extract, and interact with the web - one interface powering automated access across the public internet.
chanmeng/google-news-mcp-server 📇 ☁️ - Google News integration with automatic topic categorization, multi-language support, and comprehensive search capabilities including headlines, stories, and related topics through SerpAPI.
ConechoAI/openai-websearch-mcp 🐍 🏠 ☁️ - This is a Python-based MCP server that provides OpenAI web_search build-in tool.
dealx/mcp-server ☁️ - MCP Server for DealX platform
devflowinc/trieve 🎖️ 📇 ☁️ 🏠 - Crawl, embed, chunk, search, and retrieve information from datasets through Trieve
Dumpling-AI/mcp-server-dumplingai 🎖️ 📇 ☁️ - Access data, web scraping, and document conversion APIs by Dumpling AI
erithwik/mcp-hn 🐍 ☁️ - An MCP server to search Hacker News, get top stories, and more.
exa-labs/exa-mcp-server 🎖️ 📇 ☁️ – A Model Context Protocol (MCP) server lets AI assistants like Claude use the Exa AI Search API for web searches. This setup allows AI models to get real-time web information in a safe and controlled way.
fatwang2/search1api-mcp 📇 ☁️ - Search via search1api (requires paid API key)
genomoncology/biomcp 🐍 ☁️ - Biomedical research server providing access to PubMed, ClinicalTrials.gov, and MyVariant.info.
hellokaton/unsplash-mcp-server) 🐍 ☁️ - A MCP server for Unsplash image search.
Ihor-Sokoliuk/MCP-SearXNG 📇 🏠/☁️ - A Model Context Protocol Server for SearXNG
isnow890/naver-search-mcp 📇 ☁️ - MCP server for Naver Search API integration, supporting blog, news, shopping search and DataLab analytics features.
jae-jae/fetcher-mcp 📇 🏠 - MCP server for fetching web page content using Playwright headless browser, supporting Javascript rendering and intelligent content extraction, and outputting Markdown or HTML format.
jae-jae/g-search-mcp 📇 🏠 - A powerful MCP server for Google search that enables parallel searching with multiple keywords simultaneously.
ananddtyagi/webpage-screenshot-mcp 📇 🏠 - A MCP server for taking screenshots of webpages to use as feedback during UI developement.
leehanchung/bing-search-mcp 📇 ☁️ - Web search capabilities using Microsoft Bing Search API
kagisearch/kagimcp ☁️ 📇 – Official Kagi Search MCP Server
kshern/mcp-tavily ☁️ 📇 – Tavily AI search API
mikechao/brave-search-mcp 📇 ☁️ - Web, Image, News, Video, and Local Point of Interest search capabilities using Brave's Search API
emicklei/melrose-mcp 🏎️ 🏠 - Plays Melrōse music expressions as MIDI
modelcontextprotocol/server-brave-search 📇 ☁️ - Web search capabilities using Brave's Search API
modelcontextprotocol/server-fetch 🐍 🏠 ☁️ - Efficient web content fetching and processing for AI consumption
mzxrai/mcp-webresearch 🔍📚 - Search Google and do deep web research on any topic
nickclyde/duckduckgo-mcp-server 🐍 ☁️ - Web search using DuckDuckGo
r-huijts/opentk-mcp 📇 ☁️ - Access Dutch Parliament (Tweede Kamer) information including documents, debates, activities, and legislative cases through structured search capabilities (based on opentk project by Bert Hubert)
reading-plus-ai/mcp-server-deep-research 📇 ☁️ - MCP server providing OpenAI/Perplexity-like autonomous deep research, structured query elaboration, and concise reporting.
SecretiveShell/MCP-searxng 🐍 🏠 - An MCP Server to connect to searXNG instances
takashiishida/arxiv-latex-mcp 🐍 ☁️ - Get the LaTeX source of arXiv papers to handle mathematical content and equations
the0807/GeekNews-MCP-Server 🐍 ☁️ - An MCP Server that retrieves and processes news data from the GeekNews site.
tinyfish-io/agentql-mcp 🎖️ 📇 ☁️ - MCP server that provides AgentQL's data extraction capabilities.
Tomatio13/mcp-server-tavily ☁️ 🐍 – Tavily AI search API
vectorize-io/vectorize-mcp-server ☁️ 📇 - Vectorize MCP server for advanced retrieval, Private Deep Research, Anything-to-Markdown file extraction and text chunking.
webscraping-ai/webscraping-ai-mcp-server 🎖️ 📇 ☁️ - Interact with WebScraping.ai for web data extraction and scraping.
zhsama/duckduckgo-mcp-server 📇 🏠 ☁️ - This is a TypeScript-based MCP server that provides DuckDuckGo search functionality.
zoomeye-ai/mcp_zoomeye 📇 ☁️ - Querying network asset information by ZoomEye MCP Server
yamanoku/baseline-mcp-server 📇 🏠 - MCP server that searches Baseline status using Web Platform API
longevity-genie/biothings-mcp 🐍 ☁️ - MCP server to interact with BioThings API, including genes, genetic variants, drugs, and taxonomic information
🔒 Security
LaurieWired/GhidraMCP ☕ 🏠 - A Model Context Protocol server for Ghidra that enables LLMs to autonomously reverse engineer applications. Provides tools for decompiling binaries, renaming methods and data, and listing methods, classes, imports, and exports.
dkvdm/onepassword-mcp-server - An MCP server that enables secure credential retrieval from 1Password to be used by Agentic AI.
firstorderai/authenticator_mcp 📇 🏠 🍎 🪟 🐧 – A secure MCP (Model Context Protocol) server that enables AI agents to interact with the Authenticator App.
13bm/GhidraMCP 🐍 ☕ 🏠 - MCP server for integrating Ghidra with AI assistants. This plugin enables binary analysis, providing tools for function inspection, decompilation, memory exploration, and import/export analysis via the Model Context Protocol.
atomicchonk/roadrecon_mcp_server 🐍 🪟 🏠 MCP server for analyzing ROADrecon gather results from Azure tenant enumeration
BurtTheCoder/mcp-dnstwist 📇 🪟 ☁️ - MCP server for dnstwist, a powerful DNS fuzzing tool that helps detect typosquatting, phishing, and corporate espionage.
BurtTheCoder/mcp-maigret 📇 🪟 ☁️ - MCP server for maigret, a powerful OSINT tool that collects user account information from various public sources. This server provides tools for searching usernames across social networks and analyzing URLs.
BurtTheCoder/mcp-shodan 📇 🪟 ☁️ - MCP server for querying the Shodan API and Shodan CVEDB. This server provides tools for IP lookups, device searches, DNS lookups, vulnerability queries, CPE lookups, and more.
BurtTheCoder/mcp-virustotal 📇 🪟 ☁️ - MCP server for querying the VirusTotal API. This server provides tools for scanning URLs, analyzing file hashes, and retrieving IP address reports.
fosdickio/binary_ninja_mcp 🐍 🏠 🍎 🪟 🐧 - A Binary Ninja plugin, MCP server, and bridge that seamlessly integrates Binary Ninja with your favorite MCP client. It enables you to automate the process of performing binary analysis and reverse engineering.
fr0gger/MCP_Security 📇 ☁️ - MCP server for querying the ORKL API. This server provides tools for fetching threat reports, analyzing threat actors, and retrieving intelligence sources.
gbrigandi/mcp-server-cortex 🦀 🏠 🚨 🍎 🪟 🐧 - A Rust-based MCP server to integrate Cortex, enabling observable analysis and automated security responses through AI.
gbrigandi/mcp-server-thehive 🦀 🏠 🚨 🍎 🪟 🐧 - A Rust-based MCP server to integrate TheHive, facilitating collaborative security incident response and case management via AI.
gbrigandi/mcp-server-wazuh 🦀 🏠 🚨 🍎 🪟 🐧 - A Rust-based MCP server bridging Wazuh SIEM with AI assistants, providing real-time security alerts and event data for enhanced contextual understanding.
intruder-io/intruder-mcp 🐍 ☁️ - MCP server to access Intruder, helping you identify, understand, and fix security vulnerabilities in your infrastructure.
jyjune/mcp_vms 🐍 🏠 🪟 - A Model Context Protocol (MCP) server designed to connect to a CCTV recording program (VMS) to retrieve recorded and live video streams. It also provides tools to control the VMS software, such as showing live or playback dialogs for specific channels at specified times.
qianniuspace/mcp-security-audit 📇 ☁️ A powerful MCP (Model Context Protocol) Server that audits npm package dependencies for security vulnerabilities. Built with remote npm registry integration for real-time security checks.
semgrep/mcp 📇 ☁️ Allow AI agents to scan code for security vulnerabilites using Semgrep.
slouchd/cyberchef-api-mcp-server 🐍 ☁️ - MCP server for interacting with the CyberChef server API which will allow an MCP client to utilise the CyberChef operations.
mrexodia/ida-pro-mcp 🐍 🏠 - MCP server for IDA Pro, allowing you to perform binary analysis with AI assistants. This plugin implement decompilation, disassembly and allows you to generate malware analysis reports automatically.
rad-security/mcp-server 📇 ☁️ - MCP server for RAD Security, providing AI-powered security insights for Kubernetes and cloud environments. This server provides tools for querying the Rad Security API and retrieving security findings, reports, runtime data and many more.
securityfortech/secops-mcp 🐍 🏠 - All-in-one security testing toolbox that brings together popular open source tools through a single MCP interface. Connected to an AI agent, it enables tasks like pentesting, bug bounty hunting, threat hunting, and more.
roadwy/cve-search_mcp 🐍 🏠 - A Model Context Protocol (MCP) server for querying the CVE-Search API. This server provides comprehensive access to CVE-Search, browse vendor and product、get CVE per CVE-ID、get the last updated CVEs.
StacklokLabs/osv-mcp 🏎️ ☁️ - Access the OSV (Open Source Vulnerabilities) database for vulnerability information. Query vulnerabilities by package version or commit, batch query multiple packages, and get detailed vulnerability information by ID.
nickpending/mcp-recon 🏎️ 🏠 - Conversational recon interface and MCP server powered by httpx and asnmap. Supports various reconnaissance levels for domain analysis, security header inspection, certificate analysis, and ASN lookup.
Gaffx/volatility-mcp - MCP server for Volatility 3.x, allowing you to perform memory forensics analysis with AI assistant. Experience memory forensics without barriers as plugins like pslist and netscan become accessible through clean REST APIs and LLMs.
co-browser/attestable-mcp-server 🐍 🏠 ☁️ 🐧 - An MCP server running inside a trusted execution environment (TEE) via Gramine, showcasing remote attestation using RA-TLS. This allows an MCP client to verify the server before conencting.
zinja-coder/jadx-ai-mcp ☕ 🏠 - JADX-AI-MCP is a plugin and MCP Server for the JADX decompiler that integrates directly with Model Context Protocol (MCP) to provide live reverse engineering support with LLMs like Claude.
zinja-coder/apktool-mcp-server 🐍 🏠 - APKTool MCP Server is a MCP server for the Apk Tool to provide automation in reverse engineering of Android APKs.
🌐 Social Media
Integration with social media platforms to allow posting, analytics, and interaction management. Enables AI-driven automation for social presence.

macrocosm-os/macrocosmos-mcp - 🎖️ 🐍 ☁️ Access real-time X/Reddit/YouTube data directly in your LLM applications with search phrases, users, and date filtering.
kunallunia/twitter-mcp 🐍 🏠 - All-in-one Twitter management solution providing timeline access, user tweet retrieval, hashtag monitoring, conversation analysis, direct messaging, sentiment analysis of a post, and complete post lifecycle control - all through a streamlined API.
HagaiHen/facebook-mcp-server 🐍 ☁️ - Integrates with Facebook Pages to enable direct management of posts, comments, and engagement metrics through the Graph API for streamlined social media management.
gwbischof/bluesky-social-mcp 🐍 🏠 - An MCP server for interacting with Bluesky via the atproto client.
🏃 Sports
Tools for accessing sports-related data, results, and statistics.

mikechao/balldontlie-mcp 📇 - MCP server that integrates balldontlie api to provide information about players, teams and games for the NBA, NFL and MLB
r-huijts/firstcycling-mcp 📇 ☁️ - Access cycling race data, results, and statistics through natural language. Features include retrieving start lists, race results, and rider information from firstcycling.com.
r-huijts/strava-mcp 📇 ☁️ - A Model Context Protocol (MCP) server that connects to Strava API, providing tools to access Strava data through LLMs
willvelida/mcp-afl-server ☁️ - MCP server that integrates with the Squiggle API to provide information on Australian Football League teams, ladder standings, results, tips, and power rankings.
guillochon/mlb-api-mcp 🐍 🏠 - MCP server that acts as a proxy to the freely available MLB API, which provides player info, stats, and game information.
🎧 Support & Service Management
Tools for managing customer support, IT service management, and helpdesk operations.

effytech/freshdesk-mcp 🐍 ☁️ - MCP server that integrates with Freshdesk, enabling AI models to interact with Freshdesk modules and perform various support operations.
nguyenvanduocit/jira-mcp 🏎️ ☁️ - A Go-based MCP connector for Jira that enables AI assistants like Claude to interact with Atlassian Jira. This tool provides a seamless interface for AI models to perform common Jira operations including issue management, sprint planning, and workflow transitions.
sooperset/mcp-atlassian 🐍 ☁️ - MCP server for Atlassian products (Confluence and Jira). Supports Confluence Cloud, Jira Cloud, and Jira Server/Data Center. Provides comprehensive tools for searching, reading, creating, and managing content across Atlassian workspaces.
🌎 Translation Services
Translation tools and services to enable AI assistants to translate content between different languages.

translated/lara-mcp 🎖️ 📇 ☁️ - MCP Server for Lara Translate API, enabling powerful translation capabilities with support for language detection and context-aware translations.
mmntm/weblate-mcp 📇 ☁️ - Comprehensive Model Context Protocol server for Weblate translation management, enabling AI assistants to perform translation tasks, project management, and content discovery with smart format transformations.
🎧 Text-to-Speech
Tools for converting text-to-speech and vice-versa

mberg/kokoro-tts-mcp 🐍 🏠 - MCP Server that uses the open weight Kokoro TTS models to convert text-to-speech. Can convert text to MP3 on a local driver or auto-upload to an S3 bucket.
mbailey/voice-mcp 🐍 🏠 - Complete voice interaction server supporting speech-to-text, text-to-speech, and real-time voice conversations through local microphone, OpenAI-compatible APIs, and LiveKit integration
🚆 Travel & Transportation
Access to travel and transportation information. Enables querying schedules, routes, and real-time travel data.

Airbnb MCP Server 📇 ☁️ - Provides tools to search Airbnb and get listing details.
KyrieTangSheng/mcp-server-nationalparks 📇 ☁️ - National Park Service API integration providing latest information of park details, alerts, visitor centers, campgrounds, and events for U.S. National Parks
NS Travel Information MCP Server 📇 ☁️ - Access Dutch Railways (NS) travel information, schedules, and real-time updates
pab1it0/tripadvisor-mcp 📇 🐍 - A MCP server that enables LLMs to interact with Tripadvisor API, supporting location data, reviews, and photos through standardized MCP interfaces
lucygoodchild/mcp-national-rail 📇 ☁️ - An MCP server for UK National Rail trains service, providing train schedules and live travel information, intergrating the Realtime Trains API
🔄 Version Control
Interact with Git repositories and version control platforms. Enables repository management, code analysis, pull request handling, issue tracking, and other version control operations through standardized APIs.

adhikasp/mcp-git-ingest 🐍 🏠 - Read and analyze GitHub repositories with your LLM
ddukbg/github-enterprise-mcp 📇 ☁️ 🏠 - MCP server for GitHub Enterprise API integration
gitea/gitea-mcp 🎖️ 🏎️ ☁️ 🏠 🍎 🪟 🐧 - Interactive with Gitea instances with MCP.
github/github-mcp-server 📇 ☁️ - Official GitHub server for integration with repository management, PRs, issues, and more.
kopfrechner/gitlab-mr-mcp 📇 ☁️ - Interact seamlessly with issues and merge requests of your GitLab projects.
modelcontextprotocol/server-git 🐍 🏠 - Direct Git repository operations including reading, searching, and analyzing local repositories
modelcontextprotocol/server-gitlab 📇 ☁️ 🏠 - GitLab platform integration for project management and CI/CD operations
oschina/mcp-gitee 🏎️ ☁️ 🏠 - Gitee API integration, repository, issue, and pull request management, and more.
Tiberriver256/mcp-server-azure-devops 📇 ☁️ - Azure DevOps integration for repository management, work items, and pipelines.
kaiyuanxiaobing/atomgit-mcp-server 📇 ☁️ - Official AtomGit server for integration with repository management, PRs, issues, branches, labels, and more.
🛠️ Other Tools and Integrations
AbdelStark/bitcoin-mcp - ₿ A Model Context Protocol (MCP) server that enables AI models to interact with Bitcoin, allowing them to generate keys, validate addresses, decode transactions, query the blockchain, and more.
akseyh/bear-mcp-server - Allows the AI to read from your Bear Notes (macOS only)
allenporter/mcp-server-home-assistant 🐍 🏠 - Expose all Home Assistant voice intents through a Model Context Protocol Server allowing home control.
Amazon Bedrock Nova Canvas 📇 ☁️ - Use Amazon Nova Canvas model for image generation.
amidabuddha/unichat-mcp-server 🐍/📇 ☁️ - Send requests to OpenAI, MistralAI, Anthropic, xAI, Google AI or DeepSeek using MCP protocol via tool or predefined prompts. Vendor API key required
anaisbetts/mcp-installer 🐍 🏠 - An MCP server that installs other MCP servers for you.
anaisbetts/mcp-youtube 📇 ☁️ - Fetch YouTube subtitles
andybrandt/mcp-simple-openai-assistant - 🐍 ☁️ MCP to talk to OpenAI assistants (Claude can use any GPT model as his assitant)
andybrandt/mcp-simple-timeserver 🐍 🏠☁️ - An MCP server that allows checking local time on the client machine or current UTC time from an NTP server
apify/actors-mcp-server 📇 ☁️ - Use 3,000+ pre-built cloud tools, known as Actors, to extract data from websites, e-commerce, social media, search engines, maps, and more
apinetwork/piapi-mcp-server 📇 ☁️ PiAPI MCP server makes user able to generate media content with Midjourney/Flux/Kling/Hunyuan/Udio/Trellis directly from Claude or any other MCP-compatible apps.
awkoy/replicate-flux-mcp 📇 ☁️ - Provides the ability to generate images via Replicate's API.
awwaiid/mcp-server-taskwarrior 🏠 📇 - An MCP server for basic local taskwarrior usage (add, update, remove tasks)
baba786/phabricator-mcp-server 🐍 ☁️ - Interacting with Phabricator API
Badhansen/notion-mcp 🐍 ☁️ - A Model Context Protocol (MCP) server that integrates with Notion's API to manage personal todo lists efficiently.
bart6114/my-bear-mcp-server 📇 🏠 🍎 - Allows to read notes and tags for the Bear Note taking app, through a direct integration with Bear's sqlitedb.
billster45/mcp-chatgpt-responses 🐍 ☁️ - MCP server for Claude to talk to ChatGPT and use its web search capability.
blurrah/mcp-graphql 📇 ☁️ - Allows the AI to query GraphQL servers
calclavia/mcp-obsidian 📇 🏠 - This is a connector to allow Claude Desktop (or any MCP client) to read and search any directory containing Markdown notes (such as an Obsidian vault).
chrishayuk/mcp-cli 🐍 🏠 - Yet another CLI tool for testing MCP servers
danhilse/notion_mcp 🐍 ☁️ - Integrates with Notion's API to manage personal todo lists
EKibort/wrike-mcp-server - 🐍 🏠 - A lightweight implementation of a Wrike MCP server for interacting with Wrike tasks via public API.
ekkyarmandi/ticktick-mcp 🐍 ☁️ - TickTick MCP server that integrates with TickTick's API to manage personal todo projects and the tasks.
esignaturescom/mcp-server-esignatures 🐍 ☁️️ - Contract and template management for drafting, reviewing, and sending binding contracts via the eSignatures API.
evalstate/mcp-miro 📇 ☁️ - Access MIRO whiteboards, bulk create and read items. Requires OAUTH key for REST API.
feuerdev/keep-mcp 🐍 ☁️ - Read, create, update and delete Google Keep notes.
future-audiences/wikimedia-enterprise-model-context-protocol 🐍 ☁️ - Wikipedia Article lookup API
fotoetienne/gqai 🏎 🏠 - Define tools using regular GraphQL queries/mutations and gqai automatically generates an MCP server for you.
githejie/mcp-server-calculator 🐍 🏠 - This server enables LLMs to use calculator for precise numerical calculations
gotoolkits/DifyWorkflow - 🏎️ ☁️ Tools to the query and execute of Dify workflows
hiromitsusasaki/raindrop-io-mcp-server 📇 ☁️ - An integration that allows LLMs to interact with Raindrop.io bookmarks using the Model Context Protocol (MCP).
hmk/attio-mcp-server - 📇 ☁️ Allows AI clients to manage records and notes in Attio CRM
isaacwasserman/mcp-vegalite-server 🐍 🏠 - Generate visualizations from fetched data using the VegaLite format and renderer.
ivnvxd/mcp-server-odoo 🐍 ☁️/🏠 - Connect AI assistants to Odoo ERP systems for business data access, record management, and workflow automation.
ivo-toby/contentful-mcp 📇 🏠 - Update, create, delete content, content-models and assets in your Contentful Space
j3k0/speech.sh 🏠 - Let the agent speak things out loud, notify you when he's done working with a quick summary
jagan-shanmugam/climatiq-mcp-server 🐍 🏠 - A Model Context Protocol (MCP) server for accessing the Climatiq API to calculate carbon emissions. This allows AI assistants to perform real-time carbon calculations and provide climate impact insights.
johannesbrandenburger/typst-mcp 🐍 🏠 - MCP server for Typst, a markup-based typesetting system. It provides tools for converting between LaTeX and Typst, validating Typst syntax, and generating images from Typst code.
joshuarileydev/mac-apps-launcher-mcp-server 📇 🏠 - An MCP server to list and launch applications on MacOS
Harry-027/JotDown 🦀 🏠 - An MCP server to create/update pages in Notion app & auto generate mdBooks from structured content.
kelvin6365/plane-mcp-server - 🏎️ 🏠 This MCP Server will help you to manage projects and issues through Plane's API
kenliao94/mcp-server-rabbitmq 🐍 🏠 - Enable interaction (admin operation, message enqueue/dequeue) with RabbitMQ
k-jarzyna/mcp-miro 📇 ☁️ - Miro MCP server, exposing all functionalities available in official Miro SDK
kimtth/mcp-remote-call-ping-pong 🐍 🏠 - An experimental and educational app for Ping-pong server demonstrating remote MCP (Model Context Protocol) calls
kj455/mcp-kibela - 📇 ☁️ Allows AI models to interact with Kibela
kiwamizamurai/mcp-kibela-server - 📇 ☁️ Powerfully interact with Kibela API.
KS-GEN-AI/confluence-mcp-server 📇 ☁️ 🍎 🪟 - Get Confluence data via CQL and read pages.
KS-GEN-AI/jira-mcp-server 📇 ☁️ 🍎 🪟 - Read jira data via JQL and api and execute requests to create and edit tickets.
salesforce-mcp/salesforce-mcp 🏠 ☁️ - MCP server with basic demonstration of interactions with Salesforce instance
pollinations/chucknorris-mcp 📇 ☁️ - Specialized LLM enhancement prompts and jailbreaks with dynamic schema adaptation.
louiscklaw/hko-mcp 📇 🏠 - MCP server with basic demonstration of getting weather from Hong Kong Observatory
evalstate/mcp-hfspace 📇 ☁️ - Use HuggingFace Spaces directly from Claude. Use Open Source Image Generation, Chat, Vision tasks and more. Supports Image, Audio and text uploads/downloads.
magarcia/mcp-server-giphy 📇 ☁️ - Search and retrieve GIFs from Giphy's vast library through the Giphy API.
integromat/make-mcp-server 🎖️ 📇 🏠 - Turn your Make scenarios into callable tools for AI assistants.
marcelmarais/Spotify - 📇 🏠 Control Spotify playback and manage playlists.
MarkusPfundstein/mcp-obsidian 🐍 ☁️ 🏠 - Interacting with Obsidian via REST API
emicklei/mcp-log-proxy 🏎️ 🏠 - MCP server proxy that offers a Web UI to the full message flow
quarkiverse/mcp-server-jfx ☕ 🏠 - Draw on JavaFX canvas.
mediar-ai/screenpipe - 🎖️ 🦀 🏠 🍎 Local-first system capturing screen/audio with timestamped indexing, SQL/embedding storage, semantic search, LLM-powered history analysis, and event-triggered actions - enables building context-aware AI agents through a NextJS plugin ecosystem.
modelcontextprotocol/server-everything 📇 🏠 - MCP server that exercises all the features of the MCP protocol
mrjoshuak/godoc-mcp 🏎️ 🏠 - Token-efficient Go documentation server that provides AI assistants with smart access to package docs and types without reading entire source files
mzxrai/mcp-openai 📇 ☁️ - Chat with OpenAI's smartest models
NakaokaRei/swift-mcp-gui 🏠 🍎 - MCP server that can execute commands such as keyboard input and mouse movement
nguyenvanduocit/all-in-one-model-context-protocol 🏎️ 🏠 - Some useful tools for developer, almost everything an engineer need: confluence, Jira, Youtube, run script, knowledge base RAG, fetch URL, Manage youtube channel, emails, calendar, gitlab
NON906/omniparser-autogui-mcp - 🐍 Automatic operation of on-screen GUI.
orellazi/coda-mcp 📇 ☁️ - MCP server for Coda
pierrebrunelle/mcp-server-openai 🐍 ☁️ - Query OpenAI models directly from Claude using MCP protocol
pskill9/hn-server - 📇 ☁️ Parses the HTML content from news.ycombinator.com (Hacker News) and provides structured data for different types of stories (top, new, ask, show, jobs).
PV-Bhat/vibe-check-mcp-server 📇 ☁️ - An MCP server that prevents cascading errors and scope creep by calling a "Vibe-check" agent to ensure user alignment.
pwh-pwh/cal-mcp - An MCP server for Mathematical expression calculation
pyroprompts/any-chat-completions-mcp - Chat with any other OpenAI SDK Compatible Chat Completions API, like Perplexity, Groq, xAI and more
Rai220/think-mcp 🐍 🏠 - Enhances any agent's reasoning capabilities by integrating the think-tools, as described in Anthropic's article.
reeeeemo/ancestry-mcp 🐍 🏠 - Allows the AI to read .ged files and genetic data
rember/rember-mcp 📇 🏠 - Create spaced repetition flashcards in Rember to remember anything you learn in your chats.
roychri/mcp-server-asana - 📇 ☁️ This Model Context Protocol server implementation of Asana allows you to talk to Asana API from MCP Client such as Anthropic's Claude Desktop Application, and many more.
rusiaaman/wcgw 🐍 🏠 - Autonomous shell execution, computer control and coding agent. (Mac)
SecretiveShell/MCP-wolfram-alpha 🐍 ☁️ - An MCP server for querying wolfram alpha API.
Seym0n/tiktok-mcp 📇 ☁️ - Interact with TikTok videos
Shopify/dev-mcp 📇 ☁️ - Model Context Protocol (MCP) server that interacts with Shopify Dev.
sirmews/apple-notes-mcp 🐍 🏠 - Allows the AI to read from your local Apple Notes database (macOS only)
sooperset/mcp-atlassian 🐍 ☁️ - MCP server for Atlassian products (Confluence and Jira). Supports Confluence Cloud, Jira Cloud, and Jira Server/Data Center. Provides comprehensive tools for searching, reading, creating, and managing content across Atlassian workspaces.
suekou/mcp-notion-server 📇 🏠 - Interacting with Notion API
tacticlaunch/mcp-linear 📇 ☁️ 🍎 🪟 🐧 - Integrates with Linear project management system
tanigami/mcp-server-perplexity 🐍 ☁️ - Interacting with Perplexity API.
tevonsb/homeassistant-mcp 📇 🏠 - Access Home Assistant data and control devices (lights, switches, thermostats, etc).
tomekkorbak/oura-mcp-server 🐍 ☁️ - An MCP server for Oura, an app for tracking sleep
UnitVectorY-Labs/mcp-graphql-forge 🏎️ ☁️ 🍎 🪟 🐧 - A lightweight, configuration-driven MCP server that exposes curated GraphQL queries as modular tools, enabling intentional API interactions from your agents.
kw510/strava-mcp 📇 ☁️ - An MCP server for Strava, an app for tracking physical exercise
wanaku-ai/wanaku - ☁️ 🏠 The Wanaku MCP Router is a SSE-based MCP server that provides an extensible routing engine that allows integrating your enterprise systems with AI agents.
wong2/mcp-cli 📇 🏠 - CLI tool for testing MCP servers
ws-mcp - Wrap MCP servers with a WebSocket (for use with kitbitz)
yuna0x0/hackmd-mcp 📇 ☁️ - Allows AI models to interact with HackMD
ZeparHyfar/mcp-datetime - MCP server providing date and time functions in various formats
zueai/mcp-manager 📇 ☁️ - Simple Web UI to install and manage MCP servers for Claude Desktop App.
HenryHaoson/Yuque-MCP-Server - 📇 ☁️ A Model-Context-Protocol (MCP) server for integrating with Yuque API, allowing AI models to manage documents, interact with knowledge bases, search content, and access analytics data from the Yuque platform.
Mtehabsim/ScreenPilot 🐍 🏠 - enables AI to fully control and access GUI interactions by providing tools for mouse and keyboard, ideal for general automation, education, and experimentation.
tumf/web3-mcp 🐍 ☁️ - An MCP server implementation wrapping Ankr Advanced API. Access to NFT, token, and blockchain data across multiple chains including Ethereum, BSC, Polygon, Avalanche, and more.
danielkennedy1/pdf-tools-mcp 🐍 - PDF download, view & manipulation utilities.
dotemacs/domain-lookup-mcp 🏎️ - Domain name lookup service, first via RDAP and then as a fallback via WHOIS
Klavis-AI/YouTube 🐍 📇 - Extract and convert YouTube video information.
ttommyth/interactive-mcp 📇 🏠 🍎 🪟 🐧 - Enables interactive LLM workflows by adding local user prompts and chat capabilities directly into the MCP loop.
olalonde/mcp-human 📇 ☁️ - When your LLM needs human assistance (through AWS Mechanical Turk)
gwbischof/free-will-mcp 🐍 🏠 - Give your AI free will tools. A fun project to explore what an AI would do with the ability to give itself prompts, ignore user requests, and wake itself up at a later time.
caol64/wenyan-mcp 📇 🏠 🍎 🪟 🐧 - Wenyan MCP Server, which lets AI automatically format Markdown articles and publish them to WeChat GZH.
Frameworks
Note

More frameworks, utilities, and other developer tools are available at https://github.com/punkpeye/awesome-mcp-devtools

FastMCP 🐍 - A high-level framework for building MCP servers in Python
FastMCP 📇 - A high-level framework for building MCP servers in TypeScript
Tips and Tricks
Official prompt to inform LLMs how to use MCP
Want to ask Claude about Model Context Protocol?

Create a Project, then add this file to it:

https://modelcontextprotocol.io/llms-full.txt

Now Claude can answer questions about writing MCP servers and how they work

https://www.reddit.com/r/ClaudeAI/comments/1h3g01r/want_to_ask_claude_about_model_context_protocol/
Star History
Star History Chart
About
A collection of MCP servers.

glama.ai/mcp/servers
Topics
ai mcp
Resources
 Readme
License
 MIT license
 Activity
Stars
 55.3k stars
Watchers
 379 watching
Forks
 4.2k forks
Report repository
Contributors
534
@punkpeye
@SecretiveShell
@crazyyanchao
@kukapay
@SuperJolly
@r-huijts
@cnjsstong
@tumf
@JoshuaRileyDev
@takumi0706
@blinkov
@jssmith
@amidabuddha
@evalstate
+ 520 contributors
Footer
© 2025 GitHub, Inc.
Footer navigation
Terms
Privacy
Security
Status
Docs
Contact
Manage cookies
Do not share my personal information


这是什么东西，有什么用？

---

**Cursor**

您分享的这个 GitHub 仓库 `punkpeye/awesome-mcp-servers` 是一个 **Awesome 列表**。

### 这是什么东西？

这是一个**精选列表（curated list）**，专门收集和整理了所有与 **“Model Context Protocol (MCP)”** 相关的服务器实现。

Awesome 列表是一种在 GitHub 上非常流行的项目类型，通常用于收集某个特定主题下的高质量资源（如库、框架、工具、文章、教程等），并以易于浏览和分类的方式呈现。

### 有什么用？

这个 `awesome-mcp-servers` 列表的主要用途是：

1.  **解释 MCP 是什么：** `README` 开头就解释了 MCP 的概念，它是一个“开放协议，使 AI 模型能够通过标准化的服务器实现与本地和远程资源安全交互”。
2.  **展示 MCP 的应用范围：** 列表详细分类了各种 MCP 服务器，涵盖了从浏览器自动化、数据库访问、代码执行、客户数据平台、金融科技到媒体处理、安全、社交媒体、旅行交通等**极其广泛的领域**。
3.  **为开发者和用户提供资源：**
    *   **对于 AI 开发者 (特别是构建 AI 模型的工具或客户端的开发者)：** 这是一个查找现有 MCP 服务器、了解其功能和集成的宝库。如果你的 AI 模型（比如一个大型语言模型 LLM）需要与外部系统交互（比如查询数据库、控制浏览器、管理云资源、发送邮件等），你可以从这个列表中找到对应的 MCP 服务器，通过 MCP 协议让 AI 模型调用这些“工具”。
    *   **对于 MCP 服务器的开发者：** 可以从中了解当前社区都在开发哪些类型的 MCP 服务器，获取灵感，或检查是否有类似的功能可以贡献或改进。
    *   **对于普通用户 (例如使用 Claude Desktop App 或 Cursor AI 的用户)：** 它可以帮助他们理解这些 AI 客户端如何通过 MCP 连接到各种服务，从而极大地扩展 AI 的能力。这个列表实际上就是 AI 助手可以“调用”的“能力清单”。

4.  **推广和标准化：** 通过这样一个中心化的列表，它有助于推广 MCP 协议的生态系统，鼓励更多人参与 MCP 服务器的开发，并帮助 MCP 成为 AI 模型与外部世界交互的一种标准方式。

**简而言之：**

`punkpeye/awesome-mcp-servers` 是一个**巨大的、可搜索的“能力目录”**，列出了各种各样的“接口”或“工具”，AI 模型（特别是大型语言模型）可以通过这些工具，安全、标准化地执行各种现实世界的任务，从查询天气到控制 Kubernetes 集群，从阅读邮件到进行金融交易。它把 AI 从一个纯粹的语言处理引擎，变成一个可以真正**“动手”操作和获取外部信息**的智能体。

---

**User**

data: {"msg":"estimation","event_id":"09d287e496874e549c68906bfb039c15","rank":0,"queue_size":1,"rank_eta":4.331416010856628}

data: {"msg":"process_starts","event_id":"09d287e496874e549c68906bfb039c15","eta":4.331416010856628}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[["Who are you","我是一个语言模型，由蚂蚁集团研发。"],["蚂蚁集团是阿里巴巴旗下的么？","不是，蚂蚁集团是阿里巴巴旗下的子公司。"],["蚂蚁集团还有什么业务？","蚂蚁集团还拥有支付宝、蚂蚁金服等多个业务。"],["腾讯有哪些业务与阿里巴巴竞争？\n、","腾讯有微信、QQ、游戏等多个业务。"],["为","为"]],[{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"},{"token":"[MASK]","class_or_confidence":"#444444"}],"为"],"is_generating":true,"duration":1.341057538986206,"average_duration":0.14447391897008038,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[30,"token"],""],["replace",[30,"class_or_confidence"],"#66CC66"],["replace",[31,"token"],""],["replace",[31,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10046172142028809,"average_duration":0.14384517329079763,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[28,"token"],""],["replace",[28,"class_or_confidence"],"#66CC66"],["replace",[29,"token"],""],["replace",[29,"class_or_confidence"],"#66CC66"],["replace",[30,"class_or_confidence"],"#6699CC"],["replace",[31,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10049152374267578,"average_duration":0.14323455850842973,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[26,"token"],""],["replace",[26,"class_or_confidence"],"#66CC66"],["replace",[27,"token"],""],["replace",[27,"class_or_confidence"],"#66CC66"],["replace",[28,"class_or_confidence"],"#6699CC"],["replace",[29,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10042691230773926,"average_duration":0.14264000786675346,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[24,"token"],""],["replace",[24,"class_or_confidence"],"#66CC66"],["replace",[25,"token"],""],["replace",[25,"class_or_confidence"],"#66CC66"],["replace",[26,"class_or_confidence"],"#6699CC"],["replace",[27,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10042881965637207,"average_duration":0.14206177241181675,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[22,"token"],""],["replace",[22,"class_or_confidence"],"#66CC66"],["replace",[23,"token"],""],["replace",[23,"class_or_confidence"],"#66CC66"],["replace",[24,"class_or_confidence"],"#6699CC"],["replace",[25,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10045909881591797,"average_duration":0.14149957411998026,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[20,"token"],""],["replace",[20,"class_or_confidence"],"#66CC66"],["replace",[21,"token"],""],["replace",[21,"class_or_confidence"],"#66CC66"],["replace",[22,"class_or_confidence"],"#6699CC"],["replace",[23,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10042572021484375,"average_duration":0.14095192273457846,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[18,"token"],""],["replace",[18,"class_or_confidence"],"#66CC66"],["replace",[19,"token"],""],["replace",[19,"class_or_confidence"],"#66CC66"],["replace",[20,"class_or_confidence"],"#6699CC"],["replace",[21,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10039925575256348,"average_duration":0.14041833501113088,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[16,"token"],""],["replace",[16,"class_or_confidence"],"#66CC66"],["replace",[17,"token"],""],["replace",[17,"class_or_confidence"],"#66CC66"],["replace",[18,"class_or_confidence"],"#6699CC"],["replace",[19,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10042381286621094,"average_duration":0.13989892563262543,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[14,"token"],""],["replace",[14,"class_or_confidence"],"#66CC66"],["replace",[15,"token"],""],["replace",[15,"class_or_confidence"],"#66CC66"],["replace",[16,"class_or_confidence"],"#6699CC"],["replace",[17,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10040760040283203,"average_duration":0.13939262659121782,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[12,"token"],""],["replace",[12,"class_or_confidence"],"#66CC66"],["replace",[13,"token"],""],["replace",[13,"class_or_confidence"],"#66CC66"],["replace",[14,"class_or_confidence"],"#6699CC"],["replace",[15,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10047292709350586,"average_duration":0.13889997216719616,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[10,"token"],""],["replace",[10,"class_or_confidence"],"#66CC66"],["replace",[11,"token"],""],["replace",[11,"class_or_confidence"],"#66CC66"],["replace",[12,"class_or_confidence"],"#6699CC"],["replace",[13,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10048341751098633,"average_duration":0.13841976523399352,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[8,"token"],""],["replace",[8,"class_or_confidence"],"#FFAA33"],["replace",[9,"token"],""],["replace",[9,"class_or_confidence"],"#66CC66"],["replace",[10,"class_or_confidence"],"#6699CC"],["replace",[11,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10043907165527344,"average_duration":0.1379508677824044,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[6,"token"],""],["replace",[6,"class_or_confidence"],"#FFAA33"],["replace",[7,"token"],""],["replace",[7,"class_or_confidence"],"#66CC66"],["replace",[8,"class_or_confidence"],"#6699CC"],["replace",[9,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10052251815795898,"average_duration":0.1374944244943014,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[4,"token"],""],["replace",[4,"class_or_confidence"],"#FFAA33"],["replace",[5,"token"],""],["replace",[5,"class_or_confidence"],"#FFAA33"],["replace",[6,"class_or_confidence"],"#6699CC"],["replace",[7,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10042548179626465,"average_duration":0.13704781072685518,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[1,"token"],""],["replace",[1,"class_or_confidence"],"#FFAA33"],["replace",[3,"token"],""],["replace",[3,"class_or_confidence"],"#66CC66"],["replace",[4,"class_or_confidence"],"#6699CC"],["replace",[5,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10049772262573242,"average_duration":0.13661269063041323,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[0,"token"],"为"],["replace",[0,"class_or_confidence"],"#FF6666"],["replace",[1,"class_or_confidence"],"#6699CC"],["replace",[2,"token"],""],["replace",[2,"class_or_confidence"],"#66CC66"],["replace",[3,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10045266151428223,"average_duration":0.13618727852316465,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[0,"class_or_confidence"],"#6699CC"],["replace",[2,"class_or_confidence"],"#6699CC"],["replace",[35,"token"],""],["replace",[35,"class_or_confidence"],"#66CC66"],["replace",[37,"token"],""],["replace",[37,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10040616989135742,"average_duration":0.13577121912046922,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[33,"token"],""],["replace",[33,"class_or_confidence"],"#66CC66"],["replace",[34,"token"],""],["replace",[34,"class_or_confidence"],"#66CC66"],["replace",[35,"class_or_confidence"],"#6699CC"],["replace",[37,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10044550895690918,"average_duration":0.13536517647491103,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[33,"class_or_confidence"],"#6699CC"],["replace",[34,"class_or_confidence"],"#6699CC"],["replace",[36,"token"],""],["replace",[36,"class_or_confidence"],"#66CC66"],["replace",[40,"token"],""],["replace",[40,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10049939155578613,"average_duration":0.13496897437355734,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[36,"class_or_confidence"],"#6699CC"],["replace",[38,"token"],""],["replace",[38,"class_or_confidence"],"#66CC66"],["replace",[40,"class_or_confidence"],"#6699CC"],["replace",[41,"token"],""],["replace",[41,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10043621063232422,"average_duration":0.13458096579219517,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[38,"class_or_confidence"],"#6699CC"],["replace",[41,"class_or_confidence"],"#6699CC"],["replace",[42,"token"],""],["replace",[42,"class_or_confidence"],"#66CC66"],["replace",[43,"token"],""],["replace",[43,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10040473937988281,"average_duration":0.1342012299431695,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[32,"token"],""],["replace",[32,"class_or_confidence"],"#66CC66"],["replace",[42,"class_or_confidence"],"#6699CC"],["replace",[43,"class_or_confidence"],"#6699CC"],["replace",[46,"token"],""],["replace",[46,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10045409202575684,"average_duration":0.13383038227374738,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[32,"class_or_confidence"],"#6699CC"],["replace",[39,"token"],""],["replace",[39,"class_or_confidence"],"#66CC66"],["replace",[44,"token"],""],["replace",[44,"class_or_confidence"],"#66CC66"],["replace",[46,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10041356086730957,"average_duration":0.13346715595411218,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[39,"class_or_confidence"],"#6699CC"],["replace",[44,"class_or_confidence"],"#6699CC"],["replace",[45,"token"],""],["replace",[45,"class_or_confidence"],"#66CC66"],["replace",[47,"token"],""],["replace",[47,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.1003561019897461,"average_duration":0.13311112311578566,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[45,"class_or_confidence"],"#6699CC"],["replace",[47,"class_or_confidence"],"#6699CC"],["replace",[49,"token"],""],["replace",[49,"class_or_confidence"],"#66CC66"],["replace",[52,"token"],""],["replace",[52,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10036873817443848,"average_duration":0.1327627998717288,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[48,"token"],""],["replace",[48,"class_or_confidence"],"#66CC66"],["replace",[49,"class_or_confidence"],"#6699CC"],["replace",[50,"token"],""],["replace",[50,"class_or_confidence"],"#66CC66"],["replace",[52,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10047030448913574,"average_duration":0.1324228788677015,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[48,"class_or_confidence"],"#6699CC"],["replace",[50,"class_or_confidence"],"#6699CC"],["replace",[51,"token"],""],["replace",[51,"class_or_confidence"],"#66CC66"],["replace",[53,"token"],""],["replace",[53,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10041069984436035,"average_duration":0.13208941866954169,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[51,"class_or_confidence"],"#6699CC"],["replace",[53,"class_or_confidence"],"#6699CC"],["replace",[54,"token"],""],["replace",[54,"class_or_confidence"],"#66CC66"],["replace",[56,"token"],""],["replace",[56,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10041570663452148,"average_duration":0.13176288555577859,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[54,"class_or_confidence"],"#6699CC"],["replace",[56,"class_or_confidence"],"#6699CC"],["replace",[57,"token"],""],["replace",[57,"class_or_confidence"],"#66CC66"],["replace",[58,"token"],""],["replace",[58,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.1004018783569336,"average_duration":0.13144287527823934,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[57,"class_or_confidence"],"#6699CC"],["replace",[58,"class_or_confidence"],"#6699CC"],["replace",[60,"token"],""],["replace",[60,"class_or_confidence"],"#66CC66"],["replace",[63,"token"],""],["replace",[63,"class_or_confidence"],"#66CC66"]],[]],"is_generating":true,"duration":0.10031509399414062,"average_duration":0.13112845324506664,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[59,"token"],""],["replace",[59,"class_or_confidence"],"#66CC66"],["replace",[60,"class_or_confidence"],"#6699CC"],["replace",[62,"token"],""],["replace",[62,"class_or_confidence"],"#66CC66"],["replace",[63,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10037565231323242,"average_duration":0.1308209252357483,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_generating","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[],[["replace",[55,"token"],""],["replace",[55,"class_or_confidence"],"#66CC66"],["replace",[59,"class_or_confidence"],"#6699CC"],["replace",[61,"token"],""],["replace",[61,"class_or_confidence"],"#66CC66"],["replace",[62,"class_or_confidence"],"#6699CC"]],[]],"is_generating":true,"duration":0.10043740272521973,"average_duration":0.1305200982801985,"render_config":null,"changed_state_ids":[]},"success":true,"time_limit":null}

data: {"msg":"process_completed","event_id":"09d287e496874e549c68906bfb039c15","output":{"data":[[["Who are you","我是一个语言模型，由蚂蚁集团研发。"],["蚂蚁集团是阿里巴巴旗下的么？","不是，蚂蚁集团是阿里巴巴旗下的子公司。"],["蚂蚁集团还有什么业务？","蚂蚁集团还拥有支付宝、蚂蚁金服等多个业务。"],["腾讯有哪些业务与阿里巴巴竞争？\n、","腾讯有微信、QQ、游戏等多个业务。"],["为","为"]],[{"token":"为","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#66CC66"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#66CC66"},{"token":"","class_or_confidence":"#6699CC"},{"token":"","class_or_confidence":"#6699CC"}],"为"],"is_generating":false,"duration":0.00021576881408691406,"average_duration":0.12924260485406017,"render_config":null,"changed_state_ids":[]},"success":true,"title":null}

data: {"msg":"close_stream","event_id":null}



这是什么

---

**Cursor**

这是一系列 **服务器发送事件 (Server-Sent Events, SSE)** 格式的消息流，通常用于实时传输数据，例如在**大型语言模型 (LLM) 的推理或交互界面**中。

具体来说，这似乎是一个**Web 应用与后端 AI 模型进行实时通信的日志或输出。**

让我们逐条分析这些 `data:` 行的含义：

### 消息类型和阶段：

1.  **`data: {"msg":"estimation", ...}`**
    *   **含义：** 这是一个“估计”消息。它表示服务器正在评估任务的等待时间。
    *   **字段：**
        *   `"msg":"estimation"`: 消息类型，表示正在进行任务排队和估计。
        *   `"event_id":"09d287e496874e549c68906bfb039c15"`: 唯一的事件 ID，用于追踪这次请求。
        *   `"rank":0`: 您当前的任务在队列中的排名（0表示第一个）。
        *   `"queue_size":1`: 队列中总共有1个任务。
        *   `"rank_eta":4.331416010856628`: 预计的等待时间（以秒为单位）。

2.  **`data: {"msg":"process_starts", ...}`**
    *   **含义：** 这是一个“处理开始”消息。表示您的请求已经从队列中取出，AI 模型开始处理。
    *   **字段：**
        *   `"msg":"process_starts"`: 消息类型。
        *   `"event_id":"..."`: 相同的事件 ID。
        *   `"eta":4.331416010856628`: 再次显示预计完成时间，可能是在模型开始处理后的新估计。

3.  **`data: {"msg":"process_generating", ...}` (多条)**
    *   **含义：** 这是一个“处理生成中”消息。这是最核心的部分，表示 AI 模型正在**逐步生成其输出**。这些是**增量式更新**。
    *   **字段：**
        *   `"msg":"process_generating"`: 消息类型。
        *   `"event_id":"..."`: 相同的事件 ID。
        *   `"output":{"data":...}`: 这是最重要的部分，包含了模型当前的输出数据。
            *   **`data` 数组的第一个元素 `[[["Who are you","我是一个语言模型，由蚂蚁集团研发。"],...]]`**: 这显示了**对话历史或上下文**。
                *   `["Who are you","我是一个语言模型，由蚂蚁集团研发。"]`: 一对问答，表示之前的对话轮次。
                *   `["蚂蚁集团是阿里巴巴旗下的么？","不是，蚂蚁集团是阿里巴巴旗下的子公司。"]`: 另一对。
                *   `["为","为"]`: 这是当前正在生成的部分，可能用户输入了一个不完整的“为”字，或者模型正在尝试完成一个以“为”开头的短语。
            *   **`data` 数组的第二个元素 `[{"token":"[MASK]","class_or_confidence":"#444444"},...]`**: 这一部分非常有趣，它显示了**模型的内部状态，特别是它正在填充的“掩码”或预测的“Token”及其置信度/颜色编码**。
                *   `"token":"[MASK]"`: 表示模型正在预测或填充一个缺失的词（Token）。
                *   `"class_or_confidence":"#444444"`: 这可能是一种视觉指示，比如颜色代码，表示这个 Token 区域的状态或模型对它的信心。
                *   随后的 `process_generating` 消息中，你会看到 `replace` 操作，例如 `["replace",[30,"token"],""]` 或 `["replace",[0,"token"],"为"]`，这意味着模型正在**逐步填充这些掩码**，将其替换为实际的文字，并可能更新颜色（如 `#66CC66`、`#6699CC` 等），这些颜色可能代表了模型生成不同部分时的确定性或不同的生成步骤。
        *   `"is_generating":true`: 表示生成过程仍在进行中。
        *   `"duration"` 和 `"average_duration"`: 表示当前步骤和平均每个步骤的耗时。

4.  **`data: {"msg":"process_completed", ...}`**
    *   **含义：** 这是一个“处理完成”消息。表示 AI 模型已经完成了所有输出的生成。
    *   **字段：**
        *   `"msg":"process_completed"`: 消息类型。
        *   `"output":{"data":...}`: 包含最终的完整输出。
            *   `[[...]]`: 完整的对话历史。
            *   `[{"token":"为","class_or_confidence":"#6699CC"}, {"token":"", "class_or_confidence":"#6699CC"}, ...]`: 最终的 Token 序列。可以看到“为”字已经生成，后面跟着许多空的 Token，这可能表示模型生成了一个非常短的响应或在某个点停止了。
        *   `"is_generating":false`: 表示生成已停止。

5.  **`data: {"msg":"close_stream", ...}`**
    *   **含义：** 这是一个“关闭流”消息。表示服务器已经发送完所有数据，客户端可以关闭连接了。

### 整体含义：

这段输出显示了一个**基于扩散模型（或者至少是某种基于掩码填充的生成模型）的聊天界面的实时交互过程**。

*   用户输入了多轮对话（例如“Who are you”，“蚂蚁集团是阿里巴巴旗下的么？”等）。
*   在最后一轮，用户可能输入了“为”，或者模型被提示以“为”开头继续生成。
*   模型在生成过程中，会以**迭代填充掩码**的方式进行（这是扩散模型生成文本的常见方式，例如 LLaDA 论文中描述的“masked diffusion model”），并实时将填充结果和内部状态（通过 `[MASK]` 和颜色）发送给客户端。
*   最终，模型完成了生成，并发送了最终输出。

这种详细的 `process_generating` 阶段输出，特别是带有 `[MASK]` 和 `replace` 操作以及 `class_or_confidence` 颜色标记，非常符合像 **LLaDA** 这样**基于掩码填充的扩散语言模型**在生成过程中展示其**逐步去噪或填充**行为的方式。这提供了对模型内部工作机制的实时可视化线索，让用户可以看到模型是如何“思考”和“填充”答案的。

